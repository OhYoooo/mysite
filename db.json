{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/Memory-Leaks.JPG","path":"images/Memory-Leaks.JPG","modified":0,"renderable":0},{"_id":"source/images/cli.JPG","path":"images/cli.JPG","modified":0,"renderable":0},{"_id":"source/images/redux-saga.JPG","path":"images/redux-saga.JPG","modified":0,"renderable":0},{"_id":"source/images/neteaseMusic.JPG","path":"images/neteaseMusic.JPG","modified":0,"renderable":0},{"_id":"source/images/spark_kafka.JPG","path":"images/spark_kafka.JPG","modified":0,"renderable":0},{"_id":"source/images/vscode.JPG","path":"images/vscode.JPG","modified":0,"renderable":0},{"_id":"source/images/treeRecursion.JPG","path":"images/treeRecursion.JPG","modified":0,"renderable":0},{"_id":"source/images/treeRecursion11.JPG","path":"images/treeRecursion11.JPG","modified":0,"renderable":0},{"_id":"themes/ocean/source/404.html","path":"404.html","modified":0,"renderable":1},{"_id":"themes/ocean/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"source/images/lisp.JPG","path":"images/lisp.JPG","modified":0,"renderable":0},{"_id":"source/images/GoSlice.JPG","path":"images/GoSlice.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0504.JPG","path":"images/IMG_0504.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0606.JPG","path":"images/IMG_0606.JPG","modified":0,"renderable":0},{"_id":"themes/ocean/source/css/404.styl","path":"css/404.styl","modified":0,"renderable":1},{"_id":"themes/ocean/source/css/tranquilheart.css","path":"css/tranquilheart.css","modified":0,"renderable":1},{"_id":"themes/ocean/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.css","path":"fancybox/jquery.fancybox.min.css","modified":0,"renderable":1},{"_id":"themes/ocean/source/images/hexo.svg","path":"images/hexo.svg","modified":0,"renderable":1},{"_id":"themes/ocean/source/images/hexo-inverted.svg","path":"images/hexo-inverted.svg","modified":0,"renderable":1},{"_id":"themes/ocean/source/js/busuanzi-2.3.pure.min.js","path":"js/busuanzi-2.3.pure.min.js","modified":0,"renderable":1},{"_id":"themes/ocean/source/js/lazyload.min.js","path":"js/lazyload.min.js","modified":0,"renderable":1},{"_id":"themes/ocean/source/js/ocean.js","path":"js/ocean.js","modified":0,"renderable":1},{"_id":"themes/ocean/source/js/pace.min.js","path":"js/pace.min.js","modified":0,"renderable":1},{"_id":"themes/ocean/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/ocean/source/js/prettify.js","path":"js/prettify.js","modified":0,"renderable":1},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.js","path":"fancybox/jquery.fancybox.min.js","modified":0,"renderable":1},{"_id":"themes/ocean/source/js/jquery-2.0.3.min.js","path":"js/jquery-2.0.3.min.js","modified":0,"renderable":1},{"_id":"themes/ocean/source/images/searchfooter.png","path":"images/searchfooter.png","modified":0,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.ttf","path":"css/feathericon/feathericon.ttf","modified":0,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff","path":"css/feathericon/feathericon.woff","modified":0,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff2","path":"css/feathericon/feathericon.woff2","modified":0,"renderable":1},{"_id":"themes/ocean/source/images/ocean/overlay-hero.png","path":"images/ocean/overlay-hero.png","modified":0,"renderable":1},{"_id":"source/images/IMG_2148.JPG","path":"images/IMG_2148.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_1891.JPG","path":"images/IMG_1891.JPG","modified":0,"renderable":0},{"_id":"source/images/futako010010.JPG","path":"images/futako010010.JPG","modified":0,"renderable":0},{"_id":"themes/ocean/source/css/feathericon/feathericon.eot","path":"css/feathericon/feathericon.eot","modified":0,"renderable":1},{"_id":"themes/ocean/source/css/feathericon/feathericon.svg","path":"css/feathericon/feathericon.svg","modified":0,"renderable":1},{"_id":"source/images/IMG_0874.JPG","path":"images/IMG_0874.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_4201.JPG","path":"images/IMG_4201.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_4200.JPG","path":"images/IMG_4200.JPG","modified":0,"renderable":0},{"_id":"source/images/prefixTree.JPG","path":"images/prefixTree.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0444.JPG","path":"images/IMG_0444.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0875.JPG","path":"images/IMG_0875.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_1333.JPG","path":"images/IMG_1333.JPG","modified":0,"renderable":0},{"_id":"themes/ocean/source/images/forrestgump.png","path":"images/forrestgump.png","modified":0,"renderable":1},{"_id":"source/images/IMG_0518.JPG","path":"images/IMG_0518.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0226.JPG","path":"images/IMG_0226.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_2194.JPG","path":"images/IMG_2194.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_2221.JPG","path":"images/IMG_2221.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_2151.JPG","path":"images/IMG_2151.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_2192.JPG","path":"images/IMG_2192.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0222.JPG","path":"images/IMG_0222.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0637.JPG","path":"images/IMG_0637.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_2209.JPG","path":"images/IMG_2209.JPG","modified":0,"renderable":0},{"_id":"themes/ocean/source/images/ocean/ocean.ogv","path":"images/ocean/ocean.ogv","modified":0,"renderable":1},{"_id":"source/images/IMG_0873.JPG","path":"images/IMG_0873.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_1331.JPG","path":"images/IMG_1331.JPG","modified":0,"renderable":0},{"_id":"source/images/futako010029.JPG","path":"images/futako010029.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_2195.JPG","path":"images/IMG_2195.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0077.JPG","path":"images/IMG_0077.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0605.JPG","path":"images/IMG_0605.JPG","modified":0,"renderable":0},{"_id":"source/images/futako010013.JPG","path":"images/futako010013.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_0519.JPG","path":"images/IMG_0519.JPG","modified":0,"renderable":0},{"_id":"source/images/IMG_2200.JPG","path":"images/IMG_2200.JPG","modified":0,"renderable":0},{"_id":"source/images/futako010041.JPG","path":"images/futako010041.JPG","modified":0,"renderable":0},{"_id":"themes/ocean/source/images/ocean/ocean.png","path":"images/ocean/ocean.png","modified":0,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.webm","path":"images/ocean/ocean.webm","modified":0,"renderable":1},{"_id":"themes/ocean/source/images/ocean/ocean.mp4","path":"images/ocean/ocean.mp4","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"0525756ba9ec2b99018e7b1f3032aadbee5a9527","modified":1563798660800},{"_id":"themes/ocean/.DS_Store","hash":"dd0586a49b712829ce1e0c97a7b19a1232e0d1a3","modified":1557473998810},{"_id":"themes/ocean/_config.yml","hash":"47b265d984ab07f28744025ce97efda24a257675","modified":1556620349317},{"_id":"themes/ocean/README.md","hash":"0a5f44c3b9f6757ffdb3576f57afa53f203ea985","modified":1556351608519},{"_id":"themes/ocean/package.json","hash":"b993176f8c35bc3ab9dbd8642ec6cd125fcb447e","modified":1556351608530},{"_id":"source/_posts/How-to-Check-Open-TCP-IP-Ports-in-Mac-OS-X.md","hash":"9b68262d9aba32708433448fca14ed6298d6e978","modified":1557478812697},{"_id":"source/_posts/.DS_Store","hash":"087a7d462ed3b36bfaafc9199d92a7ca164081ad","modified":1563798664126},{"_id":"source/_posts/A-Brief-Introduce-to-Redux-Saga.md","hash":"6ba6a634260e80626ac3b2726325097a955e7e51","modified":1557482512394},{"_id":"source/_posts/How-to-debug-NodeJS-on-VS-Code.md","hash":"a8c08f053ae7192c55b1fdb50ec4b1ebe70601ac","modified":1557481978626},{"_id":"source/_posts/Implement-Zero-Data-Loss-in-Spark-Streaming.md","hash":"17eb58048f58db0446ec5a53f7b7c10b8de640c3","modified":1563459196399},{"_id":"source/_posts/Prefix-Notation.md","hash":"39925b12b50b0bd5aaf05fa8a4cd29577ef0fc84","modified":1557477976855},{"_id":"source/_posts/Prefix-Tree.md","hash":"88a1f33a83639b4d04236931bac7269a8b824814","modified":1563458914719},{"_id":"source/_posts/Memory-Leak-in-Serveral-Commonly-Used-Programming-Languages.md","hash":"532b9dfe4d56b5c88ae8a3742ff524091145e46b","modified":1557483219539},{"_id":"source/_posts/Suffix-Tree.md","hash":"d056a1c809d193256bf6e7703cc617589dad7fc0","modified":1563798951974},{"_id":"source/_posts/Type-slice-in-Golang.md","hash":"e2d218ae262cf4aa89319ac8074a384be7df778d","modified":1558514835136},{"_id":"source/_posts/如何用VS-Code免翻墙听网易云音乐.md","hash":"855aa81bf6c9219142d2eec3c971f71aca8026f3","modified":1558514862391},{"_id":"source/about/index.md","hash":"d91209004b506f3aa81f86f94a6482e1138aaeac","modified":1556364296199},{"_id":"source/_posts/hello-world.md","hash":"b28643c0f92783a2204f2bffd1031a6ecae5b0d2","modified":1556522293367},{"_id":"source/gallery/index.md","hash":"81878942167add6fa3ed58c994176e6573a859d8","modified":1556357667537},{"_id":"source/images/.DS_Store","hash":"e1fabaa3d46a24b261b1868d99a3a796980e5b14","modified":1563459602119},{"_id":"source/images/Memory-Leaks.JPG","hash":"88744198f42bcd6b31e70e82f798645cb453e2e1","modified":1557038733681},{"_id":"source/images/cli.JPG","hash":"9dc4dd69083df96b774dee488bb42cfe7dc35922","modified":1557132969055},{"_id":"source/images/redux-saga.JPG","hash":"246254876cfd63cdbb5d31523d02d078daecf40e","modified":1556430638342},{"_id":"source/images/neteaseMusic.JPG","hash":"9a9ce2438064e78c0c3c596d22e879fad40727dd","modified":1558514744734},{"_id":"source/images/spark_kafka.JPG","hash":"aadd0a222fd5fd046ec3c68b8a914080a70bf3b1","modified":1563459479553},{"_id":"source/images/vscode.JPG","hash":"7d97031e400f3b8ad192de733d9711f52a7f80ad","modified":1563459568654},{"_id":"source/images/treeRecursion.JPG","hash":"a09da0ab4779d513b081eeacff2ec0a80d78576a","modified":1556595372000},{"_id":"source/images/treeRecursion11.JPG","hash":"b3cb61f46ddff0067dc8ebe5357d21a37aeaecb7","modified":1556619951956},{"_id":"themes/ocean/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1556351608520},{"_id":"themes/ocean/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1556351608519},{"_id":"themes/ocean/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1556351608519},{"_id":"themes/ocean/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1556351608520},{"_id":"themes/ocean/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1556351608520},{"_id":"themes/ocean/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1556351608521},{"_id":"themes/ocean/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1556351608521},{"_id":"themes/ocean/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1556351608520},{"_id":"themes/ocean/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1556351608521},{"_id":"themes/ocean/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1556351608521},{"_id":"themes/ocean/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1556351608521},{"_id":"themes/ocean/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1556351608520},{"_id":"themes/ocean/layout/.DS_Store","hash":"acf7fef488db0d4e3170f1ce64478a72a6dd681a","modified":1556362284452},{"_id":"themes/ocean/layout/index.ejs","hash":"dead30ea8014348cef977dcb44eea0ae0f0601c5","modified":1556351608529},{"_id":"themes/ocean/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1556351608528},{"_id":"themes/ocean/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1556351608528},{"_id":"themes/ocean/layout/layout.ejs","hash":"9ce598d82d973518e255fe64019b8523a2d65796","modified":1556351608529},{"_id":"themes/ocean/layout/page.ejs","hash":"a9a48ae63f5d68a36382951166fdd6e482b901f1","modified":1556351608529},{"_id":"themes/ocean/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1556351608530},{"_id":"themes/ocean/layout/post.ejs","hash":"a9a48ae63f5d68a36382951166fdd6e482b901f1","modified":1556351608530},{"_id":"themes/ocean/source/.DS_Store","hash":"31e61c090aa46ea0c4d227808d995d0ef4d0ce46","modified":1557473998808},{"_id":"themes/ocean/source/404.html","hash":"fe1497ac9b2d47f4e3e880946e22fbfe3db7496e","modified":1556351608533},{"_id":"themes/ocean/source/favicon.ico","hash":"0f20298a6a4d1ebd7a7ae7b87d7a3ae9afec0623","modified":1556351608546},{"_id":"source/images/lisp.JPG","hash":"5f09d56757170819589f35eed3cb8397dea6c222","modified":1556618649935},{"_id":"source/_posts/Implement-Zero-Data-Loss-in-Spark-Streaming/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1560999629177},{"_id":"source/_posts/Prefix-Tree/prefix_tree.JPG","hash":"22e46566e03150b74cf2dee5dd95b77dbae76687","modified":1563443466189},{"_id":"source/_posts/Suffix-Tree/PrefixTree_StringProblem.JPG","hash":"5dfbc9a3742fe8d6104c4ae4897888fe907b8c18","modified":1563794755884},{"_id":"source/_posts/Suffix-Tree/prefix_tree.JPG","hash":"22e46566e03150b74cf2dee5dd95b77dbae76687","modified":1563443466189},{"_id":"source/images/GoSlice.JPG","hash":"920bdae1c37b54a35358ce3ed70ab278b8b96a7c","modified":1557649400006},{"_id":"source/images/IMG_0504.JPG","hash":"bdd3c4dcc7e813685793a2226728b0c60fb653b9","modified":1556355556624},{"_id":"source/images/IMG_0606.JPG","hash":"1aa6473addf50803fef60b42e90cdb2d71113db8","modified":1556355554985},{"_id":"themes/ocean/layout/_partial/archive.ejs","hash":"6c6cf7d1acb6548396183ce4836f1f9a3a1a4d10","modified":1556351608523},{"_id":"themes/ocean/layout/_partial/archive-post.ejs","hash":"8f46a5a73c95827d812ca3e90ebb0ad8f16fb7b2","modified":1556351608522},{"_id":"themes/ocean/layout/_partial/after-footer.ejs","hash":"f34ea37e96ce18a24e00e6f5ffdaf94b999a589a","modified":1557474490123},{"_id":"themes/ocean/layout/_partial/article.ejs","hash":"875408862fa6048e08fe04cfb1864af2ae4ef81a","modified":1556619274055},{"_id":"themes/ocean/layout/_partial/head.ejs","hash":"b8174a6094f859ce82e8af2f1b30e1ce03fd0eb2","modified":1556351608524},{"_id":"themes/ocean/layout/_partial/ocean.ejs","hash":"be76e0cbc4ecd9171972fabed6830cb592b5b343","modified":1556351608524},{"_id":"themes/ocean/layout/_partial/sidebar.ejs","hash":"6e5fadba43415d4605593674591cce822b6fb8bf","modified":1556351608528},{"_id":"themes/ocean/layout/_partial/totop.ejs","hash":"70176e319a1558c8b61abecfedbbc08b258e7beb","modified":1556351608528},{"_id":"themes/ocean/layout/_partial/footer.ejs","hash":"259129dfc8a952f81be494751982dc3d2c763037","modified":1556360249353},{"_id":"themes/ocean/source/css/404.styl","hash":"25bf5e29c00d57f90f30673912e13478e47db69c","modified":1556514756850},{"_id":"themes/ocean/source/css/_feathericon.styl","hash":"8494f0e869411781264868f08eda62fd838e0cee","modified":1556351608534},{"_id":"themes/ocean/source/css/.DS_Store","hash":"f4924987f89647cf5bd58a0bbddf33ed3e4d6c3f","modified":1557473993133},{"_id":"themes/ocean/source/css/_extend.styl","hash":"1fb5b31668579d177b340e03a78136bc04e22a36","modified":1556513611196},{"_id":"themes/ocean/source/css/_mixins.styl","hash":"fbe77673e6f8c714a90daabba6c94cf491650887","modified":1556351608535},{"_id":"themes/ocean/source/css/_normalize.styl","hash":"b3337320133b7a336db7033aa6bbe94b054c0b21","modified":1556351608535},{"_id":"themes/ocean/source/css/_variables.styl","hash":"a91de5d66d44a31b7ea7b2e918e47f9d7c662434","modified":1556514844779},{"_id":"themes/ocean/screenshots/hexo-theme-ocean.jpg","hash":"13b5045d2120cac2f68849757f5e0af08938b7c6","modified":1556351608532},{"_id":"themes/ocean/source/css/tranquilheart.css","hash":"9c669545e3517de77f5cff50a58e4ef035855c87","modified":1557474779374},{"_id":"themes/ocean/source/css/style.styl","hash":"c16117215595ef87c0e88bbb2bcd7b0d1f02290c","modified":1557641211730},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.css","hash":"2e6a66987dbc7a57bbfd2655bce166739b4ba426","modified":1556351608545},{"_id":"themes/ocean/source/images/.DS_Store","hash":"758d8ae8486197e8ad1b0dc53b5b7ce7ad19c6db","modified":1556701955107},{"_id":"themes/ocean/source/images/hexo.svg","hash":"71e7204d04ccfe260f06ea5873484791cd5f404a","modified":1556351608549},{"_id":"themes/ocean/source/images/hexo-inverted.svg","hash":"525309ea3c7360f83d1d9df6d04c256d7171950d","modified":1556351608548},{"_id":"themes/ocean/source/js/busuanzi-2.3.pure.min.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1556351608607},{"_id":"themes/ocean/source/js/lazyload.min.js","hash":"b801b3946fb9b72e03512c0663458e140e1fa77b","modified":1556351608608},{"_id":"themes/ocean/source/js/ocean.js","hash":"3457be62843930ad58997cd6fd387783285242c7","modified":1556351608609},{"_id":"themes/ocean/source/js/pace.min.js","hash":"d32ab818e0f97d3b0c80f5631fc23d8a0cb52795","modified":1556351608609},{"_id":"themes/ocean/source/js/search.js","hash":"88fa5c780f9093f70d6e3066cca0d6165a8364b4","modified":1556683974980},{"_id":"themes/ocean/source/js/prettify.js","hash":"66f5fafc7321fcb0864a8dc680b32cb1d40ac57b","modified":1362427944000},{"_id":"source/_posts/Implement-Zero-Data-Loss-in-Spark-Streaming/Spark-Streaming-flow-for-offsets.JPG","hash":"82f960e5571b62948edb9b71bdef88d0b30b94d2","modified":1558763237533},{"_id":"themes/ocean/source/css/_partial/comment.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1556351608536},{"_id":"themes/ocean/source/fancybox/jquery.fancybox.min.js","hash":"b2b093d8f5ffeee250c8d0d3a2285a213318e4ea","modified":1556351608546},{"_id":"themes/ocean/source/js/jquery-2.0.3.min.js","hash":"800edb7787c30f4982bf38f2cb8f4f6fb61340e9","modified":1556351608608},{"_id":"themes/ocean/source/images/searchfooter.png","hash":"519b76e799d2a45a456c3a90fb1308cdb011b352","modified":1556683156662},{"_id":"source/_posts/Suffix-Tree/Banana_SuffixTree.JPG","hash":"784d221ca8ad1a53104ac3543430f1a0c5a425fe","modified":1563798870918},{"_id":"source/_posts/Suffix-Tree/Compressed_PrefixTree.JPG","hash":"86b37ab22ada28eb869e02db63f550f5203be62c","modified":1563797134664},{"_id":"source/_posts/Implement-Zero-Data-Loss-in-Spark-Streaming/wal_spark.JPG","hash":"3feb86073a04c5b3ac86d535073fc38570838e42","modified":1558760012914},{"_id":"themes/ocean/layout/_partial/post/albums.ejs","hash":"dcfea9a328f5e1d90758ac71d7d7555b31b93bcb","modified":1556351608525},{"_id":"themes/ocean/layout/_partial/post/busuanzi.ejs","hash":"88462d160479cc3f0cc58efcd888fbaf22b0d4d8","modified":1556351608525},{"_id":"themes/ocean/layout/_partial/post/category.ejs","hash":"85f0ebeceee1c32623bfa1e4170dbe1e34442fea","modified":1556351608525},{"_id":"themes/ocean/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1556351608526},{"_id":"themes/ocean/layout/_partial/post/gallery.ejs","hash":"5f8487fe7bed9a09001c6655244ff35f583cf1eb","modified":1556351608526},{"_id":"themes/ocean/layout/_partial/post/gitalk.ejs","hash":"e36d149ad83c3a52562dbef61a0083957eb24578","modified":1556351608526},{"_id":"themes/ocean/layout/_partial/post/nav.ejs","hash":"e59198918e92ef92156aeefbf6023584ac1cae64","modified":1556351608527},{"_id":"themes/ocean/layout/_partial/post/search.ejs","hash":"2c9d19d1685e834aa2020998da2a2d259ce9b9ff","modified":1556351608527},{"_id":"themes/ocean/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1556351608527},{"_id":"themes/ocean/layout/_partial/post/title.ejs","hash":"f8c9cb35d8d1975aa3b457d9a92f38c462e97732","modified":1556351608527},{"_id":"themes/ocean/source/css/_partial/.DS_Store","hash":"7e82340cd372744d424f18f0b7b25c3c2d9f21fe","modified":1557473001675},{"_id":"themes/ocean/source/css/_partial/albums.styl","hash":"0659d5f7469f24a415354ff767d949926465d515","modified":1556351608535},{"_id":"themes/ocean/source/css/_partial/archive.styl","hash":"8aefdcf2d542ad839018c2c58511e3318a38490d","modified":1556351608536},{"_id":"themes/ocean/source/css/_partial/float.styl","hash":"d888df89a172e4c8119cb8740fc1eae1a9539157","modified":1556351608537},{"_id":"themes/ocean/source/css/_partial/article.styl","hash":"93905de0339f3e831a383739bdc477c29c1914c4","modified":1556351608536},{"_id":"themes/ocean/source/css/_partial/articles.styl","hash":"7bf289013d304505984b251be725b49165a694fd","modified":1556351608536},{"_id":"themes/ocean/source/css/_partial/footer.styl","hash":"24779cbce1012d4f35ffc6b3ec0830cbc2ea3b3f","modified":1556351608537},{"_id":"themes/ocean/source/css/_partial/gallery.styl","hash":"7bdc2c9fb4971dbd7511c5cbb69bd611f20db591","modified":1556351608537},{"_id":"themes/ocean/source/css/_partial/gitalk.styl","hash":"3706eef2e0541493f1679a30241d279e29dfdc17","modified":1556351608537},{"_id":"themes/ocean/source/css/_partial/highlight.styl","hash":"c6e99fd23056fb01177aeefbc5dd4a8e88cf8f81","modified":1556351608538},{"_id":"themes/ocean/source/css/_partial/layou.styl","hash":"47a8a98aaaf7db4d2d89b8c41b43394d1cc92849","modified":1556351608538},{"_id":"themes/ocean/source/css/_partial/lists.styl","hash":"087f08e0ce9aca48e096dabca6eed2368b5bcd6b","modified":1556351608538},{"_id":"themes/ocean/source/css/_partial/mobile.styl","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1556351608539},{"_id":"themes/ocean/source/css/_partial/navbar.styl","hash":"fb32443da45975567ebae8683c18f0cda0aa0a3d","modified":1556351608539},{"_id":"themes/ocean/source/css/_partial/ocean.styl","hash":"6c68a00bcd69853711db48f5fdd02544d8a6152e","modified":1556351608539},{"_id":"themes/ocean/source/css/_partial/pace.styl","hash":"e326918ba276ee332d0598d8193ccd8353e7d916","modified":1556351608540},{"_id":"themes/ocean/source/css/_partial/search.styl","hash":"a81fe253dae61b114d9cdb15673d1588aae35285","modified":1556513307655},{"_id":"themes/ocean/source/css/_partial/sidebar.styl","hash":"600c70f1de82da5223af290d47a583f9c379d188","modified":1556351608540},{"_id":"themes/ocean/source/css/_partial/totop.styl","hash":"69fcb0c9adb45f592838c3babc58d3490f413db2","modified":1556351608541},{"_id":"themes/ocean/source/css/_partial/prettify.css","hash":"d920b34adf4458bfc13937cbd083f4ca04341e8a","modified":1362427944000},{"_id":"themes/ocean/source/css/feathericon/feathericon.ttf","hash":"d0d80c3c960d7d45e6bd7fa428d8a6a8c8245b2d","modified":1556351608543},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff","hash":"d22fe861e47afd92969ab46c7cbb7ea9c225aaf8","modified":1556351608544},{"_id":"themes/ocean/source/css/feathericon/feathericon.woff2","hash":"2c11c45331d914ee38ad42ccf966132a508b5596","modified":1556351608544},{"_id":"themes/ocean/source/images/ocean/overlay-hero.png","hash":"92481a1848c35be96a693af11f77265323a7c189","modified":1556351608606},{"_id":"source/images/IMG_2148.JPG","hash":"9f08338b4842b7853a7132a098c0d2440afc8fd8","modified":1556356922750},{"_id":"source/images/IMG_1891.JPG","hash":"676dcaf369929ef6b3d9ec8e8f7cf088c8ecdae4","modified":1556357445197},{"_id":"source/images/futako010010.JPG","hash":"6f13fba723f69932261d8db10e2af818aa52dac4","modified":1556356904280},{"_id":"themes/ocean/source/css/feathericon/feathericon.eot","hash":"e2a01ae6f849841bc7a9fd21e5b7b450f1ded19b","modified":1556351608542},{"_id":"themes/ocean/source/css/feathericon/feathericon.svg","hash":"c113006c6822451802c8457128c352c0e4934453","modified":1556351608543},{"_id":"source/images/IMG_0874.JPG","hash":"09ef9b6fe4b1f11b2adc52865f98a66f79bdbe58","modified":1556357450715},{"_id":"source/_posts/Suffix-Tree/Banana_PrefixTree.JPG","hash":"b91a80fff213bea39331878b1e7ffa146636c7ab","modified":1563798591382},{"_id":"source/images/IMG_4201.JPG","hash":"df6d4734798dbbe1b22e82864ab38739fecce8d3","modified":1556357442956},{"_id":"source/images/IMG_4200.JPG","hash":"b66b09b2026648ca9bff1b9d7f3adbbe0f5a0413","modified":1556357449003},{"_id":"source/images/prefixTree.JPG","hash":"d32a29f0a1bbb54b2dcf0834a1a9f1b38ee7c4ad","modified":1563459006837},{"_id":"source/images/IMG_0444.JPG","hash":"a07f11160a4ff6ebfdbc4faf25868ea8c3f18b71","modified":1556356906367},{"_id":"source/images/IMG_0875.JPG","hash":"de4c08db2a5751996b99fbc8dfded7c53da935f9","modified":1556357449935},{"_id":"source/images/IMG_1333.JPG","hash":"176423f03fdb3bf5e56643027cf01005a5eb2a74","modified":1556357447915},{"_id":"themes/ocean/source/images/forrestgump.png","hash":"18ad6a8ba815878e36a0d5562136dc4fb8920c12","modified":1556351608548},{"_id":"source/images/IMG_0518.JPG","hash":"cdf83313366ef6bfedc05f8127d08e4897de5154","modified":1556355555852},{"_id":"source/images/IMG_0226.JPG","hash":"08238c849695d9214b2f5b8d807a9f5c22d805cd","modified":1556356907611},{"_id":"source/images/IMG_2194.JPG","hash":"cb802e60eb35ea0e5135c786cef1608b1f32f1c0","modified":1556356920498},{"_id":"source/images/IMG_2221.JPG","hash":"707dfcc82b32818ff8b1d12840a863bca517cd16","modified":1556356919011},{"_id":"source/images/IMG_2151.JPG","hash":"a22eccebcc934cb9ce8e4038fee0d39d0469f1a2","modified":1556357444520},{"_id":"source/images/IMG_2192.JPG","hash":"e725c4037834f60d40b777244edbeb6de337ad9a","modified":1556356911364},{"_id":"source/images/IMG_0222.JPG","hash":"8424385dc15a5f8069190386292cce0fbfdc086f","modified":1556356908641},{"_id":"source/images/IMG_0637.JPG","hash":"bd5f1d97a0be08a89769b01429a35465e0fc85d7","modified":1556355551809},{"_id":"source/images/IMG_2209.JPG","hash":"0045c460208e70189907588d74542e7dd7a9537b","modified":1556356917888},{"_id":"themes/ocean/source/images/ocean/ocean.ogv","hash":"9c6b5d6b0544472cee39f5eafac2d5cbba5fd86b","modified":1556351608579},{"_id":"source/images/IMG_0873.JPG","hash":"e4754500a08ed9a78e9154e67c67eb61daa9d3ab","modified":1556357452650},{"_id":"source/images/IMG_1331.JPG","hash":"1274e2476d4aaa0b24eb6330cdf87e238595c0f2","modified":1556357446640},{"_id":"source/images/futako010029.JPG","hash":"8438933ed62e1cf5b2db488140fb3d93d6bae0f5","modified":1556356902207},{"_id":"source/images/IMG_2195.JPG","hash":"dbf948d502538248eef8e73df5418530b8cc891c","modified":1556356912377},{"_id":"source/images/IMG_0077.JPG","hash":"33b8f580dd1c24eaf7e2c2ed42958638637da02a","modified":1556356909567},{"_id":"source/images/IMG_0605.JPG","hash":"fe5cfd86c45ddba8ac06c7834deb3acb0ca21ba0","modified":1556355554106},{"_id":"source/images/futako010013.JPG","hash":"d2bc5d03172a0664ae145288257d1dac48047cb5","modified":1556356901206},{"_id":"source/images/IMG_0519.JPG","hash":"af2e9071c8f319b00bf4a84ff6b78e2d413c46b6","modified":1556355557862},{"_id":"source/images/IMG_2200.JPG","hash":"6741848ea3e666881f9dc9a31efa817207a7a1c2","modified":1556356921703},{"_id":"source/images/futako010041.JPG","hash":"06474d6fa82c7e0183f90fb6a1660280a87f2d89","modified":1556356903293},{"_id":"themes/ocean/source/images/ocean/ocean.png","hash":"8245d07f812625d19b48ad2d00f8191f2aa4d304","modified":1556351608586},{"_id":"themes/ocean/source/images/ocean/ocean.webm","hash":"65aa2b6483e0151611899e31571057334c60d9e4","modified":1556351608604},{"_id":"themes/ocean/source/images/ocean/ocean.mp4","hash":"1e89cac2d652005d9dafd3ecb4dd460a8ff6d6af","modified":1556351608572},{"_id":"public/atom.xml","hash":"8b52f928f956d08c3e4019d0dee0061070808b03","modified":1563798960689},{"_id":"public/search.xml","hash":"f0a86a4df1585758307617cd5d0b7806d27cac9a","modified":1563798960692},{"_id":"public/about/index.html","hash":"1e47e5d15ac8e1ce03f936607a7e6225c470190e","modified":1563798960900},{"_id":"public/2019/07/18/Prefix-Tree/index.html","hash":"e79a611d904a90595176db2deee5e8043d5f24cb","modified":1563798960901},{"_id":"public/2019/05/22/如何用VS-Code免翻墙听网易云音乐/index.html","hash":"580b034a6dbf1c3fe90b0047024984d64d10d99f","modified":1563798960916},{"_id":"public/2019/05/12/Type-slice-in-Golang/index.html","hash":"73685d41aaebecfbc6e0f8e7fd8fb89db9b3fb7d","modified":1563798960920},{"_id":"public/2019/05/08/How-to-debug-NodeJS-on-VS-Code/index.html","hash":"e15ebe4ebe2f624c8766723d8dec58af80237092","modified":1563798960920},{"_id":"public/2019/05/06/How-to-Check-Open-TCP-IP-Ports-in-Mac-OS-X/index.html","hash":"78d013f72275894be3720a41324f2d9f55a493e1","modified":1563798960920},{"_id":"public/2019/05/05/Memory-Leak-in-Serveral-Commonly-Used-Programming-Languages/index.html","hash":"6d438803c120d5c1193bc6a1145d90823cc58f80","modified":1563798960920},{"_id":"public/2019/04/30/Prefix-Notation/index.html","hash":"839e72b5e105f0cd3519a13305de9be283b31577","modified":1563798960920},{"_id":"public/2019/04/27/hello-world/index.html","hash":"50b376c24628a74e272b709412596c3253371d5a","modified":1563798960920},{"_id":"public/archives/index.html","hash":"84a0b4e38822fad598f3c792daae91a8c3c86641","modified":1563798960920},{"_id":"public/archives/2019/index.html","hash":"bdafd3547b230ed887c3f2d3f475d4b06059e92e","modified":1563798960920},{"_id":"public/archives/2019/04/index.html","hash":"430b1da1587a68877cf6fff0917f943e31533169","modified":1563798960920},{"_id":"public/archives/2019/05/index.html","hash":"e5dd5ff291aa465bf10dd3b5d1f8909e9095c661","modified":1563798960920},{"_id":"public/archives/2019/07/index.html","hash":"b733fb276fa111b8647da5fdfe1c1ca1a1f1c50f","modified":1563798960920},{"_id":"public/tags/CLI/index.html","hash":"90e53a734c270db93b92b759c3c86380e91cc267","modified":1563798960921},{"_id":"public/tags/Mac-OS/index.html","hash":"a6f06d7dfade3f2e189839909376b34b196ee734","modified":1563798960921},{"_id":"public/tags/port/index.html","hash":"638b0d85476282d1d8679572cae51bb774fee768","modified":1563798960921},{"_id":"public/tags/VS-Code/index.html","hash":"62f6f525cfa514b0ad2f6bd8fbbe0da2aa01ee58","modified":1563798960921},{"_id":"public/tags/NodeJS/index.html","hash":"42326ee666044bb5c6d9567443db9c3f39883f0a","modified":1563798960921},{"_id":"public/tags/Prefix-Tree/index.html","hash":"962c5067aecef64294a7b6b8861135ef65b65399","modified":1563798960921},{"_id":"public/tags/Lisp/index.html","hash":"5f908517e00c344822c4247ab43fa8359bcc2950","modified":1563798960921},{"_id":"public/tags/Scheme/index.html","hash":"ac6f34b4b2ce5c6110a922c5a34780600497e595","modified":1563798960921},{"_id":"public/tags/Prefix-Notation/index.html","hash":"dd181c6c11de63a9dd511da82743687d13b69312","modified":1563798960921},{"_id":"public/tags/Functional-Programming/index.html","hash":"c0e8ec462fcdedd85f0b65aba140747ad9d2264d","modified":1563798960921},{"_id":"public/tags/Golang/index.html","hash":"9bc22b64801cee997807ede43cef141d43decd4b","modified":1563798960921},{"_id":"public/tags/Array/index.html","hash":"f79b0e54ad36a3ef8e48f580b5419ca3b21c7f76","modified":1563798960921},{"_id":"public/tags/Slice/index.html","hash":"1f2964c90eab904f9d87532ba203395cca9dc327","modified":1563798960921},{"_id":"public/tags/NeteaseMusic/index.html","hash":"3d22c3cd89332b415704fe70af2826880d57b8d6","modified":1563798960921},{"_id":"public/tags/网易云/index.html","hash":"93a0b606f9066dfbfefe0631b936bf2d1cbe0cd1","modified":1563798960921},{"_id":"public/tags/Redux/index.html","hash":"11b859a346bbc9b9a53bf196085dd233c9456c47","modified":1563798960922},{"_id":"public/tags/Saga/index.html","hash":"249ea79e6f5cf1acdf6dd533c8a3640d1a980152","modified":1563798960922},{"_id":"public/tags/React/index.html","hash":"d4466be14a30c44a8ae547fd336c2a04b2901412","modified":1563798960922},{"_id":"public/tags/Spark/index.html","hash":"02ecf4b1cfeda9553c51a056ef1aeeda60e7c53b","modified":1563798960922},{"_id":"public/tags/Kafka/index.html","hash":"d751328a203f6dba4990b3ef2af60402861d6dc9","modified":1563798960922},{"_id":"public/tags/C/index.html","hash":"04106198dc78be56f3ba770ca7df30e94eec2d0a","modified":1563798960922},{"_id":"public/tags/Java/index.html","hash":"89dcbed938c4eabcf3b91324b2cc09d06a807f5a","modified":1563798960922},{"_id":"public/tags/Python/index.html","hash":"88ecdefe581833adec83943e21d30cf9e8aec65e","modified":1563798960922},{"_id":"public/gallery/index.html","hash":"aae9f6454980ed42436a9d06d2ec6cec53a6d386","modified":1563798960922},{"_id":"public/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/index.html","hash":"49e056618b34382dae2df0d50c90d5e7077703a4","modified":1563798960922},{"_id":"public/2019/04/28/A-Brief-Introduce-to-Redux-Saga/index.html","hash":"5400587779c358cae94e995845b0e2eb88fba0b2","modified":1563798960922},{"_id":"public/index.html","hash":"e17675a79b39228dadb62408150997fbdea71615","modified":1563798960922},{"_id":"public/2019/07/22/Suffix-Tree/index.html","hash":"14ef079a6fbb918badafd1f3b25dee5d53ff1e9b","modified":1563798960937},{"_id":"public/archives/page/2/index.html","hash":"07b96acbfafbac073fa2c8df11c109a8a149aa7e","modified":1563798960937},{"_id":"public/archives/2019/page/2/index.html","hash":"0faca2efe2b48edc52f1977b8b1486554341b98e","modified":1563798960937},{"_id":"public/page/2/index.html","hash":"fe81d4c734fffb9e286e9b4055cbac790c9e3211","modified":1563798960937},{"_id":"public/tags/Suffix-Tree/index.html","hash":"773f84ca66943025c737aa85cd9dcea66890e61f","modified":1563798960937},{"_id":"public/images/cli.JPG","hash":"9dc4dd69083df96b774dee488bb42cfe7dc35922","modified":1563798960953},{"_id":"public/images/redux-saga.JPG","hash":"246254876cfd63cdbb5d31523d02d078daecf40e","modified":1563798960953},{"_id":"public/images/neteaseMusic.JPG","hash":"9a9ce2438064e78c0c3c596d22e879fad40727dd","modified":1563798960953},{"_id":"public/images/spark_kafka.JPG","hash":"aadd0a222fd5fd046ec3c68b8a914080a70bf3b1","modified":1563798960954},{"_id":"public/images/vscode.JPG","hash":"7d97031e400f3b8ad192de733d9711f52a7f80ad","modified":1563798960954},{"_id":"public/images/treeRecursion.JPG","hash":"a09da0ab4779d513b081eeacff2ec0a80d78576a","modified":1563798960954},{"_id":"public/images/treeRecursion11.JPG","hash":"b3cb61f46ddff0067dc8ebe5357d21a37aeaecb7","modified":1563798960954},{"_id":"public/favicon.ico","hash":"0f20298a6a4d1ebd7a7ae7b87d7a3ae9afec0623","modified":1563798960954},{"_id":"public/images/hexo.svg","hash":"71e7204d04ccfe260f06ea5873484791cd5f404a","modified":1563798960954},{"_id":"public/images/hexo-inverted.svg","hash":"525309ea3c7360f83d1d9df6d04c256d7171950d","modified":1563798960954},{"_id":"public/images/searchfooter.png","hash":"519b76e799d2a45a456c3a90fb1308cdb011b352","modified":1563798960954},{"_id":"public/css/feathericon/feathericon.ttf","hash":"d0d80c3c960d7d45e6bd7fa428d8a6a8c8245b2d","modified":1563798960954},{"_id":"public/css/feathericon/feathericon.woff","hash":"d22fe861e47afd92969ab46c7cbb7ea9c225aaf8","modified":1563798960954},{"_id":"public/css/feathericon/feathericon.woff2","hash":"2c11c45331d914ee38ad42ccf966132a508b5596","modified":1563798960954},{"_id":"public/images/ocean/overlay-hero.png","hash":"92481a1848c35be96a693af11f77265323a7c189","modified":1563798960955},{"_id":"public/css/feathericon/feathericon.eot","hash":"e2a01ae6f849841bc7a9fd21e5b7b450f1ded19b","modified":1563798960955},{"_id":"public/2019/07/18/Prefix-Tree/prefix_tree.JPG","hash":"22e46566e03150b74cf2dee5dd95b77dbae76687","modified":1563798960955},{"_id":"public/live2dw/assets/assets/koharu.model.json","hash":"ceccdefd776b7c9475a29cff0842796e4f58b7e9","modified":1563798960955},{"_id":"public/live2dw/assets/assets/koharu.physics.json","hash":"2fbf886979212357ba293bd35884f2cb5b26b6a6","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/01.mtn","hash":"61d7d590d9feb71b32fd6bd142b59410d75bc1fa","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/02.mtn","hash":"efc99efdff39c93372cff0f6d62c4e748e1a5593","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/04.mtn","hash":"32c888667455a3ff6f1b04f910c1a5cc4de30af0","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/03.mtn","hash":"a72b697a92a7cff40d15774b143b465b34cee5e6","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/05.mtn","hash":"637e00442da4042cd4b0ed2cc62ffb1559881814","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/06.mtn","hash":"df10cc1d333c96da1296a4853c1ddbd44d8a11f3","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/08.mtn","hash":"9b95ef8548b979d1fca557c74f8d66fb15b34578","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/07.mtn","hash":"d8c9410135c81604eba665b59808089808e0851a","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/09.mtn","hash":"ecf1283b72e1c4b7e3a97343cd97726813f18790","modified":1563798960955},{"_id":"public/live2dw/lib/L2Dwidget.min.js","hash":"5f1a807437cc723bcadc3791d37add5ceed566a2","modified":1563798960955},{"_id":"public/live2dw/assets/assets/mtn/idle.mtn","hash":"058d4628ab04bf42c279501ba4fa37116d384e41","modified":1563798960956},{"_id":"public/live2dw/assets/package.json","hash":"e86bf8d88b361e79270a918e095a0a1ee5adef8c","modified":1563798960956},{"_id":"public/2019/07/22/Suffix-Tree/prefix_tree.JPG","hash":"22e46566e03150b74cf2dee5dd95b77dbae76687","modified":1563798960956},{"_id":"public/2019/07/22/Suffix-Tree/PrefixTree_StringProblem.JPG","hash":"5dfbc9a3742fe8d6104c4ae4897888fe907b8c18","modified":1563798960956},{"_id":"public/images/Memory-Leaks.JPG","hash":"88744198f42bcd6b31e70e82f798645cb453e2e1","modified":1563798961387},{"_id":"public/images/lisp.JPG","hash":"5f09d56757170819589f35eed3cb8397dea6c222","modified":1563798961388},{"_id":"public/css/feathericon/feathericon.svg","hash":"c113006c6822451802c8457128c352c0e4934453","modified":1563798961390},{"_id":"public/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/Spark-Streaming-flow-for-offsets.JPG","hash":"82f960e5571b62948edb9b71bdef88d0b30b94d2","modified":1563798961390},{"_id":"public/live2dw/lib/L2Dwidget.min.js.map","hash":"3290fe2df45f065b51a1cd7b24ec325cbf9bb5ce","modified":1563798961390},{"_id":"public/2019/07/22/Suffix-Tree/Banana_SuffixTree.JPG","hash":"784d221ca8ad1a53104ac3543430f1a0c5a425fe","modified":1563798961390},{"_id":"public/css/404.css","hash":"dedb3146be1f9955c1d915afd505b2778912c06c","modified":1563798961440},{"_id":"public/css/tranquilheart.css","hash":"9c669545e3517de77f5cff50a58e4ef035855c87","modified":1563798961440},{"_id":"public/js/busuanzi-2.3.pure.min.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1563798961440},{"_id":"public/js/lazyload.min.js","hash":"b801b3946fb9b72e03512c0663458e140e1fa77b","modified":1563798961441},{"_id":"public/js/ocean.js","hash":"3457be62843930ad58997cd6fd387783285242c7","modified":1563798961441},{"_id":"public/js/search.js","hash":"88fa5c780f9093f70d6e3066cca0d6165a8364b4","modified":1563798961441},{"_id":"public/404.html","hash":"f3d2a3f7d02edbd25f1e7b79cfc65813b2665386","modified":1563798961441},{"_id":"public/css/style.css","hash":"b90ab4883d20ccc31408cff32573d6c1edcbe4b0","modified":1563798961441},{"_id":"public/images/IMG_0504.JPG","hash":"bdd3c4dcc7e813685793a2226728b0c60fb653b9","modified":1563798961441},{"_id":"public/images/IMG_0606.JPG","hash":"1aa6473addf50803fef60b42e90cdb2d71113db8","modified":1563798961441},{"_id":"public/live2dw/lib/L2Dwidget.0.min.js","hash":"35bb5b588b6de25c9be2dd51d3fd331feafac02d","modified":1563798961441},{"_id":"public/2019/07/22/Suffix-Tree/Compressed_PrefixTree.JPG","hash":"86b37ab22ada28eb869e02db63f550f5203be62c","modified":1563798961441},{"_id":"public/fancybox/jquery.fancybox.min.css","hash":"2e6a66987dbc7a57bbfd2655bce166739b4ba426","modified":1563798961458},{"_id":"public/js/pace.min.js","hash":"d32ab818e0f97d3b0c80f5631fc23d8a0cb52795","modified":1563798961458},{"_id":"public/images/GoSlice.JPG","hash":"920bdae1c37b54a35358ce3ed70ab278b8b96a7c","modified":1563798961459},{"_id":"public/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/wal_spark.JPG","hash":"3feb86073a04c5b3ac86d535073fc38570838e42","modified":1563798961462},{"_id":"public/live2dw/assets/assets/moc/koharu.moc","hash":"5eec3fba21444dd6f774b913510b5955e2c0605b","modified":1563798961462},{"_id":"public/images/forrestgump.png","hash":"18ad6a8ba815878e36a0d5562136dc4fb8920c12","modified":1563798961494},{"_id":"public/live2dw/assets/assets/moc/koharu.2048/texture_00.png","hash":"0879b61b745084781722636bba9f278f31ce5fc1","modified":1563798961494},{"_id":"public/2019/07/22/Suffix-Tree/Banana_PrefixTree.JPG","hash":"b91a80fff213bea39331878b1e7ffa146636c7ab","modified":1563798961495},{"_id":"public/images/futako010010.JPG","hash":"6f13fba723f69932261d8db10e2af818aa52dac4","modified":1563798961548},{"_id":"public/images/IMG_4200.JPG","hash":"b66b09b2026648ca9bff1b9d7f3adbbe0f5a0413","modified":1563798961548},{"_id":"public/images/IMG_1891.JPG","hash":"676dcaf369929ef6b3d9ec8e8f7cf088c8ecdae4","modified":1563798961584},{"_id":"public/images/IMG_0874.JPG","hash":"09ef9b6fe4b1f11b2adc52865f98a66f79bdbe58","modified":1563798961584},{"_id":"public/images/IMG_0444.JPG","hash":"a07f11160a4ff6ebfdbc4faf25868ea8c3f18b71","modified":1563798961585},{"_id":"public/images/ocean/ocean.ogv","hash":"9c6b5d6b0544472cee39f5eafac2d5cbba5fd86b","modified":1563798961586},{"_id":"public/images/prefixTree.JPG","hash":"d32a29f0a1bbb54b2dcf0834a1a9f1b38ee7c4ad","modified":1563798961600},{"_id":"public/images/IMG_4201.JPG","hash":"df6d4734798dbbe1b22e82864ab38739fecce8d3","modified":1563798961600},{"_id":"public/js/prettify.js","hash":"66f5fafc7321fcb0864a8dc680b32cb1d40ac57b","modified":1563798961609},{"_id":"public/images/IMG_0226.JPG","hash":"08238c849695d9214b2f5b8d807a9f5c22d805cd","modified":1563798961609},{"_id":"public/images/IMG_2194.JPG","hash":"cb802e60eb35ea0e5135c786cef1608b1f32f1c0","modified":1563798961612},{"_id":"public/images/IMG_2221.JPG","hash":"707dfcc82b32818ff8b1d12840a863bca517cd16","modified":1563798961612},{"_id":"public/live2dw/lib/L2Dwidget.0.min.js.map","hash":"35e71cc2a130199efb167b9a06939576602f0d75","modified":1563798961613},{"_id":"public/fancybox/jquery.fancybox.min.js","hash":"b2b093d8f5ffeee250c8d0d3a2285a213318e4ea","modified":1563798961624},{"_id":"public/images/IMG_2148.JPG","hash":"9f08338b4842b7853a7132a098c0d2440afc8fd8","modified":1563798961624},{"_id":"public/images/IMG_0875.JPG","hash":"de4c08db2a5751996b99fbc8dfded7c53da935f9","modified":1563798961624},{"_id":"public/images/IMG_2151.JPG","hash":"a22eccebcc934cb9ce8e4038fee0d39d0469f1a2","modified":1563798961624},{"_id":"public/images/IMG_0637.JPG","hash":"bd5f1d97a0be08a89769b01429a35465e0fc85d7","modified":1563798961625},{"_id":"public/images/IMG_1333.JPG","hash":"176423f03fdb3bf5e56643027cf01005a5eb2a74","modified":1563798961632},{"_id":"public/images/IMG_0518.JPG","hash":"cdf83313366ef6bfedc05f8127d08e4897de5154","modified":1563798961633},{"_id":"public/images/IMG_0222.JPG","hash":"8424385dc15a5f8069190386292cce0fbfdc086f","modified":1563798961634},{"_id":"public/js/jquery-2.0.3.min.js","hash":"800edb7787c30f4982bf38f2cb8f4f6fb61340e9","modified":1563798961648},{"_id":"public/images/IMG_2192.JPG","hash":"e725c4037834f60d40b777244edbeb6de337ad9a","modified":1563798961649},{"_id":"public/images/IMG_2195.JPG","hash":"dbf948d502538248eef8e73df5418530b8cc891c","modified":1563798961668},{"_id":"public/images/IMG_2209.JPG","hash":"0045c460208e70189907588d74542e7dd7a9537b","modified":1563798961668},{"_id":"public/images/IMG_0873.JPG","hash":"e4754500a08ed9a78e9154e67c67eb61daa9d3ab","modified":1563798961682},{"_id":"public/images/IMG_1331.JPG","hash":"1274e2476d4aaa0b24eb6330cdf87e238595c0f2","modified":1563798961682},{"_id":"public/images/futako010029.JPG","hash":"8438933ed62e1cf5b2db488140fb3d93d6bae0f5","modified":1563798961684},{"_id":"public/images/ocean/ocean.png","hash":"8245d07f812625d19b48ad2d00f8191f2aa4d304","modified":1563798961698},{"_id":"public/images/futako010013.JPG","hash":"d2bc5d03172a0664ae145288257d1dac48047cb5","modified":1563798961703},{"_id":"public/images/IMG_0077.JPG","hash":"33b8f580dd1c24eaf7e2c2ed42958638637da02a","modified":1563798961708},{"_id":"public/images/IMG_0605.JPG","hash":"fe5cfd86c45ddba8ac06c7834deb3acb0ca21ba0","modified":1563798961708},{"_id":"public/images/IMG_2200.JPG","hash":"6741848ea3e666881f9dc9a31efa817207a7a1c2","modified":1563798961710},{"_id":"public/images/IMG_0519.JPG","hash":"af2e9071c8f319b00bf4a84ff6b78e2d413c46b6","modified":1563798961720},{"_id":"public/images/futako010041.JPG","hash":"06474d6fa82c7e0183f90fb6a1660280a87f2d89","modified":1563798961728},{"_id":"public/images/ocean/ocean.webm","hash":"65aa2b6483e0151611899e31571057334c60d9e4","modified":1563798961769},{"_id":"public/images/ocean/ocean.mp4","hash":"1e89cac2d652005d9dafd3ecb4dd460a8ff6d6af","modified":1563798961787}],"Category":[],"Data":[],"Page":[{"title":"About Me","date":"2019-04-27T11:24:18.000Z","_content":"","source":"about/index.md","raw":"---\ntitle: About Me\ndate: 2019-04-27 20:24:18\n---\n","updated":"2019-04-27T11:24:56.199Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjyedivug0001t8aoq80eat12","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"My Gallery","date":"2019-04-27T08:49:44.000Z","albums":[["../images/IMG_0504.JPG","Nikko, JP"],["../images/IMG_0518.JPG","Meguro, JP"],["../images/IMG_0519.JPG","Meguro, JP"],["../images/IMG_0605.JPG","Kyoto Yasaka Shrine, JP"],["../images/IMG_0606.JPG","Kyoto Kiyomizu, JP"],["../images/IMG_0637.JPG","Odaiba, JP"],["../images/futako010010.JPG","Futakotawagama, JP"],["../images/futako010013.JPG","Futakotawagama, JP"],["../images/futako010029.JPG","Futakotawagama, JP"],["../images/futako010041.JPG","Futakotawagama, JP"],["../images/IMG_0077.JPG","Kamakura, JP"],["../images/IMG_0222.JPG","Roppongi Hills, JP"],["../images/IMG_0226.JPG","Roppongi Hills, JP"],["../images/IMG_0444.JPG","Nara, JP"],["../images/IMG_2148.JPG","Niagara Falls, CA"],["../images/IMG_2192.JPG","Banff National Parks, CA"],["../images/IMG_2194.JPG","Banff National Parks, CA"],["../images/IMG_2195.JPG","Banff National Parks, CA"],["../images/IMG_2200.JPG","Banff National Parks, CA"],["../images/IMG_2209.JPG","Banff Town, CA"],["../images/IMG_2221.JPG","Banff, Lake Louis, CA"],["../images/IMG_4200.JPG","Waterloo, CA"],["../images/IMG_4201.JPG","Niagara Falls, CA"],["../images/IMG_1331.JPG","Waterloo, CA"],["../images/IMG_1333.JPG","Waterloo, CA"],["../images/IMG_2151.JPG","Niagara Falls, CA"],["../images/IMG_1891.JPG","Montreal, CA"],["../images/IMG_0873.JPG","Dali, CN"],["../images/IMG_0874.JPG","Dali, CN"],["../images/IMG_0875.JPG","Dali, CN"]],"_content":"","source":"gallery/index.md","raw":"---\ntitle: My Gallery\ndate: 2019-04-27 17:49:44\nalbums: [\n    [\"../images/IMG_0504.JPG\", \"Nikko, JP\"],\n    [\"../images/IMG_0518.JPG\", \"Meguro, JP\"],\n    [\"../images/IMG_0519.JPG\", \"Meguro, JP\"],\n    [\"../images/IMG_0605.JPG\", \"Kyoto Yasaka Shrine, JP\"],\n    [\"../images/IMG_0606.JPG\", \"Kyoto Kiyomizu, JP\"],\n    [\"../images/IMG_0637.JPG\", \"Odaiba, JP\"],\n    [\"../images/futako010010.JPG\", \"Futakotawagama, JP\"],\n    [\"../images/futako010013.JPG\", \"Futakotawagama, JP\"],\n    [\"../images/futako010029.JPG\", \"Futakotawagama, JP\"],\n    [\"../images/futako010041.JPG\", \"Futakotawagama, JP\"],\n    [\"../images/IMG_0077.JPG\", \"Kamakura, JP\"],\n    [\"../images/IMG_0222.JPG\", \"Roppongi Hills, JP\"],\n    [\"../images/IMG_0226.JPG\", \"Roppongi Hills, JP\"],\n    [\"../images/IMG_0444.JPG\", \"Nara, JP\"],\n    [\"../images/IMG_2148.JPG\", \"Niagara Falls, CA\"],\n    [\"../images/IMG_2192.JPG\", \"Banff National Parks, CA\"],\n    [\"../images/IMG_2194.JPG\", \"Banff National Parks, CA\"],\n    [\"../images/IMG_2195.JPG\", \"Banff National Parks, CA\"],\n    [\"../images/IMG_2200.JPG\", \"Banff National Parks, CA\"],\n    [\"../images/IMG_2209.JPG\", \"Banff Town, CA\"],\n    [\"../images/IMG_2221.JPG\", \"Banff, Lake Louis, CA\"],\n    [\"../images/IMG_4200.JPG\", \"Waterloo, CA\"],\n    [\"../images/IMG_4201.JPG\", \"Niagara Falls, CA\"],\n    [\"../images/IMG_1331.JPG\", \"Waterloo, CA\"],\n    [\"../images/IMG_1333.JPG\", \"Waterloo, CA\"],\n    [\"../images/IMG_2151.JPG\", \"Niagara Falls, CA\"],\n    [\"../images/IMG_1891.JPG\", \"Montreal, CA\"],\n    [\"../images/IMG_0873.JPG\", \"Dali, CN\"],\n    [\"../images/IMG_0874.JPG\", \"Dali, CN\"],\n    [\"../images/IMG_0875.JPG\", \"Dali, CN\"]\n]\n---\n","updated":"2019-04-27T09:34:27.537Z","path":"gallery/index.html","comments":1,"layout":"page","_id":"cjyedivui0003t8aou6q8vjg2","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"How to Check Open TCP/IP Ports in Mac OS X","date":"2019-05-06T08:48:23.000Z","photos":["../images/cli.JPG"],"_content":"The core of Mac OS is Darwin and we can use most of the CLI tools in Mac OS just like how it feels like in Linux. If we want to check out the current ports in usage, the command **`netstat`** is useful:\n```\nnetstat -ap tcp | grep -i \"listen\"\n```\n<!-- more -->\nThat will print out something like this in the console:\n```\nAchive Internet connections(including servers)\nProto     Recv-Q      Send-Q       Local Address       Foreign Address     (state)     \ntcp4      0           0            localhost.25035     *.*                  LISTEN\n```\nThat works but the problem is that it doesn't show up the names of the procedures which occupy the ports. Sometimes we want to know precisely which program is exposing the port. \n\nThen found out that there is another command **`lsof`**:\n```\nsudo lsof -nP -iTCP:PortNumber -sTCP:LISTEN\n```\nwhich prints out all the processes running in a given port with specific names:\n```\nCOMMAND    PID    USER    FD    TYPE    DEVICE    SIZE/OFF    NODE       NAME\nsyslogd    350    root    5w    VREG    222,5     0           440818     /var/adm/messages     \nsyslogd    350    root    6w    VREG    222,5     339098      6248       /var/log/syslog\ncron       353    root    cwd   VDIR    222,5     512         254550     /var -- atjobs\n```\n\n**`-n`** : No dns (no host name)\n**`-P`** : List port number instead of its name\n**`-i `** : Lists IP sockets\n\nTo view the port associated with a daemon:\n```\nlsof -i -n -P | grep python\n```\n\nIf we just want to see the name:\n```\nsudo lsof -i :PortNumber | grep LISTEN\n```\n\nGet all running **PID** in a specific port:\n```\nsudo lsof -i :PortNumber| grep LISTEN | awk '{ print $2; }' | head -n 2 | grep -v PID   \n```\n\nAnd then we can kill all the processes:\n```\nsudo kill -9 $(sudo lsof -i :PortNumber| grep LISTEN | awk '{ print $2; }' | head -n 2 | grep -v PID)   \n```\n\nlist all commands:\n```\nlsof -h\n```\n","source":"_posts/How-to-Check-Open-TCP-IP-Ports-in-Mac-OS-X.md","raw":"---\ntitle: How to Check Open TCP/IP Ports in Mac OS X\ndate: 2019-05-06 17:48:23\ntags: [CLI, Mac OS, port]\nphotos: [\"../images/cli.JPG\"]\n---\nThe core of Mac OS is Darwin and we can use most of the CLI tools in Mac OS just like how it feels like in Linux. If we want to check out the current ports in usage, the command **`netstat`** is useful:\n```\nnetstat -ap tcp | grep -i \"listen\"\n```\n<!-- more -->\nThat will print out something like this in the console:\n```\nAchive Internet connections(including servers)\nProto     Recv-Q      Send-Q       Local Address       Foreign Address     (state)     \ntcp4      0           0            localhost.25035     *.*                  LISTEN\n```\nThat works but the problem is that it doesn't show up the names of the procedures which occupy the ports. Sometimes we want to know precisely which program is exposing the port. \n\nThen found out that there is another command **`lsof`**:\n```\nsudo lsof -nP -iTCP:PortNumber -sTCP:LISTEN\n```\nwhich prints out all the processes running in a given port with specific names:\n```\nCOMMAND    PID    USER    FD    TYPE    DEVICE    SIZE/OFF    NODE       NAME\nsyslogd    350    root    5w    VREG    222,5     0           440818     /var/adm/messages     \nsyslogd    350    root    6w    VREG    222,5     339098      6248       /var/log/syslog\ncron       353    root    cwd   VDIR    222,5     512         254550     /var -- atjobs\n```\n\n**`-n`** : No dns (no host name)\n**`-P`** : List port number instead of its name\n**`-i `** : Lists IP sockets\n\nTo view the port associated with a daemon:\n```\nlsof -i -n -P | grep python\n```\n\nIf we just want to see the name:\n```\nsudo lsof -i :PortNumber | grep LISTEN\n```\n\nGet all running **PID** in a specific port:\n```\nsudo lsof -i :PortNumber| grep LISTEN | awk '{ print $2; }' | head -n 2 | grep -v PID   \n```\n\nAnd then we can kill all the processes:\n```\nsudo kill -9 $(sudo lsof -i :PortNumber| grep LISTEN | awk '{ print $2; }' | head -n 2 | grep -v PID)   \n```\n\nlist all commands:\n```\nlsof -h\n```\n","slug":"How-to-Check-Open-TCP-IP-Ports-in-Mac-OS-X","published":1,"updated":"2019-05-10T09:00:12.697Z","comments":1,"layout":"post","link":"","_id":"cjyedivub0000t8aogl3u33ce","content":"<p>The core of Mac OS is Darwin and we can use most of the CLI tools in Mac OS just like how it feels like in Linux. If we want to check out the current ports in usage, the command <strong><code>netstat</code></strong> is useful:</p>\n<pre><code>netstat -ap tcp | grep -i &quot;listen&quot;\n</code></pre><a id=\"more\"></a>\n<p>That will print out something like this in the console:</p>\n<pre><code>Achive Internet connections(including servers)\nProto     Recv-Q      Send-Q       Local Address       Foreign Address     (state)     \ntcp4      0           0            localhost.25035     *.*                  LISTEN\n</code></pre><p>That works but the problem is that it doesn’t show up the names of the procedures which occupy the ports. Sometimes we want to know precisely which program is exposing the port. </p>\n<p>Then found out that there is another command <strong><code>lsof</code></strong>:</p>\n<pre><code>sudo lsof -nP -iTCP:PortNumber -sTCP:LISTEN\n</code></pre><p>which prints out all the processes running in a given port with specific names:</p>\n<pre><code>COMMAND    PID    USER    FD    TYPE    DEVICE    SIZE/OFF    NODE       NAME\nsyslogd    350    root    5w    VREG    222,5     0           440818     /var/adm/messages     \nsyslogd    350    root    6w    VREG    222,5     339098      6248       /var/log/syslog\ncron       353    root    cwd   VDIR    222,5     512         254550     /var -- atjobs\n</code></pre><p><strong><code>-n</code></strong> : No dns (no host name)<br><strong><code>-P</code></strong> : List port number instead of its name<br><strong><code>-i</code></strong> : Lists IP sockets</p>\n<p>To view the port associated with a daemon:</p>\n<pre><code>lsof -i -n -P | grep python\n</code></pre><p>If we just want to see the name:</p>\n<pre><code>sudo lsof -i :PortNumber | grep LISTEN\n</code></pre><p>Get all running <strong>PID</strong> in a specific port:</p>\n<pre><code>sudo lsof -i :PortNumber| grep LISTEN | awk &#39;{ print $2; }&#39; | head -n 2 | grep -v PID   \n</code></pre><p>And then we can kill all the processes:</p>\n<pre><code>sudo kill -9 $(sudo lsof -i :PortNumber| grep LISTEN | awk &#39;{ print $2; }&#39; | head -n 2 | grep -v PID)   \n</code></pre><p>list all commands:</p>\n<pre><code>lsof -h\n</code></pre>","site":{"data":{}},"excerpt":"<p>The core of Mac OS is Darwin and we can use most of the CLI tools in Mac OS just like how it feels like in Linux. If we want to check out the current ports in usage, the command <strong><code>netstat</code></strong> is useful:</p>\n<pre><code>netstat -ap tcp | grep -i &quot;listen&quot;\n</code></pre>","more":"<p>That will print out something like this in the console:</p>\n<pre><code>Achive Internet connections(including servers)\nProto     Recv-Q      Send-Q       Local Address       Foreign Address     (state)     \ntcp4      0           0            localhost.25035     *.*                  LISTEN\n</code></pre><p>That works but the problem is that it doesn’t show up the names of the procedures which occupy the ports. Sometimes we want to know precisely which program is exposing the port. </p>\n<p>Then found out that there is another command <strong><code>lsof</code></strong>:</p>\n<pre><code>sudo lsof -nP -iTCP:PortNumber -sTCP:LISTEN\n</code></pre><p>which prints out all the processes running in a given port with specific names:</p>\n<pre><code>COMMAND    PID    USER    FD    TYPE    DEVICE    SIZE/OFF    NODE       NAME\nsyslogd    350    root    5w    VREG    222,5     0           440818     /var/adm/messages     \nsyslogd    350    root    6w    VREG    222,5     339098      6248       /var/log/syslog\ncron       353    root    cwd   VDIR    222,5     512         254550     /var -- atjobs\n</code></pre><p><strong><code>-n</code></strong> : No dns (no host name)<br><strong><code>-P</code></strong> : List port number instead of its name<br><strong><code>-i</code></strong> : Lists IP sockets</p>\n<p>To view the port associated with a daemon:</p>\n<pre><code>lsof -i -n -P | grep python\n</code></pre><p>If we just want to see the name:</p>\n<pre><code>sudo lsof -i :PortNumber | grep LISTEN\n</code></pre><p>Get all running <strong>PID</strong> in a specific port:</p>\n<pre><code>sudo lsof -i :PortNumber| grep LISTEN | awk &#39;{ print $2; }&#39; | head -n 2 | grep -v PID   \n</code></pre><p>And then we can kill all the processes:</p>\n<pre><code>sudo kill -9 $(sudo lsof -i :PortNumber| grep LISTEN | awk &#39;{ print $2; }&#39; | head -n 2 | grep -v PID)   \n</code></pre><p>list all commands:</p>\n<pre><code>lsof -h\n</code></pre>"},{"title":"How to debug NodeJS on VS Code","date":"2019-05-08T02:50:53.000Z","photos":["../images/vscode.JPG"],"_content":"Here are the steps to start debug mode in VS Code:\n\n1. On the left side bar, click \"debug\" icon to switch to debug viewlet\n\n2. On the top left, click the gear icon\n\n3. Then `launch.json` will be opened in the editor\n\n4. Replace the content of the file to be:\n<!-- more -->\n```json\n{\n\t\"version\": \"0.2.0\",\n\t\"configurations\": [\n\t\t{\n\t\t\t\"type\": \"node\",\n\t\t\t\"request\": \"launch\",\n\t\t\t\"name\": \"Launch app.js\",\n\t\t\t\"program\": \"${workspaceRoot}/app.js\",\n\t\t\t\"stopOnEntry\": true,\n\t\t\t\"args\": [\n\t\t\t\t\"arg1\", \"arg2\", \"arg3\"\n\t\t\t]\n\t\t}\n\t]\n}\n```\n\n5. Replace the command line arguments to whatever you need\n\n6. Start the debugger or press `F5`\n\nYou are all good to go!\n\nIf your program reads from **stdin**, please add a \"console\" attribute to the launch config:\n```json\n{\n\t\"version\": \"0.2.0\",\n\t\"configurations\": [\n\t\t{\n\t\t\t\"type\": \"node\",\n\t\t\t\"request\": \"launch\",\n\t\t\t\"name\": \"Launch app.js\",\n\t\t\t\"program\": \"${workspaceRoot}/app.js\",\n\t\t\t\"stopOnEntry\": true,\n\t\t\t\"args\": [\n\t\t\t\t\"arg1\", \"arg2\", \"arg3\"\n\t\t\t],\n\t\t\t\"console\": \"integratedTerminal\"\n\t\t}\n\t]\n}\n```\n\nIf you are running the program in the **terminal**, you can change the content alternatively to be:\n```json\n{\n\t\"version\": \"0.2.0\",\n\t\"configurations\": [\n\t\t{\n\t\t\t\"type\": \"node\",\n\t\t\t\"request\": \"attach\",\n\t\t\t\"name\": \"Attach to app.js\",\n\t\t\t\"port\": \"5858\"\n\t\t}\n\t]\n}\n```\nThe port is the **debug port** and it has nothing to do with your program (no matter it is a service or not). Then in the terminal, run the command:\n```shell\nnode --debug-brk app.js arg1 arg2 arg3...\n```\n>The `--debug-brk` lets your program wait for the debugger to attach to. So there is no problem that it terminates before the debugger could attach.\n\n</br>\n\nRunning such command, you may encounter a warning below:\n```\n(node:31245) [DEP0062] DeprecationWarning: `node --inspect --debug-brk` is deprecated. Please use `node --inspect-brk` instead.     \n```\nAs discussed in [microsoft github offical repository](https://github.com/Microsoft/vscode/issues/32529), currently there is **no way** to prevent this happening. The reason why using `--inspect --debug-brk` is explained [here](https://github.com/microsoft/vscode/issues/27731):\n>This combination of args is the only way to enter debug mode across all node versions. At some point I'll switch to inspect-brk if we don't want to support node 6.x anymore, or will do version detection for it and do something for runtimeExecutable scenarios.\n\n>The problem is that we do not really know what version of node a user is using, so we cannot adapt the flags we use to the node version in order to minimize the resulting deprecation warnings.\n\n","source":"_posts/How-to-debug-NodeJS-on-VS-Code.md","raw":"---\ntitle: How to debug NodeJS on VS Code\ndate: 2019-05-08 11:50:53\ntags: [VS Code, NodeJS]\nphotos: [\"../images/vscode.JPG\"]\n---\nHere are the steps to start debug mode in VS Code:\n\n1. On the left side bar, click \"debug\" icon to switch to debug viewlet\n\n2. On the top left, click the gear icon\n\n3. Then `launch.json` will be opened in the editor\n\n4. Replace the content of the file to be:\n<!-- more -->\n```json\n{\n\t\"version\": \"0.2.0\",\n\t\"configurations\": [\n\t\t{\n\t\t\t\"type\": \"node\",\n\t\t\t\"request\": \"launch\",\n\t\t\t\"name\": \"Launch app.js\",\n\t\t\t\"program\": \"${workspaceRoot}/app.js\",\n\t\t\t\"stopOnEntry\": true,\n\t\t\t\"args\": [\n\t\t\t\t\"arg1\", \"arg2\", \"arg3\"\n\t\t\t]\n\t\t}\n\t]\n}\n```\n\n5. Replace the command line arguments to whatever you need\n\n6. Start the debugger or press `F5`\n\nYou are all good to go!\n\nIf your program reads from **stdin**, please add a \"console\" attribute to the launch config:\n```json\n{\n\t\"version\": \"0.2.0\",\n\t\"configurations\": [\n\t\t{\n\t\t\t\"type\": \"node\",\n\t\t\t\"request\": \"launch\",\n\t\t\t\"name\": \"Launch app.js\",\n\t\t\t\"program\": \"${workspaceRoot}/app.js\",\n\t\t\t\"stopOnEntry\": true,\n\t\t\t\"args\": [\n\t\t\t\t\"arg1\", \"arg2\", \"arg3\"\n\t\t\t],\n\t\t\t\"console\": \"integratedTerminal\"\n\t\t}\n\t]\n}\n```\n\nIf you are running the program in the **terminal**, you can change the content alternatively to be:\n```json\n{\n\t\"version\": \"0.2.0\",\n\t\"configurations\": [\n\t\t{\n\t\t\t\"type\": \"node\",\n\t\t\t\"request\": \"attach\",\n\t\t\t\"name\": \"Attach to app.js\",\n\t\t\t\"port\": \"5858\"\n\t\t}\n\t]\n}\n```\nThe port is the **debug port** and it has nothing to do with your program (no matter it is a service or not). Then in the terminal, run the command:\n```shell\nnode --debug-brk app.js arg1 arg2 arg3...\n```\n>The `--debug-brk` lets your program wait for the debugger to attach to. So there is no problem that it terminates before the debugger could attach.\n\n</br>\n\nRunning such command, you may encounter a warning below:\n```\n(node:31245) [DEP0062] DeprecationWarning: `node --inspect --debug-brk` is deprecated. Please use `node --inspect-brk` instead.     \n```\nAs discussed in [microsoft github offical repository](https://github.com/Microsoft/vscode/issues/32529), currently there is **no way** to prevent this happening. The reason why using `--inspect --debug-brk` is explained [here](https://github.com/microsoft/vscode/issues/27731):\n>This combination of args is the only way to enter debug mode across all node versions. At some point I'll switch to inspect-brk if we don't want to support node 6.x anymore, or will do version detection for it and do something for runtimeExecutable scenarios.\n\n>The problem is that we do not really know what version of node a user is using, so we cannot adapt the flags we use to the node version in order to minimize the resulting deprecation warnings.\n\n","slug":"How-to-debug-NodeJS-on-VS-Code","published":1,"updated":"2019-05-10T09:52:58.626Z","comments":1,"layout":"post","link":"","_id":"cjyedivuh0002t8aozzxepfjl","content":"<p>Here are the steps to start debug mode in VS Code:</p>\n<ol>\n<li><p>On the left side bar, click “debug” icon to switch to debug viewlet</p>\n</li>\n<li><p>On the top left, click the gear icon</p>\n</li>\n<li><p>Then <code>launch.json</code> will be opened in the editor</p>\n</li>\n<li><p>Replace the content of the file to be:</p>\n<a id=\"more\"></a>\n<pre><code class=\"json\">{\n &quot;version&quot;: &quot;0.2.0&quot;,\n &quot;configurations&quot;: [\n     {\n         &quot;type&quot;: &quot;node&quot;,\n         &quot;request&quot;: &quot;launch&quot;,\n         &quot;name&quot;: &quot;Launch app.js&quot;,\n         &quot;program&quot;: &quot;${workspaceRoot}/app.js&quot;,\n         &quot;stopOnEntry&quot;: true,\n         &quot;args&quot;: [\n             &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot;\n         ]\n     }\n ]\n}\n</code></pre>\n</li>\n<li><p>Replace the command line arguments to whatever you need</p>\n</li>\n<li><p>Start the debugger or press <code>F5</code></p>\n</li>\n</ol>\n<p>You are all good to go!</p>\n<p>If your program reads from <strong>stdin</strong>, please add a “console” attribute to the launch config:</p>\n<pre><code class=\"json\">{\n    &quot;version&quot;: &quot;0.2.0&quot;,\n    &quot;configurations&quot;: [\n        {\n            &quot;type&quot;: &quot;node&quot;,\n            &quot;request&quot;: &quot;launch&quot;,\n            &quot;name&quot;: &quot;Launch app.js&quot;,\n            &quot;program&quot;: &quot;${workspaceRoot}/app.js&quot;,\n            &quot;stopOnEntry&quot;: true,\n            &quot;args&quot;: [\n                &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot;\n            ],\n            &quot;console&quot;: &quot;integratedTerminal&quot;\n        }\n    ]\n}\n</code></pre>\n<p>If you are running the program in the <strong>terminal</strong>, you can change the content alternatively to be:</p>\n<pre><code class=\"json\">{\n    &quot;version&quot;: &quot;0.2.0&quot;,\n    &quot;configurations&quot;: [\n        {\n            &quot;type&quot;: &quot;node&quot;,\n            &quot;request&quot;: &quot;attach&quot;,\n            &quot;name&quot;: &quot;Attach to app.js&quot;,\n            &quot;port&quot;: &quot;5858&quot;\n        }\n    ]\n}\n</code></pre>\n<p>The port is the <strong>debug port</strong> and it has nothing to do with your program (no matter it is a service or not). Then in the terminal, run the command:</p>\n<pre><code class=\"shell\">node --debug-brk app.js arg1 arg2 arg3...\n</code></pre>\n<blockquote>\n<p>The <code>--debug-brk</code> lets your program wait for the debugger to attach to. So there is no problem that it terminates before the debugger could attach.</p>\n</blockquote>\n<p><br></p>\n<p>Running such command, you may encounter a warning below:</p>\n<pre><code>(node:31245) [DEP0062] DeprecationWarning: `node --inspect --debug-brk` is deprecated. Please use `node --inspect-brk` instead.     \n</code></pre><p>As discussed in <a href=\"https://github.com/Microsoft/vscode/issues/32529\" target=\"_blank\" rel=\"noopener\">microsoft github offical repository</a>, currently there is <strong>no way</strong> to prevent this happening. The reason why using <code>--inspect --debug-brk</code> is explained <a href=\"https://github.com/microsoft/vscode/issues/27731\" target=\"_blank\" rel=\"noopener\">here</a>:</p>\n<blockquote>\n<p>This combination of args is the only way to enter debug mode across all node versions. At some point I’ll switch to inspect-brk if we don’t want to support node 6.x anymore, or will do version detection for it and do something for runtimeExecutable scenarios.</p>\n</blockquote>\n<blockquote>\n<p>The problem is that we do not really know what version of node a user is using, so we cannot adapt the flags we use to the node version in order to minimize the resulting deprecation warnings.</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>Here are the steps to start debug mode in VS Code:</p>\n<ol>\n<li><p>On the left side bar, click “debug” icon to switch to debug viewlet</p>\n</li>\n<li><p>On the top left, click the gear icon</p>\n</li>\n<li><p>Then <code>launch.json</code> will be opened in the editor</p>\n</li>\n<li><p>Replace the content of the file to be:</p></li></ol>","more":"<pre><code class=\"json\">{\n &quot;version&quot;: &quot;0.2.0&quot;,\n &quot;configurations&quot;: [\n     {\n         &quot;type&quot;: &quot;node&quot;,\n         &quot;request&quot;: &quot;launch&quot;,\n         &quot;name&quot;: &quot;Launch app.js&quot;,\n         &quot;program&quot;: &quot;${workspaceRoot}/app.js&quot;,\n         &quot;stopOnEntry&quot;: true,\n         &quot;args&quot;: [\n             &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot;\n         ]\n     }\n ]\n}\n</code></pre>\n\n<li><p>Replace the command line arguments to whatever you need</p>\n</li>\n<li><p>Start the debugger or press <code>F5</code></p>\n</li>\n\n<p>You are all good to go!</p>\n<p>If your program reads from <strong>stdin</strong>, please add a “console” attribute to the launch config:</p>\n<pre><code class=\"json\">{\n    &quot;version&quot;: &quot;0.2.0&quot;,\n    &quot;configurations&quot;: [\n        {\n            &quot;type&quot;: &quot;node&quot;,\n            &quot;request&quot;: &quot;launch&quot;,\n            &quot;name&quot;: &quot;Launch app.js&quot;,\n            &quot;program&quot;: &quot;${workspaceRoot}/app.js&quot;,\n            &quot;stopOnEntry&quot;: true,\n            &quot;args&quot;: [\n                &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot;\n            ],\n            &quot;console&quot;: &quot;integratedTerminal&quot;\n        }\n    ]\n}\n</code></pre>\n<p>If you are running the program in the <strong>terminal</strong>, you can change the content alternatively to be:</p>\n<pre><code class=\"json\">{\n    &quot;version&quot;: &quot;0.2.0&quot;,\n    &quot;configurations&quot;: [\n        {\n            &quot;type&quot;: &quot;node&quot;,\n            &quot;request&quot;: &quot;attach&quot;,\n            &quot;name&quot;: &quot;Attach to app.js&quot;,\n            &quot;port&quot;: &quot;5858&quot;\n        }\n    ]\n}\n</code></pre>\n<p>The port is the <strong>debug port</strong> and it has nothing to do with your program (no matter it is a service or not). Then in the terminal, run the command:</p>\n<pre><code class=\"shell\">node --debug-brk app.js arg1 arg2 arg3...\n</code></pre>\n<blockquote>\n<p>The <code>--debug-brk</code> lets your program wait for the debugger to attach to. So there is no problem that it terminates before the debugger could attach.</p>\n</blockquote>\n<p><br></p>\n<p>Running such command, you may encounter a warning below:</p>\n<pre><code>(node:31245) [DEP0062] DeprecationWarning: `node --inspect --debug-brk` is deprecated. Please use `node --inspect-brk` instead.     \n</code></pre><p>As discussed in <a href=\"https://github.com/Microsoft/vscode/issues/32529\" target=\"_blank\" rel=\"noopener\">microsoft github offical repository</a>, currently there is <strong>no way</strong> to prevent this happening. The reason why using <code>--inspect --debug-brk</code> is explained <a href=\"https://github.com/microsoft/vscode/issues/27731\" target=\"_blank\" rel=\"noopener\">here</a>:</p>\n<blockquote>\n<p>This combination of args is the only way to enter debug mode across all node versions. At some point I’ll switch to inspect-brk if we don’t want to support node 6.x anymore, or will do version detection for it and do something for runtimeExecutable scenarios.</p>\n</blockquote>\n<blockquote>\n<p>The problem is that we do not really know what version of node a user is using, so we cannot adapt the flags we use to the node version in order to minimize the resulting deprecation warnings.</p>\n</blockquote>"},{"title":"Prefix Tree","date":"2019-07-18T09:30:12.000Z","photos":["../images/prefixTree.JPG"],"_content":"\n> A trie, also called digital tree, radix tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. \n\nEach node of the tree represents a string (prefix) and has multiple child nodes. A prefix tree is always used as search prompt when users browse through the search engine. The runtime of each query operation is independent with the size of the prefix tree; instead, it is decided by the length of the query target. <!-- more -->\n</br>\n\n## Sample\nHere list an array of some random words:\n> sin, sis, con, com, cin, cmd\n\nThen we can construct a prefix tree as following:\n![prefix tree sample](prefix_tree.JPG)\nDuring the query operation, there is no need to iterate the whole tree; instead, simply following each node and recursively going through its children will return the target or none if the target does not exist in the tree.\n</br>\n\n## Construct A Prefix Tree\nThere are multiple ways to form a prefix tree. The most popular method is to maintain three arrays in the instance to record end, path and next.\n\n- **Node**\n```python\nclass TrieNode:\n    def __init__(self):\n        self.nodes = {}\n        self.count = 0\n        self.isword = False\n```\n\n- **CRUD**\n```python\nclass Trie:\n    def __init__(self):\n        \"\"\"\n        Initialize data structure.\n        \"\"\"\n        self.root = TrieNode()\n\n    def insert(self, word: str):\n        \"\"\"\n        Inserts a word into the trie.\n        :type word: str\n        :rtype: void\n        \"\"\"\n        curr = self.root\n        for char in word:\n            if char not in curr.nodes:\n                curr.nodes[char] = TrieNode()\n            curr.nodes[char].count += 1\n            curr = curr.nodes[char]\n        curr.isword = True\n\n    def query(self, target: str):\n        \"\"\"\n        Returns if the word is in the trie.\n        :type target: str\n        :rtype: bool\n        \"\"\"\n        curr = self.root\n        for char in target:\n            if char not in curr.nodes:\n                return False\n            curr = curr.nodes[char]\n        return curr.isword\n\n    def startWith(self, prefix: str):\n        \"\"\"\n        Returns the number of words in the trie that start with the given prefix.   \n        :type prefix: str\n        :rtype: int\n        \"\"\"\n        curr = self.root\n        for char in prefix:\n            if char not in curr.nodes:\n                return 0\n            curr = curr.nodes[char]\n        return curr.count\n\n    def delete(self, target: str):\n        \"\"\"\n        Returns true if target exist and successfully delete from the trie.\n        :type target: str\n        :rtype: bool\n        \"\"\"\n        curr = self.root\n        for char in prefix:\n            if char not in curr.nodes:\n                return False\n            curr = curr.nodes[char]\n        if curr.isword:\n            curr.isword = False\n            return True\n        return False\n\n    def listAllMatches(self, prefix: str):\n        \"\"\"\n        Returns all words started with prefix\n        :param prefix:\n        :return: List[str]\n        \"\"\"\n        result = []\n        def recursiveQuery(node: TrieNode, path: str):\n            if not node.nodes:\n                result.append(path)\n            else:\n                for key in node.nodes.keys():\n                    recursiveQuery(node.nodes[key], path + key)\n        curr = self.root\n        for char in prefix:\n            if char not in curr.nodes:\n                return result\n            curr = curr.nodes[char]\n        recursiveQuery('')\n        return result\n```\n","source":"_posts/Prefix-Tree.md","raw":"---\ntitle: Prefix Tree\ndate: 2019-07-18 18:30:12\ntags: [Prefix Tree]\nphotos: [\"../images/prefixTree.JPG\"]\n---\n\n> A trie, also called digital tree, radix tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. \n\nEach node of the tree represents a string (prefix) and has multiple child nodes. A prefix tree is always used as search prompt when users browse through the search engine. The runtime of each query operation is independent with the size of the prefix tree; instead, it is decided by the length of the query target. <!-- more -->\n</br>\n\n## Sample\nHere list an array of some random words:\n> sin, sis, con, com, cin, cmd\n\nThen we can construct a prefix tree as following:\n![prefix tree sample](prefix_tree.JPG)\nDuring the query operation, there is no need to iterate the whole tree; instead, simply following each node and recursively going through its children will return the target or none if the target does not exist in the tree.\n</br>\n\n## Construct A Prefix Tree\nThere are multiple ways to form a prefix tree. The most popular method is to maintain three arrays in the instance to record end, path and next.\n\n- **Node**\n```python\nclass TrieNode:\n    def __init__(self):\n        self.nodes = {}\n        self.count = 0\n        self.isword = False\n```\n\n- **CRUD**\n```python\nclass Trie:\n    def __init__(self):\n        \"\"\"\n        Initialize data structure.\n        \"\"\"\n        self.root = TrieNode()\n\n    def insert(self, word: str):\n        \"\"\"\n        Inserts a word into the trie.\n        :type word: str\n        :rtype: void\n        \"\"\"\n        curr = self.root\n        for char in word:\n            if char not in curr.nodes:\n                curr.nodes[char] = TrieNode()\n            curr.nodes[char].count += 1\n            curr = curr.nodes[char]\n        curr.isword = True\n\n    def query(self, target: str):\n        \"\"\"\n        Returns if the word is in the trie.\n        :type target: str\n        :rtype: bool\n        \"\"\"\n        curr = self.root\n        for char in target:\n            if char not in curr.nodes:\n                return False\n            curr = curr.nodes[char]\n        return curr.isword\n\n    def startWith(self, prefix: str):\n        \"\"\"\n        Returns the number of words in the trie that start with the given prefix.   \n        :type prefix: str\n        :rtype: int\n        \"\"\"\n        curr = self.root\n        for char in prefix:\n            if char not in curr.nodes:\n                return 0\n            curr = curr.nodes[char]\n        return curr.count\n\n    def delete(self, target: str):\n        \"\"\"\n        Returns true if target exist and successfully delete from the trie.\n        :type target: str\n        :rtype: bool\n        \"\"\"\n        curr = self.root\n        for char in prefix:\n            if char not in curr.nodes:\n                return False\n            curr = curr.nodes[char]\n        if curr.isword:\n            curr.isword = False\n            return True\n        return False\n\n    def listAllMatches(self, prefix: str):\n        \"\"\"\n        Returns all words started with prefix\n        :param prefix:\n        :return: List[str]\n        \"\"\"\n        result = []\n        def recursiveQuery(node: TrieNode, path: str):\n            if not node.nodes:\n                result.append(path)\n            else:\n                for key in node.nodes.keys():\n                    recursiveQuery(node.nodes[key], path + key)\n        curr = self.root\n        for char in prefix:\n            if char not in curr.nodes:\n                return result\n            curr = curr.nodes[char]\n        recursiveQuery('')\n        return result\n```\n","slug":"Prefix-Tree","published":1,"updated":"2019-07-18T14:08:34.719Z","comments":1,"layout":"post","link":"","_id":"cjyedivum0005t8aoelix8c8k","content":"<blockquote>\n<p>A trie, also called digital tree, radix tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. </p>\n</blockquote>\n<p>Each node of the tree represents a string (prefix) and has multiple child nodes. A prefix tree is always used as search prompt when users browse through the search engine. The runtime of each query operation is independent with the size of the prefix tree; instead, it is decided by the length of the query target. <a id=\"more\"></a><br><br></p>\n<h2 id=\"Sample\"><a href=\"#Sample\" class=\"headerlink\" title=\"Sample\"></a>Sample</h2><p>Here list an array of some random words:</p>\n<blockquote>\n<p>sin, sis, con, com, cin, cmd</p>\n</blockquote>\n<p>Then we can construct a prefix tree as following:<br><img src=\"/2019/07/18/Prefix-Tree/prefix_tree.JPG\" alt=\"prefix tree sample\"><br>During the query operation, there is no need to iterate the whole tree; instead, simply following each node and recursively going through its children will return the target or none if the target does not exist in the tree.<br><br></p>\n<h2 id=\"Construct-A-Prefix-Tree\"><a href=\"#Construct-A-Prefix-Tree\" class=\"headerlink\" title=\"Construct A Prefix Tree\"></a>Construct A Prefix Tree</h2><p>There are multiple ways to form a prefix tree. The most popular method is to maintain three arrays in the instance to record end, path and next.</p>\n<ul>\n<li><p><strong>Node</strong></p>\n<pre><code class=\"python\">class TrieNode:\n  def __init__(self):\n      self.nodes = {}\n      self.count = 0\n      self.isword = False\n</code></pre>\n</li>\n<li><p><strong>CRUD</strong></p>\n<pre><code class=\"python\">class Trie:\n  def __init__(self):\n      &quot;&quot;&quot;\n      Initialize data structure.\n      &quot;&quot;&quot;\n      self.root = TrieNode()\n\n  def insert(self, word: str):\n      &quot;&quot;&quot;\n      Inserts a word into the trie.\n      :type word: str\n      :rtype: void\n      &quot;&quot;&quot;\n      curr = self.root\n      for char in word:\n          if char not in curr.nodes:\n              curr.nodes[char] = TrieNode()\n          curr.nodes[char].count += 1\n          curr = curr.nodes[char]\n      curr.isword = True\n\n  def query(self, target: str):\n      &quot;&quot;&quot;\n      Returns if the word is in the trie.\n      :type target: str\n      :rtype: bool\n      &quot;&quot;&quot;\n      curr = self.root\n      for char in target:\n          if char not in curr.nodes:\n              return False\n          curr = curr.nodes[char]\n      return curr.isword\n\n  def startWith(self, prefix: str):\n      &quot;&quot;&quot;\n      Returns the number of words in the trie that start with the given prefix.   \n      :type prefix: str\n      :rtype: int\n      &quot;&quot;&quot;\n      curr = self.root\n      for char in prefix:\n          if char not in curr.nodes:\n              return 0\n          curr = curr.nodes[char]\n      return curr.count\n\n  def delete(self, target: str):\n      &quot;&quot;&quot;\n      Returns true if target exist and successfully delete from the trie.\n      :type target: str\n      :rtype: bool\n      &quot;&quot;&quot;\n      curr = self.root\n      for char in prefix:\n          if char not in curr.nodes:\n              return False\n          curr = curr.nodes[char]\n      if curr.isword:\n          curr.isword = False\n          return True\n      return False\n\n  def listAllMatches(self, prefix: str):\n      &quot;&quot;&quot;\n      Returns all words started with prefix\n      :param prefix:\n      :return: List[str]\n      &quot;&quot;&quot;\n      result = []\n      def recursiveQuery(node: TrieNode, path: str):\n          if not node.nodes:\n              result.append(path)\n          else:\n              for key in node.nodes.keys():\n                  recursiveQuery(node.nodes[key], path + key)\n      curr = self.root\n      for char in prefix:\n          if char not in curr.nodes:\n              return result\n          curr = curr.nodes[char]\n      recursiveQuery(&#39;&#39;)\n      return result\n</code></pre>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>A trie, also called digital tree, radix tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings. </p>\n</blockquote>\n<p>Each node of the tree represents a string (prefix) and has multiple child nodes. A prefix tree is always used as search prompt when users browse through the search engine. The runtime of each query operation is independent with the size of the prefix tree; instead, it is decided by the length of the query target.</p>","more":"<br><br><p></p>\n<h2 id=\"Sample\"><a href=\"#Sample\" class=\"headerlink\" title=\"Sample\"></a>Sample</h2><p>Here list an array of some random words:</p>\n<blockquote>\n<p>sin, sis, con, com, cin, cmd</p>\n</blockquote>\n<p>Then we can construct a prefix tree as following:<br><img src=\"/2019/07/18/Prefix-Tree/prefix_tree.JPG\" alt=\"prefix tree sample\"><br>During the query operation, there is no need to iterate the whole tree; instead, simply following each node and recursively going through its children will return the target or none if the target does not exist in the tree.<br><br></p>\n<h2 id=\"Construct-A-Prefix-Tree\"><a href=\"#Construct-A-Prefix-Tree\" class=\"headerlink\" title=\"Construct A Prefix Tree\"></a>Construct A Prefix Tree</h2><p>There are multiple ways to form a prefix tree. The most popular method is to maintain three arrays in the instance to record end, path and next.</p>\n<ul>\n<li><p><strong>Node</strong></p>\n<pre><code class=\"python\">class TrieNode:\n  def __init__(self):\n      self.nodes = {}\n      self.count = 0\n      self.isword = False\n</code></pre>\n</li>\n<li><p><strong>CRUD</strong></p>\n<pre><code class=\"python\">class Trie:\n  def __init__(self):\n      &quot;&quot;&quot;\n      Initialize data structure.\n      &quot;&quot;&quot;\n      self.root = TrieNode()\n\n  def insert(self, word: str):\n      &quot;&quot;&quot;\n      Inserts a word into the trie.\n      :type word: str\n      :rtype: void\n      &quot;&quot;&quot;\n      curr = self.root\n      for char in word:\n          if char not in curr.nodes:\n              curr.nodes[char] = TrieNode()\n          curr.nodes[char].count += 1\n          curr = curr.nodes[char]\n      curr.isword = True\n\n  def query(self, target: str):\n      &quot;&quot;&quot;\n      Returns if the word is in the trie.\n      :type target: str\n      :rtype: bool\n      &quot;&quot;&quot;\n      curr = self.root\n      for char in target:\n          if char not in curr.nodes:\n              return False\n          curr = curr.nodes[char]\n      return curr.isword\n\n  def startWith(self, prefix: str):\n      &quot;&quot;&quot;\n      Returns the number of words in the trie that start with the given prefix.   \n      :type prefix: str\n      :rtype: int\n      &quot;&quot;&quot;\n      curr = self.root\n      for char in prefix:\n          if char not in curr.nodes:\n              return 0\n          curr = curr.nodes[char]\n      return curr.count\n\n  def delete(self, target: str):\n      &quot;&quot;&quot;\n      Returns true if target exist and successfully delete from the trie.\n      :type target: str\n      :rtype: bool\n      &quot;&quot;&quot;\n      curr = self.root\n      for char in prefix:\n          if char not in curr.nodes:\n              return False\n          curr = curr.nodes[char]\n      if curr.isword:\n          curr.isword = False\n          return True\n      return False\n\n  def listAllMatches(self, prefix: str):\n      &quot;&quot;&quot;\n      Returns all words started with prefix\n      :param prefix:\n      :return: List[str]\n      &quot;&quot;&quot;\n      result = []\n      def recursiveQuery(node: TrieNode, path: str):\n          if not node.nodes:\n              result.append(path)\n          else:\n              for key in node.nodes.keys():\n                  recursiveQuery(node.nodes[key], path + key)\n      curr = self.root\n      for char in prefix:\n          if char not in curr.nodes:\n              return result\n          curr = curr.nodes[char]\n      recursiveQuery(&#39;&#39;)\n      return result\n</code></pre>\n</li>\n</ul>"},{"title":"Prefix Notation","date":"2019-04-30T02:11:21.000Z","photos":["../images/lisp.JPG"],"_content":"```python\n( 20 + 5 )\n( 16 / 4 )\n```\nSuch expressions which denote procedures, are called ***combinations***. The left and the right elements are called ***operands***, and the element in the middle to indicate the operation is called ***operator***. This is the most common style we have seen by now; however there is another way to construct a procedure known as ***prefix notation***:\n```python\n( + 20 5 )\n( / 16 4 )\n```\nInstead of injecting the operator between operands, which is a more human readable style, the prefix notation requires the operator always to be at the left most.<!-- more -->\n\nconditions:\n```python\n( define ( abs x )\n    ( cond (( > x 0 ) x )\n           (( = x 0 ) 0 )\n           (( < x 0 ) ( - x ))))\n```\nThe general form can be expressed as:\n>( cond (<\\P1> <\\E1>)\n>       (<\\P2> <\\E2>)\n>            ...\n>       (<\\Pn> <\\En>))\n\nIf none of them is evaluated to be **true**, then the value of the **cond** will be **undefined**. It can also be simplified by using ***else***:\n```python\n( define ( abs x )\n    ( cond (( < x 0 ) ( - x ))\n           ( else  x )))\n```\nIf there is only two ***predicates*** (the expression to be interpreted as either true of false), then it can use a special form ***if***:\n```python\n( define ( abs x )\n    ( if ( < x 0 )\n         ( - x )\n         x ))\n```\nThe general form of an ***if*** expression is:\n>( if <\\predicate> <\\consequent> <\\alternative> )\n\nThe logic operators:\n>( and <\\E1> ... <\\En> )\n>( or <\\E1> ... <\\En> )\n>( not <E> )\n\nThen use the logic operators to define a predicate to evaluate if a number id larger or equal to the other one:\n```python\n( define ( >= x y )\n    ( or ( > x y ) ( = x y ))\n```\nThat is all the syntax, **there is no loop in a functional programming language!**</br></br>\n## Recursion\nConsidering the factorial function:\n> n! = n ⋅ (n-1) ⋅ (n-2) ⋅ ... ⋅2⋅1\n\nWhich can be computed as:\n> n! = n ⋅ (n-1)!\n\nIf we end it up with **1!**, then simply output **1**. Then the factorial function can be implemented in ***linear recursion***:\n```python\n( define ( factorial n )\n    ( if ( = n 1 )\n        1\n        ( * n ( factorial ( - n 1 )))))\n```\n***Linear recursion*** defines that the computation chains of operations is proportional to n and hence grows linearly. There is also another pattern of recursion, known as ***Tree Recursion***. The best example will be the Fibonacci series, in which each element is the sum of the previous two:\n```python\n( define ( fib n )\n    ( cond ( = n 0 ) 0 )\n           ( = n 1 ) 1 )\n           ( else ( + ( fib ( - n 1 ) )\n                      ( fib ( - n 2 ) )))))\n```\nYou may find out that this procedure is not really efficient because to compute **fib( - n 1)**, **fib( - n 2)** has to be computed one more time which causes duplicated work.\n![Tree Recursion](../images/treeRecursion.png)\nTherefore, instead of ***Tree Recursion***, let's try to convert it to be ***Linear Recursion***. Reasign the sum of **a** and **b** to **a**, and the previous **a** to **b**:\n```python\n( define ( fib n )\n    ( iterate 1 0 n ))\n\n( define ( iterate a b count )\n    ( if ( = count 0 )\n        b\n        ( iterate ( + a b ) a ( - count 1 ))))\n```\n</br>\n## Lambda\nInstead of defining some trivial procedures so that we can pass them as arguments of the other procedures, functional programming provides ***Lambda Expression***:\n>( lambda ( <\\formal-param> ) <\\body> )\n\nFor instance,\n```python\n( define ( Add a b ) ( + a b ))\n```\ncan be written as:\n```python\n( define add ( lambda ( a b ) ( + a b )))\n```\nAnd operators can also be represented by ***Lambda Expression***:\n```python\n(( lambda ( a b ) ( + ( * a a ) ( * b b ))) 2 3 )\n```\nAnother use of ***Lambda Expression*** is creating local variables. An expression can be binded with a specific name by using keyword ***let***. The above example then can be interpreted as:\n```python\n( define ( sumsqr x y )\n    ( let ( a ( * x x ))\n          ( b ( * y y ))\n        ( + a b )))\n```\n***Note:*** The scope of a variable specified by a ***let*** is only applied to the **body** of the ***let***. For example, if the evalue of **x** is **2**, then the expression:\n```python\n( let (( x 3 )\n        ( y ( + x 2 )))\n    ( * x y ))\n```\nThe value of **y** will be **4** as being outside of the **let** body, and the output will be **3 * 4 = 12**. It seems like ***let*** is really similar to ***define***; however, in the most cases, we much prefer using ***let*** and only apply ***define*** to **internal procedures**.","source":"_posts/Prefix-Notation.md","raw":"---\ntitle: Prefix Notation\ndate: 2019-04-30 11:11:21\ntags: [Lisp, Scheme, Prefix Notation, Functional Programming]\nphotos: [\"../images/lisp.JPG\"]\n---\n```python\n( 20 + 5 )\n( 16 / 4 )\n```\nSuch expressions which denote procedures, are called ***combinations***. The left and the right elements are called ***operands***, and the element in the middle to indicate the operation is called ***operator***. This is the most common style we have seen by now; however there is another way to construct a procedure known as ***prefix notation***:\n```python\n( + 20 5 )\n( / 16 4 )\n```\nInstead of injecting the operator between operands, which is a more human readable style, the prefix notation requires the operator always to be at the left most.<!-- more -->\n\nconditions:\n```python\n( define ( abs x )\n    ( cond (( > x 0 ) x )\n           (( = x 0 ) 0 )\n           (( < x 0 ) ( - x ))))\n```\nThe general form can be expressed as:\n>( cond (<\\P1> <\\E1>)\n>       (<\\P2> <\\E2>)\n>            ...\n>       (<\\Pn> <\\En>))\n\nIf none of them is evaluated to be **true**, then the value of the **cond** will be **undefined**. It can also be simplified by using ***else***:\n```python\n( define ( abs x )\n    ( cond (( < x 0 ) ( - x ))\n           ( else  x )))\n```\nIf there is only two ***predicates*** (the expression to be interpreted as either true of false), then it can use a special form ***if***:\n```python\n( define ( abs x )\n    ( if ( < x 0 )\n         ( - x )\n         x ))\n```\nThe general form of an ***if*** expression is:\n>( if <\\predicate> <\\consequent> <\\alternative> )\n\nThe logic operators:\n>( and <\\E1> ... <\\En> )\n>( or <\\E1> ... <\\En> )\n>( not <E> )\n\nThen use the logic operators to define a predicate to evaluate if a number id larger or equal to the other one:\n```python\n( define ( >= x y )\n    ( or ( > x y ) ( = x y ))\n```\nThat is all the syntax, **there is no loop in a functional programming language!**</br></br>\n## Recursion\nConsidering the factorial function:\n> n! = n ⋅ (n-1) ⋅ (n-2) ⋅ ... ⋅2⋅1\n\nWhich can be computed as:\n> n! = n ⋅ (n-1)!\n\nIf we end it up with **1!**, then simply output **1**. Then the factorial function can be implemented in ***linear recursion***:\n```python\n( define ( factorial n )\n    ( if ( = n 1 )\n        1\n        ( * n ( factorial ( - n 1 )))))\n```\n***Linear recursion*** defines that the computation chains of operations is proportional to n and hence grows linearly. There is also another pattern of recursion, known as ***Tree Recursion***. The best example will be the Fibonacci series, in which each element is the sum of the previous two:\n```python\n( define ( fib n )\n    ( cond ( = n 0 ) 0 )\n           ( = n 1 ) 1 )\n           ( else ( + ( fib ( - n 1 ) )\n                      ( fib ( - n 2 ) )))))\n```\nYou may find out that this procedure is not really efficient because to compute **fib( - n 1)**, **fib( - n 2)** has to be computed one more time which causes duplicated work.\n![Tree Recursion](../images/treeRecursion.png)\nTherefore, instead of ***Tree Recursion***, let's try to convert it to be ***Linear Recursion***. Reasign the sum of **a** and **b** to **a**, and the previous **a** to **b**:\n```python\n( define ( fib n )\n    ( iterate 1 0 n ))\n\n( define ( iterate a b count )\n    ( if ( = count 0 )\n        b\n        ( iterate ( + a b ) a ( - count 1 ))))\n```\n</br>\n## Lambda\nInstead of defining some trivial procedures so that we can pass them as arguments of the other procedures, functional programming provides ***Lambda Expression***:\n>( lambda ( <\\formal-param> ) <\\body> )\n\nFor instance,\n```python\n( define ( Add a b ) ( + a b ))\n```\ncan be written as:\n```python\n( define add ( lambda ( a b ) ( + a b )))\n```\nAnd operators can also be represented by ***Lambda Expression***:\n```python\n(( lambda ( a b ) ( + ( * a a ) ( * b b ))) 2 3 )\n```\nAnother use of ***Lambda Expression*** is creating local variables. An expression can be binded with a specific name by using keyword ***let***. The above example then can be interpreted as:\n```python\n( define ( sumsqr x y )\n    ( let ( a ( * x x ))\n          ( b ( * y y ))\n        ( + a b )))\n```\n***Note:*** The scope of a variable specified by a ***let*** is only applied to the **body** of the ***let***. For example, if the evalue of **x** is **2**, then the expression:\n```python\n( let (( x 3 )\n        ( y ( + x 2 )))\n    ( * x y ))\n```\nThe value of **y** will be **4** as being outside of the **let** body, and the output will be **3 * 4 = 12**. It seems like ***let*** is really similar to ***define***; however, in the most cases, we much prefer using ***let*** and only apply ***define*** to **internal procedures**.","slug":"Prefix-Notation","published":1,"updated":"2019-05-10T08:46:16.855Z","comments":1,"layout":"post","link":"","_id":"cjyedivun0006t8aogaj6kwbl","content":"<pre><code class=\"python\">( 20 + 5 )\n( 16 / 4 )\n</code></pre>\n<p>Such expressions which denote procedures, are called <strong><em>combinations</em></strong>. The left and the right elements are called <strong><em>operands</em></strong>, and the element in the middle to indicate the operation is called <strong><em>operator</em></strong>. This is the most common style we have seen by now; however there is another way to construct a procedure known as <strong><em>prefix notation</em></strong>:</p>\n<pre><code class=\"python\">( + 20 5 )\n( / 16 4 )\n</code></pre>\n<p>Instead of injecting the operator between operands, which is a more human readable style, the prefix notation requires the operator always to be at the left most.<a id=\"more\"></a></p>\n<p>conditions:</p>\n<pre><code class=\"python\">( define ( abs x )\n    ( cond (( &gt; x 0 ) x )\n           (( = x 0 ) 0 )\n           (( &lt; x 0 ) ( - x ))))\n</code></pre>\n<p>The general form can be expressed as:</p>\n<blockquote>\n<p>( cond (&lt;\\P1&gt; &lt;\\E1&gt;)<br>      (&lt;\\P2&gt; &lt;\\E2&gt;)<br>           …<br>      (&lt;\\Pn&gt; &lt;\\En&gt;))</p>\n</blockquote>\n<p>If none of them is evaluated to be <strong>true</strong>, then the value of the <strong>cond</strong> will be <strong>undefined</strong>. It can also be simplified by using <strong><em>else</em></strong>:</p>\n<pre><code class=\"python\">( define ( abs x )\n    ( cond (( &lt; x 0 ) ( - x ))\n           ( else  x )))\n</code></pre>\n<p>If there is only two <strong><em>predicates</em></strong> (the expression to be interpreted as either true of false), then it can use a special form <strong><em>if</em></strong>:</p>\n<pre><code class=\"python\">( define ( abs x )\n    ( if ( &lt; x 0 )\n         ( - x )\n         x ))\n</code></pre>\n<p>The general form of an <strong><em>if</em></strong> expression is:</p>\n<blockquote>\n<p>( if &lt;\\predicate&gt; &lt;\\consequent&gt; &lt;\\alternative&gt; )</p>\n</blockquote>\n<p>The logic operators:</p>\n<blockquote>\n<p>( and &lt;\\E1&gt; … &lt;\\En&gt; )<br>( or &lt;\\E1&gt; … &lt;\\En&gt; )<br>( not <e> )</e></p>\n</blockquote>\n<p>Then use the logic operators to define a predicate to evaluate if a number id larger or equal to the other one:</p>\n<pre><code class=\"python\">( define ( &gt;= x y )\n    ( or ( &gt; x y ) ( = x y ))\n</code></pre>\n<p>That is all the syntax, <strong>there is no loop in a functional programming language!</strong><br><br></p>\n<h2 id=\"Recursion\"><a href=\"#Recursion\" class=\"headerlink\" title=\"Recursion\"></a>Recursion</h2><p>Considering the factorial function:</p>\n<blockquote>\n<p>n! = n ⋅ (n-1) ⋅ (n-2) ⋅ … ⋅2⋅1</p>\n</blockquote>\n<p>Which can be computed as:</p>\n<blockquote>\n<p>n! = n ⋅ (n-1)!</p>\n</blockquote>\n<p>If we end it up with <strong>1!</strong>, then simply output <strong>1</strong>. Then the factorial function can be implemented in <strong><em>linear recursion</em></strong>:</p>\n<pre><code class=\"python\">( define ( factorial n )\n    ( if ( = n 1 )\n        1\n        ( * n ( factorial ( - n 1 )))))\n</code></pre>\n<p><strong><em>Linear recursion</em></strong> defines that the computation chains of operations is proportional to n and hence grows linearly. There is also another pattern of recursion, known as <strong><em>Tree Recursion</em></strong>. The best example will be the Fibonacci series, in which each element is the sum of the previous two:</p>\n<pre><code class=\"python\">( define ( fib n )\n    ( cond ( = n 0 ) 0 )\n           ( = n 1 ) 1 )\n           ( else ( + ( fib ( - n 1 ) )\n                      ( fib ( - n 2 ) )))))\n</code></pre>\n<p>You may find out that this procedure is not really efficient because to compute <strong>fib( - n 1)</strong>, <strong>fib( - n 2)</strong> has to be computed one more time which causes duplicated work.<br><img src=\"/2019/04/30/Prefix-Notation/images/treeRecursion.png\" alt=\"Tree Recursion\"><br>Therefore, instead of <strong><em>Tree Recursion</em></strong>, let’s try to convert it to be <strong><em>Linear Recursion</em></strong>. Reasign the sum of <strong>a</strong> and <strong>b</strong> to <strong>a</strong>, and the previous <strong>a</strong> to <strong>b</strong>:</p>\n<pre><code class=\"python\">( define ( fib n )\n    ( iterate 1 0 n ))\n\n( define ( iterate a b count )\n    ( if ( = count 0 )\n        b\n        ( iterate ( + a b ) a ( - count 1 ))))\n</code></pre>\n<p><br></p>\n<h2 id=\"Lambda\"><a href=\"#Lambda\" class=\"headerlink\" title=\"Lambda\"></a>Lambda</h2><p>Instead of defining some trivial procedures so that we can pass them as arguments of the other procedures, functional programming provides <strong><em>Lambda Expression</em></strong>:</p>\n<blockquote>\n<p>( lambda ( &lt;\\formal-param&gt; ) &lt;\\body&gt; )</p>\n</blockquote>\n<p>For instance,</p>\n<pre><code class=\"python\">( define ( Add a b ) ( + a b ))\n</code></pre>\n<p>can be written as:</p>\n<pre><code class=\"python\">( define add ( lambda ( a b ) ( + a b )))\n</code></pre>\n<p>And operators can also be represented by <strong><em>Lambda Expression</em></strong>:</p>\n<pre><code class=\"python\">(( lambda ( a b ) ( + ( * a a ) ( * b b ))) 2 3 )\n</code></pre>\n<p>Another use of <strong><em>Lambda Expression</em></strong> is creating local variables. An expression can be binded with a specific name by using keyword <strong><em>let</em></strong>. The above example then can be interpreted as:</p>\n<pre><code class=\"python\">( define ( sumsqr x y )\n    ( let ( a ( * x x ))\n          ( b ( * y y ))\n        ( + a b )))\n</code></pre>\n<p><strong><em>Note:</em></strong> The scope of a variable specified by a <strong><em>let</em></strong> is only applied to the <strong>body</strong> of the <strong><em>let</em></strong>. For example, if the evalue of <strong>x</strong> is <strong>2</strong>, then the expression:</p>\n<pre><code class=\"python\">( let (( x 3 )\n        ( y ( + x 2 )))\n    ( * x y ))\n</code></pre>\n<p>The value of <strong>y</strong> will be <strong>4</strong> as being outside of the <strong>let</strong> body, and the output will be <strong>3 * 4 = 12</strong>. It seems like <strong><em>let</em></strong> is really similar to <strong><em>define</em></strong>; however, in the most cases, we much prefer using <strong><em>let</em></strong> and only apply <strong><em>define</em></strong> to <strong>internal procedures</strong>.</p>\n","site":{"data":{}},"excerpt":"<pre><code class=\"python\">( 20 + 5 )\n( 16 / 4 )\n</code></pre>\n<p>Such expressions which denote procedures, are called <strong><em>combinations</em></strong>. The left and the right elements are called <strong><em>operands</em></strong>, and the element in the middle to indicate the operation is called <strong><em>operator</em></strong>. This is the most common style we have seen by now; however there is another way to construct a procedure known as <strong><em>prefix notation</em></strong>:</p>\n<pre><code class=\"python\">( + 20 5 )\n( / 16 4 )\n</code></pre>\n<p>Instead of injecting the operator between operands, which is a more human readable style, the prefix notation requires the operator always to be at the left most.</p>","more":"<p></p>\n<p>conditions:</p>\n<pre><code class=\"python\">( define ( abs x )\n    ( cond (( &gt; x 0 ) x )\n           (( = x 0 ) 0 )\n           (( &lt; x 0 ) ( - x ))))\n</code></pre>\n<p>The general form can be expressed as:</p>\n<blockquote>\n<p>( cond (&lt;\\P1&gt; &lt;\\E1&gt;)<br>      (&lt;\\P2&gt; &lt;\\E2&gt;)<br>           …<br>      (&lt;\\Pn&gt; &lt;\\En&gt;))</p>\n</blockquote>\n<p>If none of them is evaluated to be <strong>true</strong>, then the value of the <strong>cond</strong> will be <strong>undefined</strong>. It can also be simplified by using <strong><em>else</em></strong>:</p>\n<pre><code class=\"python\">( define ( abs x )\n    ( cond (( &lt; x 0 ) ( - x ))\n           ( else  x )))\n</code></pre>\n<p>If there is only two <strong><em>predicates</em></strong> (the expression to be interpreted as either true of false), then it can use a special form <strong><em>if</em></strong>:</p>\n<pre><code class=\"python\">( define ( abs x )\n    ( if ( &lt; x 0 )\n         ( - x )\n         x ))\n</code></pre>\n<p>The general form of an <strong><em>if</em></strong> expression is:</p>\n<blockquote>\n<p>( if &lt;\\predicate&gt; &lt;\\consequent&gt; &lt;\\alternative&gt; )</p>\n</blockquote>\n<p>The logic operators:</p>\n<blockquote>\n<p>( and &lt;\\E1&gt; … &lt;\\En&gt; )<br>( or &lt;\\E1&gt; … &lt;\\En&gt; )<br>( not <e> )</e></p>\n</blockquote>\n<p>Then use the logic operators to define a predicate to evaluate if a number id larger or equal to the other one:</p>\n<pre><code class=\"python\">( define ( &gt;= x y )\n    ( or ( &gt; x y ) ( = x y ))\n</code></pre>\n<p>That is all the syntax, <strong>there is no loop in a functional programming language!</strong><br><br></p>\n<h2 id=\"Recursion\"><a href=\"#Recursion\" class=\"headerlink\" title=\"Recursion\"></a>Recursion</h2><p>Considering the factorial function:</p>\n<blockquote>\n<p>n! = n ⋅ (n-1) ⋅ (n-2) ⋅ … ⋅2⋅1</p>\n</blockquote>\n<p>Which can be computed as:</p>\n<blockquote>\n<p>n! = n ⋅ (n-1)!</p>\n</blockquote>\n<p>If we end it up with <strong>1!</strong>, then simply output <strong>1</strong>. Then the factorial function can be implemented in <strong><em>linear recursion</em></strong>:</p>\n<pre><code class=\"python\">( define ( factorial n )\n    ( if ( = n 1 )\n        1\n        ( * n ( factorial ( - n 1 )))))\n</code></pre>\n<p><strong><em>Linear recursion</em></strong> defines that the computation chains of operations is proportional to n and hence grows linearly. There is also another pattern of recursion, known as <strong><em>Tree Recursion</em></strong>. The best example will be the Fibonacci series, in which each element is the sum of the previous two:</p>\n<pre><code class=\"python\">( define ( fib n )\n    ( cond ( = n 0 ) 0 )\n           ( = n 1 ) 1 )\n           ( else ( + ( fib ( - n 1 ) )\n                      ( fib ( - n 2 ) )))))\n</code></pre>\n<p>You may find out that this procedure is not really efficient because to compute <strong>fib( - n 1)</strong>, <strong>fib( - n 2)</strong> has to be computed one more time which causes duplicated work.<br><img src=\"/2019/04/30/Prefix-Notation/images/treeRecursion.png\" alt=\"Tree Recursion\"><br>Therefore, instead of <strong><em>Tree Recursion</em></strong>, let’s try to convert it to be <strong><em>Linear Recursion</em></strong>. Reasign the sum of <strong>a</strong> and <strong>b</strong> to <strong>a</strong>, and the previous <strong>a</strong> to <strong>b</strong>:</p>\n<pre><code class=\"python\">( define ( fib n )\n    ( iterate 1 0 n ))\n\n( define ( iterate a b count )\n    ( if ( = count 0 )\n        b\n        ( iterate ( + a b ) a ( - count 1 ))))\n</code></pre>\n<p><br></p>\n<h2 id=\"Lambda\"><a href=\"#Lambda\" class=\"headerlink\" title=\"Lambda\"></a>Lambda</h2><p>Instead of defining some trivial procedures so that we can pass them as arguments of the other procedures, functional programming provides <strong><em>Lambda Expression</em></strong>:</p>\n<blockquote>\n<p>( lambda ( &lt;\\formal-param&gt; ) &lt;\\body&gt; )</p>\n</blockquote>\n<p>For instance,</p>\n<pre><code class=\"python\">( define ( Add a b ) ( + a b ))\n</code></pre>\n<p>can be written as:</p>\n<pre><code class=\"python\">( define add ( lambda ( a b ) ( + a b )))\n</code></pre>\n<p>And operators can also be represented by <strong><em>Lambda Expression</em></strong>:</p>\n<pre><code class=\"python\">(( lambda ( a b ) ( + ( * a a ) ( * b b ))) 2 3 )\n</code></pre>\n<p>Another use of <strong><em>Lambda Expression</em></strong> is creating local variables. An expression can be binded with a specific name by using keyword <strong><em>let</em></strong>. The above example then can be interpreted as:</p>\n<pre><code class=\"python\">( define ( sumsqr x y )\n    ( let ( a ( * x x ))\n          ( b ( * y y ))\n        ( + a b )))\n</code></pre>\n<p><strong><em>Note:</em></strong> The scope of a variable specified by a <strong><em>let</em></strong> is only applied to the <strong>body</strong> of the <strong><em>let</em></strong>. For example, if the evalue of <strong>x</strong> is <strong>2</strong>, then the expression:</p>\n<pre><code class=\"python\">( let (( x 3 )\n        ( y ( + x 2 )))\n    ( * x y ))\n</code></pre>\n<p>The value of <strong>y</strong> will be <strong>4</strong> as being outside of the <strong>let</strong> body, and the output will be <strong>3 * 4 = 12</strong>. It seems like <strong><em>let</em></strong> is really similar to <strong><em>define</em></strong>; however, in the most cases, we much prefer using <strong><em>let</em></strong> and only apply <strong><em>define</em></strong> to <strong>internal procedures</strong>.</p>"},{"title":"Suffix Tree","date":"2019-07-22T11:07:12.000Z","_content":"Suffix tree is a data structure aimming at solving **string** related problems in **linear time**\n<!-- more -->\n\n## String Match Algorithm\n\nA common string match problem always contains:\n- **Text** as an array of length n: T[n]\n- **Pattern** as an array of length m which m <=n: P[m]\n- **Valid Shift** which is the offset of the first character of the pattern showing up in the target text\n![String Match Problem Sample](PrefixTree_StringProblem.JPG)\nWe usually use two methods to solve string match problem:\n- Naive String Matching Algorithm\n- Knuth-Morris-Pratt Algorithm（KMP Algorithm)\n\nThe algorithm always consists with two steps: **Preprocessing** and **Matching**, and the total runtime will be accordingly the sum of two procedures.\n> Naive String Matching: O((n-m)m)\n> KMP: O(m + n)\nSuch algorithms are all doing preprocessings on the pattern to boost the searching procedure and the best runtime perfomance will be O(m) on preprocessing where m represents the length of the pattern. On contrary, is there a preprocessing which can be applied on **text** to speed up the whole process? This is the key reason why I am moving to **suffix tree**, which is a data structure doing preprocessing on text.\n</br>\n\n## Suffix Tree\n\nThe previous post used to introduce a prefix tree:\n![prefix tree sample](prefix_tree.JPG)\nIndividual nodes branch out from the same prefix. As you can see, there are some nodes which only have one child. Let's try to compress them together:\n![prefix tree sample](Compressed_PrefixTree.JPG)\nAfter the compression, we get a **Compressed Prefix Tree**. A compressed prefix tree, also called compressed tire is the fundamental of suffix tree. Besides, the key values stored in each nodes of a suffix tree is all the possible suffix.\n\nFor example, for a single word (**Text**) `banana\\0`, we have the following set of suffix:\n```\nbanana\\0\nanana\\0\nnana\\0\nana\\0\nna\\0\na\\0\n\\0\n```\nConstruct a prefix tree using the above key word:\n![Banana Prefix Tree](Banana_PrefixTree.JPG)\nAfterwards, compress it:\n![Banana Suffix Tree](Banana_SuffixTree.JPG)\nHere we get a suffix tree!\n\n\n\n","source":"_posts/Suffix-Tree.md","raw":"---\ntitle: Suffix Tree\ndate: 2019-07-22 20:07:12\ntags: [Suffix Tree]\n---\nSuffix tree is a data structure aimming at solving **string** related problems in **linear time**\n<!-- more -->\n\n## String Match Algorithm\n\nA common string match problem always contains:\n- **Text** as an array of length n: T[n]\n- **Pattern** as an array of length m which m <=n: P[m]\n- **Valid Shift** which is the offset of the first character of the pattern showing up in the target text\n![String Match Problem Sample](PrefixTree_StringProblem.JPG)\nWe usually use two methods to solve string match problem:\n- Naive String Matching Algorithm\n- Knuth-Morris-Pratt Algorithm（KMP Algorithm)\n\nThe algorithm always consists with two steps: **Preprocessing** and **Matching**, and the total runtime will be accordingly the sum of two procedures.\n> Naive String Matching: O((n-m)m)\n> KMP: O(m + n)\nSuch algorithms are all doing preprocessings on the pattern to boost the searching procedure and the best runtime perfomance will be O(m) on preprocessing where m represents the length of the pattern. On contrary, is there a preprocessing which can be applied on **text** to speed up the whole process? This is the key reason why I am moving to **suffix tree**, which is a data structure doing preprocessing on text.\n</br>\n\n## Suffix Tree\n\nThe previous post used to introduce a prefix tree:\n![prefix tree sample](prefix_tree.JPG)\nIndividual nodes branch out from the same prefix. As you can see, there are some nodes which only have one child. Let's try to compress them together:\n![prefix tree sample](Compressed_PrefixTree.JPG)\nAfter the compression, we get a **Compressed Prefix Tree**. A compressed prefix tree, also called compressed tire is the fundamental of suffix tree. Besides, the key values stored in each nodes of a suffix tree is all the possible suffix.\n\nFor example, for a single word (**Text**) `banana\\0`, we have the following set of suffix:\n```\nbanana\\0\nanana\\0\nnana\\0\nana\\0\nna\\0\na\\0\n\\0\n```\nConstruct a prefix tree using the above key word:\n![Banana Prefix Tree](Banana_PrefixTree.JPG)\nAfterwards, compress it:\n![Banana Suffix Tree](Banana_SuffixTree.JPG)\nHere we get a suffix tree!\n\n\n\n","slug":"Suffix-Tree","published":1,"updated":"2019-07-22T12:35:51.974Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyedivup0007t8aonhzgs8ph","content":"<p>Suffix tree is a data structure aimming at solving <strong>string</strong> related problems in <strong>linear time</strong><br><a id=\"more\"></a></p>\n<h2 id=\"String-Match-Algorithm\"><a href=\"#String-Match-Algorithm\" class=\"headerlink\" title=\"String Match Algorithm\"></a>String Match Algorithm</h2><p>A common string match problem always contains:</p>\n<ul>\n<li><strong>Text</strong> as an array of length n: T[n]</li>\n<li><strong>Pattern</strong> as an array of length m which m &lt;=n: P[m]</li>\n<li><strong>Valid Shift</strong> which is the offset of the first character of the pattern showing up in the target text<br><img src=\"/2019/07/22/Suffix-Tree/PrefixTree_StringProblem.JPG\" alt=\"String Match Problem Sample\"><br>We usually use two methods to solve string match problem:</li>\n<li>Naive String Matching Algorithm</li>\n<li>Knuth-Morris-Pratt Algorithm（KMP Algorithm)</li>\n</ul>\n<p>The algorithm always consists with two steps: <strong>Preprocessing</strong> and <strong>Matching</strong>, and the total runtime will be accordingly the sum of two procedures.</p>\n<blockquote>\n<p>Naive String Matching: O((n-m)m)<br>KMP: O(m + n)<br>Such algorithms are all doing preprocessings on the pattern to boost the searching procedure and the best runtime perfomance will be O(m) on preprocessing where m represents the length of the pattern. On contrary, is there a preprocessing which can be applied on <strong>text</strong> to speed up the whole process? This is the key reason why I am moving to <strong>suffix tree</strong>, which is a data structure doing preprocessing on text.<br><br></p>\n</blockquote>\n<h2 id=\"Suffix-Tree\"><a href=\"#Suffix-Tree\" class=\"headerlink\" title=\"Suffix Tree\"></a>Suffix Tree</h2><p>The previous post used to introduce a prefix tree:<br><img src=\"/2019/07/22/Suffix-Tree/prefix_tree.JPG\" alt=\"prefix tree sample\"><br>Individual nodes branch out from the same prefix. As you can see, there are some nodes which only have one child. Let’s try to compress them together:<br><img src=\"/2019/07/22/Suffix-Tree/Compressed_PrefixTree.JPG\" alt=\"prefix tree sample\"><br>After the compression, we get a <strong>Compressed Prefix Tree</strong>. A compressed prefix tree, also called compressed tire is the fundamental of suffix tree. Besides, the key values stored in each nodes of a suffix tree is all the possible suffix.</p>\n<p>For example, for a single word (<strong>Text</strong>) <code>banana\\0</code>, we have the following set of suffix:</p>\n<pre><code>banana\\0\nanana\\0\nnana\\0\nana\\0\nna\\0\na\\0\n\\0\n</code></pre><p>Construct a prefix tree using the above key word:<br><img src=\"/2019/07/22/Suffix-Tree/Banana_PrefixTree.JPG\" alt=\"Banana Prefix Tree\"><br>Afterwards, compress it:<br><img src=\"/2019/07/22/Suffix-Tree/Banana_SuffixTree.JPG\" alt=\"Banana Suffix Tree\"><br>Here we get a suffix tree!</p>\n","site":{"data":{}},"excerpt":"<p>Suffix tree is a data structure aimming at solving <strong>string</strong> related problems in <strong>linear time</strong><br></p>","more":"<p></p>\n<h2 id=\"String-Match-Algorithm\"><a href=\"#String-Match-Algorithm\" class=\"headerlink\" title=\"String Match Algorithm\"></a>String Match Algorithm</h2><p>A common string match problem always contains:</p>\n<ul>\n<li><strong>Text</strong> as an array of length n: T[n]</li>\n<li><strong>Pattern</strong> as an array of length m which m &lt;=n: P[m]</li>\n<li><strong>Valid Shift</strong> which is the offset of the first character of the pattern showing up in the target text<br><img src=\"/2019/07/22/Suffix-Tree/PrefixTree_StringProblem.JPG\" alt=\"String Match Problem Sample\"><br>We usually use two methods to solve string match problem:</li>\n<li>Naive String Matching Algorithm</li>\n<li>Knuth-Morris-Pratt Algorithm（KMP Algorithm)</li>\n</ul>\n<p>The algorithm always consists with two steps: <strong>Preprocessing</strong> and <strong>Matching</strong>, and the total runtime will be accordingly the sum of two procedures.</p>\n<blockquote>\n<p>Naive String Matching: O((n-m)m)<br>KMP: O(m + n)<br>Such algorithms are all doing preprocessings on the pattern to boost the searching procedure and the best runtime perfomance will be O(m) on preprocessing where m represents the length of the pattern. On contrary, is there a preprocessing which can be applied on <strong>text</strong> to speed up the whole process? This is the key reason why I am moving to <strong>suffix tree</strong>, which is a data structure doing preprocessing on text.<br><br></p>\n</blockquote>\n<h2 id=\"Suffix-Tree\"><a href=\"#Suffix-Tree\" class=\"headerlink\" title=\"Suffix Tree\"></a>Suffix Tree</h2><p>The previous post used to introduce a prefix tree:<br><img src=\"/2019/07/22/Suffix-Tree/prefix_tree.JPG\" alt=\"prefix tree sample\"><br>Individual nodes branch out from the same prefix. As you can see, there are some nodes which only have one child. Let’s try to compress them together:<br><img src=\"/2019/07/22/Suffix-Tree/Compressed_PrefixTree.JPG\" alt=\"prefix tree sample\"><br>After the compression, we get a <strong>Compressed Prefix Tree</strong>. A compressed prefix tree, also called compressed tire is the fundamental of suffix tree. Besides, the key values stored in each nodes of a suffix tree is all the possible suffix.</p>\n<p>For example, for a single word (<strong>Text</strong>) <code>banana\\0</code>, we have the following set of suffix:</p>\n<pre><code>banana\\0\nanana\\0\nnana\\0\nana\\0\nna\\0\na\\0\n\\0\n</code></pre><p>Construct a prefix tree using the above key word:<br><img src=\"/2019/07/22/Suffix-Tree/Banana_PrefixTree.JPG\" alt=\"Banana Prefix Tree\"><br>Afterwards, compress it:<br><img src=\"/2019/07/22/Suffix-Tree/Banana_SuffixTree.JPG\" alt=\"Banana Suffix Tree\"><br>Here we get a suffix tree!</p>"},{"title":"Slice in Golang","date":"2019-05-12T06:35:54.000Z","photos":["../images/GoSlice.JPG"],"_content":"This article is a summary from [Andrew Gerrand's blog](https://blog.golang.org/go-slices-usage-and-internals)\n\nGolang has an unique type **slice** which is an abstraction built on top of Go's **array** type. They are really similar but providing different means of working with sequences of typed data. So to understand slices we must first understand arrays.<!-- more -->\n</br>\n\n## Arrays in Go\nAn array in Go has to specify its **length** and **element type**. **The size of the array is fixed and its length is part of its type**. For example `[4]int` and `[5]int` are distinct and have different types even though they all store integers. And contrary to **C/C++**, the initial value of an array will be filled with **0** if it is not initialized.\n```Go\nvar a [4]int\na[0] = 1\ni := a[0]\nj := a[1]\n//i == 1\n//j == 0\n```\nGo's arrays are values. **An array variable denotes the entire array**; it is not a pointer to the first array element (as would be the case in C/C++). This means that when you assign or pass around an array value you will make a copy of its contents. (To avoid the copy you could pass a pointer to the array, but then that's a pointer to an array, not an array)\n\nAn array literal can be specified like so:\n```Go\nb := [2]string{\"aa\", \"bb\"}\n```\nOr, you can have the compiler counting the array elements for you:\n```Go\nb := [...]string{\"aa\", \"bb\"}\n```\nIn both cases, the type of b is **[2]string**.\n</br>\n\n## Slices in Go\nArrays are a bit inflexible, so you don't see them too often in the code. Slices, though, are everywhere. Unlike an array type, a slice type has no specific length:\n```Go\nb := []string{\"aa\", \"bb\"}\n```\nWe can use build-in function `make()` to define a slice:\n```Go\nfunc make([]T, len, cap) []T\n```\n**T** represent the type of the elements. Function **make** accepts type, length and capacity(optional) as parameters. When it is called, **make** will allocate an array and returns a slice that refers to that array\n```Go\nvar s []byte\ns = make([]byte, 5, 5)\n//s == []byte{0, 0, 0, 0, 0}\n```\nIf **cap** is not specified, it will be init as the value of **len**. We can use the build-in functions `len()` and `cap()` to check the length and capacity of a slice:\n```Go\nlen(s) == 5\ncap(s) == 5\n```\nThe zero value of a slice is **nil**. The len and cap functions will both return **0** for a nil slice.\n\nA slice can also be formed by \"slicing\" an existing slice or array, for example, the expression b[1:4] creates a slice including elements 1 through 3 of b:\n```Go\nb := []byte{'a', 'b', 'c', 'd', 'e', 'f'}\n// b[1:4] == []byte{'b', 'c', 'd'}, sharing the same storage as b\n```\nThe start and end indices of a slice expression are optional; they default to zero and the slice's length respectively:\n```Go\n// b[:2] == []byte{'a', 'b'}\n// b[2:] == []byte{'c', 'd', 'e', 'f'}\n// b[:] == b\n```\nThis is also the syntax to create a slice given an array:\n```Go\nx := [3]string{\"Лайка\", \"Белка\", \"Стрелка\"}\ns := x[:] // a slice referencing the storage of x\n```\n\nSlicing does not copy the slice's data. It creates a new slice value that points to the original array. This makes slice operations as efficient as manipulating array indices. Therefore, modifying the elements of a re-slice modifies the elements of the original slice:\n```Go\nd := []byte{'a', 'b', 'c', 'd'}\ne := d[2:]\n// e == []byte{'c', 'd'}\n\n// now change the re-slice will also change the original slice  \ne[1] = 'm'\n// e == []byte{'c', 'm'}\n// d == []byte{'a', 'b', 'c', 'm'}\n```\nA slice cannot be grown beyond its capacity. Attempting to do so will cause a ***runtime panic***, just as when indexing outside the bounds of a slice or array. Similarly, slices cannot be re-sliced below zero to access earlier elements in the array.\n</br>\n\n## Double the capacity of a slice\nTo increase the capacity of a slice, we must create a new, larger slice and **copy** the contents of the original slice into it. The belowing example shows how to create a new slice **t** whihc doubles the capacity of **s**:\n```Go\nt := make([]byte, len(s), (cap(s) * 2))\nfor i:= range s {\n    t[i] = s[i]\n}\ns = t   //reassign s to t\n```\nThe loop can be replaced by the build-in function `copy()`, which copies the data from source and returns the number of elements copied:\n```Go\nfunc copy(dst, src []T) int\n```\nThe function **copy** supports copying between slices of different lengths (it will copy only up to the smaller number of elements) and the case that two slices refer to the same array. Using **copy**, the above double size code snippet can be rewritten as:\n```Go\nt := make([]byte, len(s), (cap(s) * 2))\ncopy(t, s)\ns = t\n```\nA common operation is to append new data to the tail of a slice:\n```Go\nfunc AppendByte(slice []byte, data ...type) []byte {\n    m := len(slice)\n    n := m + len(data)\n    if n > cap(slice) { //if the original capacity is not big enough     \n        newSlice := make([]byte, (n + 1) * 2)\n        copy(newSlice, slice)\n        slice = newSlice\n    }\n    slice = slice[0:n] //shrink the capacity to the length of data  \n    copy(slice[m:n], data)\n    return slice\n}\n```\nThis customized AppendByte function is really useful because we can fully control the size of a slice. However most programs do need such complete control. Go provides a build-in function `append()` which appends slice x to the end of slice s, expanding s if needed:\n```Go\nfunc append(s []T, x ...T) []T\n```\nUsing **...** to append one slice to the end of another:\n```Go\na := []string{\"aa\", \"bb\"}\nb := []string{\"cc\", \"dd\"}\na = append(a, b...) //same as append(a, b[0], b[1], b[2])   \n```\nAnother example of append:\n```Go\nfunc Filter(s []int, fn func(int) bool) []int {\n    var p []int // p == nil\n    for _, v := range s {\n        if fn(v) {\n            p = append(p, v)\n        }\n    }\n    return p\n}\n```\n\n\n\n\n","source":"_posts/Type-slice-in-Golang.md","raw":"---\ntitle: Slice in Golang\ndate: 2019-05-12 15:35:54\ntags: [Golang, Array, Slice]\nphotos: [\"../images/GoSlice.JPG\"]\n---\nThis article is a summary from [Andrew Gerrand's blog](https://blog.golang.org/go-slices-usage-and-internals)\n\nGolang has an unique type **slice** which is an abstraction built on top of Go's **array** type. They are really similar but providing different means of working with sequences of typed data. So to understand slices we must first understand arrays.<!-- more -->\n</br>\n\n## Arrays in Go\nAn array in Go has to specify its **length** and **element type**. **The size of the array is fixed and its length is part of its type**. For example `[4]int` and `[5]int` are distinct and have different types even though they all store integers. And contrary to **C/C++**, the initial value of an array will be filled with **0** if it is not initialized.\n```Go\nvar a [4]int\na[0] = 1\ni := a[0]\nj := a[1]\n//i == 1\n//j == 0\n```\nGo's arrays are values. **An array variable denotes the entire array**; it is not a pointer to the first array element (as would be the case in C/C++). This means that when you assign or pass around an array value you will make a copy of its contents. (To avoid the copy you could pass a pointer to the array, but then that's a pointer to an array, not an array)\n\nAn array literal can be specified like so:\n```Go\nb := [2]string{\"aa\", \"bb\"}\n```\nOr, you can have the compiler counting the array elements for you:\n```Go\nb := [...]string{\"aa\", \"bb\"}\n```\nIn both cases, the type of b is **[2]string**.\n</br>\n\n## Slices in Go\nArrays are a bit inflexible, so you don't see them too often in the code. Slices, though, are everywhere. Unlike an array type, a slice type has no specific length:\n```Go\nb := []string{\"aa\", \"bb\"}\n```\nWe can use build-in function `make()` to define a slice:\n```Go\nfunc make([]T, len, cap) []T\n```\n**T** represent the type of the elements. Function **make** accepts type, length and capacity(optional) as parameters. When it is called, **make** will allocate an array and returns a slice that refers to that array\n```Go\nvar s []byte\ns = make([]byte, 5, 5)\n//s == []byte{0, 0, 0, 0, 0}\n```\nIf **cap** is not specified, it will be init as the value of **len**. We can use the build-in functions `len()` and `cap()` to check the length and capacity of a slice:\n```Go\nlen(s) == 5\ncap(s) == 5\n```\nThe zero value of a slice is **nil**. The len and cap functions will both return **0** for a nil slice.\n\nA slice can also be formed by \"slicing\" an existing slice or array, for example, the expression b[1:4] creates a slice including elements 1 through 3 of b:\n```Go\nb := []byte{'a', 'b', 'c', 'd', 'e', 'f'}\n// b[1:4] == []byte{'b', 'c', 'd'}, sharing the same storage as b\n```\nThe start and end indices of a slice expression are optional; they default to zero and the slice's length respectively:\n```Go\n// b[:2] == []byte{'a', 'b'}\n// b[2:] == []byte{'c', 'd', 'e', 'f'}\n// b[:] == b\n```\nThis is also the syntax to create a slice given an array:\n```Go\nx := [3]string{\"Лайка\", \"Белка\", \"Стрелка\"}\ns := x[:] // a slice referencing the storage of x\n```\n\nSlicing does not copy the slice's data. It creates a new slice value that points to the original array. This makes slice operations as efficient as manipulating array indices. Therefore, modifying the elements of a re-slice modifies the elements of the original slice:\n```Go\nd := []byte{'a', 'b', 'c', 'd'}\ne := d[2:]\n// e == []byte{'c', 'd'}\n\n// now change the re-slice will also change the original slice  \ne[1] = 'm'\n// e == []byte{'c', 'm'}\n// d == []byte{'a', 'b', 'c', 'm'}\n```\nA slice cannot be grown beyond its capacity. Attempting to do so will cause a ***runtime panic***, just as when indexing outside the bounds of a slice or array. Similarly, slices cannot be re-sliced below zero to access earlier elements in the array.\n</br>\n\n## Double the capacity of a slice\nTo increase the capacity of a slice, we must create a new, larger slice and **copy** the contents of the original slice into it. The belowing example shows how to create a new slice **t** whihc doubles the capacity of **s**:\n```Go\nt := make([]byte, len(s), (cap(s) * 2))\nfor i:= range s {\n    t[i] = s[i]\n}\ns = t   //reassign s to t\n```\nThe loop can be replaced by the build-in function `copy()`, which copies the data from source and returns the number of elements copied:\n```Go\nfunc copy(dst, src []T) int\n```\nThe function **copy** supports copying between slices of different lengths (it will copy only up to the smaller number of elements) and the case that two slices refer to the same array. Using **copy**, the above double size code snippet can be rewritten as:\n```Go\nt := make([]byte, len(s), (cap(s) * 2))\ncopy(t, s)\ns = t\n```\nA common operation is to append new data to the tail of a slice:\n```Go\nfunc AppendByte(slice []byte, data ...type) []byte {\n    m := len(slice)\n    n := m + len(data)\n    if n > cap(slice) { //if the original capacity is not big enough     \n        newSlice := make([]byte, (n + 1) * 2)\n        copy(newSlice, slice)\n        slice = newSlice\n    }\n    slice = slice[0:n] //shrink the capacity to the length of data  \n    copy(slice[m:n], data)\n    return slice\n}\n```\nThis customized AppendByte function is really useful because we can fully control the size of a slice. However most programs do need such complete control. Go provides a build-in function `append()` which appends slice x to the end of slice s, expanding s if needed:\n```Go\nfunc append(s []T, x ...T) []T\n```\nUsing **...** to append one slice to the end of another:\n```Go\na := []string{\"aa\", \"bb\"}\nb := []string{\"cc\", \"dd\"}\na = append(a, b...) //same as append(a, b[0], b[1], b[2])   \n```\nAnother example of append:\n```Go\nfunc Filter(s []int, fn func(int) bool) []int {\n    var p []int // p == nil\n    for _, v := range s {\n        if fn(v) {\n            p = append(p, v)\n        }\n    }\n    return p\n}\n```\n\n\n\n\n","slug":"Type-slice-in-Golang","published":1,"updated":"2019-05-22T08:47:15.136Z","comments":1,"layout":"post","link":"","_id":"cjyedivur0009t8ao5963efhv","content":"<p>This article is a summary from <a href=\"https://blog.golang.org/go-slices-usage-and-internals\" target=\"_blank\" rel=\"noopener\">Andrew Gerrand’s blog</a></p>\n<p>Golang has an unique type <strong>slice</strong> which is an abstraction built on top of Go’s <strong>array</strong> type. They are really similar but providing different means of working with sequences of typed data. So to understand slices we must first understand arrays.<a id=\"more\"></a><br><br></p>\n<h2 id=\"Arrays-in-Go\"><a href=\"#Arrays-in-Go\" class=\"headerlink\" title=\"Arrays in Go\"></a>Arrays in Go</h2><p>An array in Go has to specify its <strong>length</strong> and <strong>element type</strong>. <strong>The size of the array is fixed and its length is part of its type</strong>. For example <code>[4]int</code> and <code>[5]int</code> are distinct and have different types even though they all store integers. And contrary to <strong>C/C++</strong>, the initial value of an array will be filled with <strong>0</strong> if it is not initialized.</p>\n<pre><code class=\"Go\">var a [4]int\na[0] = 1\ni := a[0]\nj := a[1]\n//i == 1\n//j == 0\n</code></pre>\n<p>Go’s arrays are values. <strong>An array variable denotes the entire array</strong>; it is not a pointer to the first array element (as would be the case in C/C++). This means that when you assign or pass around an array value you will make a copy of its contents. (To avoid the copy you could pass a pointer to the array, but then that’s a pointer to an array, not an array)</p>\n<p>An array literal can be specified like so:</p>\n<pre><code class=\"Go\">b := [2]string{&quot;aa&quot;, &quot;bb&quot;}\n</code></pre>\n<p>Or, you can have the compiler counting the array elements for you:</p>\n<pre><code class=\"Go\">b := [...]string{&quot;aa&quot;, &quot;bb&quot;}\n</code></pre>\n<p>In both cases, the type of b is <strong>[2]string</strong>.<br><br></p>\n<h2 id=\"Slices-in-Go\"><a href=\"#Slices-in-Go\" class=\"headerlink\" title=\"Slices in Go\"></a>Slices in Go</h2><p>Arrays are a bit inflexible, so you don’t see them too often in the code. Slices, though, are everywhere. Unlike an array type, a slice type has no specific length:</p>\n<pre><code class=\"Go\">b := []string{&quot;aa&quot;, &quot;bb&quot;}\n</code></pre>\n<p>We can use build-in function <code>make()</code> to define a slice:</p>\n<pre><code class=\"Go\">func make([]T, len, cap) []T\n</code></pre>\n<p><strong>T</strong> represent the type of the elements. Function <strong>make</strong> accepts type, length and capacity(optional) as parameters. When it is called, <strong>make</strong> will allocate an array and returns a slice that refers to that array</p>\n<pre><code class=\"Go\">var s []byte\ns = make([]byte, 5, 5)\n//s == []byte{0, 0, 0, 0, 0}\n</code></pre>\n<p>If <strong>cap</strong> is not specified, it will be init as the value of <strong>len</strong>. We can use the build-in functions <code>len()</code> and <code>cap()</code> to check the length and capacity of a slice:</p>\n<pre><code class=\"Go\">len(s) == 5\ncap(s) == 5\n</code></pre>\n<p>The zero value of a slice is <strong>nil</strong>. The len and cap functions will both return <strong>0</strong> for a nil slice.</p>\n<p>A slice can also be formed by “slicing” an existing slice or array, for example, the expression b[1:4] creates a slice including elements 1 through 3 of b:</p>\n<pre><code class=\"Go\">b := []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;}\n// b[1:4] == []byte{&#39;b&#39;, &#39;c&#39;, &#39;d&#39;}, sharing the same storage as b\n</code></pre>\n<p>The start and end indices of a slice expression are optional; they default to zero and the slice’s length respectively:</p>\n<pre><code class=\"Go\">// b[:2] == []byte{&#39;a&#39;, &#39;b&#39;}\n// b[2:] == []byte{&#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;}\n// b[:] == b\n</code></pre>\n<p>This is also the syntax to create a slice given an array:</p>\n<pre><code class=\"Go\">x := [3]string{&quot;Лайка&quot;, &quot;Белка&quot;, &quot;Стрелка&quot;}\ns := x[:] // a slice referencing the storage of x\n</code></pre>\n<p>Slicing does not copy the slice’s data. It creates a new slice value that points to the original array. This makes slice operations as efficient as manipulating array indices. Therefore, modifying the elements of a re-slice modifies the elements of the original slice:</p>\n<pre><code class=\"Go\">d := []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;}\ne := d[2:]\n// e == []byte{&#39;c&#39;, &#39;d&#39;}\n\n// now change the re-slice will also change the original slice  \ne[1] = &#39;m&#39;\n// e == []byte{&#39;c&#39;, &#39;m&#39;}\n// d == []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;m&#39;}\n</code></pre>\n<p>A slice cannot be grown beyond its capacity. Attempting to do so will cause a <strong><em>runtime panic</em></strong>, just as when indexing outside the bounds of a slice or array. Similarly, slices cannot be re-sliced below zero to access earlier elements in the array.<br><br></p>\n<h2 id=\"Double-the-capacity-of-a-slice\"><a href=\"#Double-the-capacity-of-a-slice\" class=\"headerlink\" title=\"Double the capacity of a slice\"></a>Double the capacity of a slice</h2><p>To increase the capacity of a slice, we must create a new, larger slice and <strong>copy</strong> the contents of the original slice into it. The belowing example shows how to create a new slice <strong>t</strong> whihc doubles the capacity of <strong>s</strong>:</p>\n<pre><code class=\"Go\">t := make([]byte, len(s), (cap(s) * 2))\nfor i:= range s {\n    t[i] = s[i]\n}\ns = t   //reassign s to t\n</code></pre>\n<p>The loop can be replaced by the build-in function <code>copy()</code>, which copies the data from source and returns the number of elements copied:</p>\n<pre><code class=\"Go\">func copy(dst, src []T) int\n</code></pre>\n<p>The function <strong>copy</strong> supports copying between slices of different lengths (it will copy only up to the smaller number of elements) and the case that two slices refer to the same array. Using <strong>copy</strong>, the above double size code snippet can be rewritten as:</p>\n<pre><code class=\"Go\">t := make([]byte, len(s), (cap(s) * 2))\ncopy(t, s)\ns = t\n</code></pre>\n<p>A common operation is to append new data to the tail of a slice:</p>\n<pre><code class=\"Go\">func AppendByte(slice []byte, data ...type) []byte {\n    m := len(slice)\n    n := m + len(data)\n    if n &gt; cap(slice) { //if the original capacity is not big enough     \n        newSlice := make([]byte, (n + 1) * 2)\n        copy(newSlice, slice)\n        slice = newSlice\n    }\n    slice = slice[0:n] //shrink the capacity to the length of data  \n    copy(slice[m:n], data)\n    return slice\n}\n</code></pre>\n<p>This customized AppendByte function is really useful because we can fully control the size of a slice. However most programs do need such complete control. Go provides a build-in function <code>append()</code> which appends slice x to the end of slice s, expanding s if needed:</p>\n<pre><code class=\"Go\">func append(s []T, x ...T) []T\n</code></pre>\n<p>Using <strong>…</strong> to append one slice to the end of another:</p>\n<pre><code class=\"Go\">a := []string{&quot;aa&quot;, &quot;bb&quot;}\nb := []string{&quot;cc&quot;, &quot;dd&quot;}\na = append(a, b...) //same as append(a, b[0], b[1], b[2])   \n</code></pre>\n<p>Another example of append:</p>\n<pre><code class=\"Go\">func Filter(s []int, fn func(int) bool) []int {\n    var p []int // p == nil\n    for _, v := range s {\n        if fn(v) {\n            p = append(p, v)\n        }\n    }\n    return p\n}\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>This article is a summary from <a href=\"https://blog.golang.org/go-slices-usage-and-internals\" target=\"_blank\" rel=\"noopener\">Andrew Gerrand’s blog</a></p>\n<p>Golang has an unique type <strong>slice</strong> which is an abstraction built on top of Go’s <strong>array</strong> type. They are really similar but providing different means of working with sequences of typed data. So to understand slices we must first understand arrays.</p>","more":"<br><br><p></p>\n<h2 id=\"Arrays-in-Go\"><a href=\"#Arrays-in-Go\" class=\"headerlink\" title=\"Arrays in Go\"></a>Arrays in Go</h2><p>An array in Go has to specify its <strong>length</strong> and <strong>element type</strong>. <strong>The size of the array is fixed and its length is part of its type</strong>. For example <code>[4]int</code> and <code>[5]int</code> are distinct and have different types even though they all store integers. And contrary to <strong>C/C++</strong>, the initial value of an array will be filled with <strong>0</strong> if it is not initialized.</p>\n<pre><code class=\"Go\">var a [4]int\na[0] = 1\ni := a[0]\nj := a[1]\n//i == 1\n//j == 0\n</code></pre>\n<p>Go’s arrays are values. <strong>An array variable denotes the entire array</strong>; it is not a pointer to the first array element (as would be the case in C/C++). This means that when you assign or pass around an array value you will make a copy of its contents. (To avoid the copy you could pass a pointer to the array, but then that’s a pointer to an array, not an array)</p>\n<p>An array literal can be specified like so:</p>\n<pre><code class=\"Go\">b := [2]string{&quot;aa&quot;, &quot;bb&quot;}\n</code></pre>\n<p>Or, you can have the compiler counting the array elements for you:</p>\n<pre><code class=\"Go\">b := [...]string{&quot;aa&quot;, &quot;bb&quot;}\n</code></pre>\n<p>In both cases, the type of b is <strong>[2]string</strong>.<br><br></p>\n<h2 id=\"Slices-in-Go\"><a href=\"#Slices-in-Go\" class=\"headerlink\" title=\"Slices in Go\"></a>Slices in Go</h2><p>Arrays are a bit inflexible, so you don’t see them too often in the code. Slices, though, are everywhere. Unlike an array type, a slice type has no specific length:</p>\n<pre><code class=\"Go\">b := []string{&quot;aa&quot;, &quot;bb&quot;}\n</code></pre>\n<p>We can use build-in function <code>make()</code> to define a slice:</p>\n<pre><code class=\"Go\">func make([]T, len, cap) []T\n</code></pre>\n<p><strong>T</strong> represent the type of the elements. Function <strong>make</strong> accepts type, length and capacity(optional) as parameters. When it is called, <strong>make</strong> will allocate an array and returns a slice that refers to that array</p>\n<pre><code class=\"Go\">var s []byte\ns = make([]byte, 5, 5)\n//s == []byte{0, 0, 0, 0, 0}\n</code></pre>\n<p>If <strong>cap</strong> is not specified, it will be init as the value of <strong>len</strong>. We can use the build-in functions <code>len()</code> and <code>cap()</code> to check the length and capacity of a slice:</p>\n<pre><code class=\"Go\">len(s) == 5\ncap(s) == 5\n</code></pre>\n<p>The zero value of a slice is <strong>nil</strong>. The len and cap functions will both return <strong>0</strong> for a nil slice.</p>\n<p>A slice can also be formed by “slicing” an existing slice or array, for example, the expression b[1:4] creates a slice including elements 1 through 3 of b:</p>\n<pre><code class=\"Go\">b := []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;}\n// b[1:4] == []byte{&#39;b&#39;, &#39;c&#39;, &#39;d&#39;}, sharing the same storage as b\n</code></pre>\n<p>The start and end indices of a slice expression are optional; they default to zero and the slice’s length respectively:</p>\n<pre><code class=\"Go\">// b[:2] == []byte{&#39;a&#39;, &#39;b&#39;}\n// b[2:] == []byte{&#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;}\n// b[:] == b\n</code></pre>\n<p>This is also the syntax to create a slice given an array:</p>\n<pre><code class=\"Go\">x := [3]string{&quot;Лайка&quot;, &quot;Белка&quot;, &quot;Стрелка&quot;}\ns := x[:] // a slice referencing the storage of x\n</code></pre>\n<p>Slicing does not copy the slice’s data. It creates a new slice value that points to the original array. This makes slice operations as efficient as manipulating array indices. Therefore, modifying the elements of a re-slice modifies the elements of the original slice:</p>\n<pre><code class=\"Go\">d := []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;}\ne := d[2:]\n// e == []byte{&#39;c&#39;, &#39;d&#39;}\n\n// now change the re-slice will also change the original slice  \ne[1] = &#39;m&#39;\n// e == []byte{&#39;c&#39;, &#39;m&#39;}\n// d == []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;m&#39;}\n</code></pre>\n<p>A slice cannot be grown beyond its capacity. Attempting to do so will cause a <strong><em>runtime panic</em></strong>, just as when indexing outside the bounds of a slice or array. Similarly, slices cannot be re-sliced below zero to access earlier elements in the array.<br><br></p>\n<h2 id=\"Double-the-capacity-of-a-slice\"><a href=\"#Double-the-capacity-of-a-slice\" class=\"headerlink\" title=\"Double the capacity of a slice\"></a>Double the capacity of a slice</h2><p>To increase the capacity of a slice, we must create a new, larger slice and <strong>copy</strong> the contents of the original slice into it. The belowing example shows how to create a new slice <strong>t</strong> whihc doubles the capacity of <strong>s</strong>:</p>\n<pre><code class=\"Go\">t := make([]byte, len(s), (cap(s) * 2))\nfor i:= range s {\n    t[i] = s[i]\n}\ns = t   //reassign s to t\n</code></pre>\n<p>The loop can be replaced by the build-in function <code>copy()</code>, which copies the data from source and returns the number of elements copied:</p>\n<pre><code class=\"Go\">func copy(dst, src []T) int\n</code></pre>\n<p>The function <strong>copy</strong> supports copying between slices of different lengths (it will copy only up to the smaller number of elements) and the case that two slices refer to the same array. Using <strong>copy</strong>, the above double size code snippet can be rewritten as:</p>\n<pre><code class=\"Go\">t := make([]byte, len(s), (cap(s) * 2))\ncopy(t, s)\ns = t\n</code></pre>\n<p>A common operation is to append new data to the tail of a slice:</p>\n<pre><code class=\"Go\">func AppendByte(slice []byte, data ...type) []byte {\n    m := len(slice)\n    n := m + len(data)\n    if n &gt; cap(slice) { //if the original capacity is not big enough     \n        newSlice := make([]byte, (n + 1) * 2)\n        copy(newSlice, slice)\n        slice = newSlice\n    }\n    slice = slice[0:n] //shrink the capacity to the length of data  \n    copy(slice[m:n], data)\n    return slice\n}\n</code></pre>\n<p>This customized AppendByte function is really useful because we can fully control the size of a slice. However most programs do need such complete control. Go provides a build-in function <code>append()</code> which appends slice x to the end of slice s, expanding s if needed:</p>\n<pre><code class=\"Go\">func append(s []T, x ...T) []T\n</code></pre>\n<p>Using <strong>…</strong> to append one slice to the end of another:</p>\n<pre><code class=\"Go\">a := []string{&quot;aa&quot;, &quot;bb&quot;}\nb := []string{&quot;cc&quot;, &quot;dd&quot;}\na = append(a, b...) //same as append(a, b[0], b[1], b[2])   \n</code></pre>\n<p>Another example of append:</p>\n<pre><code class=\"Go\">func Filter(s []int, fn func(int) bool) []int {\n    var p []int // p == nil\n    for _, v := range s {\n        if fn(v) {\n            p = append(p, v)\n        }\n    }\n    return p\n}\n</code></pre>"},{"title":"如何用VS Code免翻墙听网易云音乐","date":"2019-05-22T07:27:00.000Z","photos":["../images/neteaseMusic.JPG"],"_content":"在墙外打开网易云音乐发现全是灰色的？ 抱歉您所在区域无法播放？ 该资源暂无版权？ 需要vip？\nVS Code**一行script**全搞定～～ <!-- more -->\n\n- 打开VS Code\n\n- 左侧 Extensions 搜索 Netease Music (VSC Netease Music) 或者点[这里](https://marketplace.visualstudio.com/items?itemName=nondanee.vsc-netease-music)\n\n- 点击 Install 后重启 VS Code\n\n- 作者提供了基于 VS Code 自身插件工具 Webview 实现的通过替换 electron 动态链接库翻墙的插件, 有详细的[中文文档](https://marketplace.visualstudio.com/items?itemName=nondanee.vsc-netease-music), 这里 **Unix Shell** 用户（包括 **MacOS**）可以直接在 **Terminal** （任意dir）输入以下script:\n```\ncurl https://gist.githubusercontent.com/nondanee/f157bbbccecfe29e48d87273cd02e213/raw | python\n```\n  script 输出结果为:\n```\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  3060  100  3060    0     0   8496      0 --:--:-- --:--:-- --:--:--  8476  \nvscode 1.34.0 x64\nelectron 3.1.8\ndownload well\nreplace done\nremove temp\n```\n\n- 替换完毕后打开 VS Code, 上方工具栏 Go -> Go to file... 或者 `Command(⌘) P`, 输入：\n```\n>NeteaseMusic: Start\n```\n  等待 editor 跳出 **Please preserve this webview tab** 后就可以使用所有网易云的功能了, 注意这个tab页面必须要保留（不能关闭）\n\n- 使用时直接`Command(⌘) P**`后输入命令即可, 命令都是以 `>NeteaseMusic`起头, 输入 `>ne` 后VS Code会自动跳出并补齐可行指令, 甚至还可以登陆收藏查看评论","source":"_posts/如何用VS-Code免翻墙听网易云音乐.md","raw":"---\ntitle: 如何用VS Code免翻墙听网易云音乐\ndate: 2019-05-22 16:27:00\ntags: [VS Code, NeteaseMusic, 网易云]\nphotos: [\"../images/neteaseMusic.JPG\"]\n---\n在墙外打开网易云音乐发现全是灰色的？ 抱歉您所在区域无法播放？ 该资源暂无版权？ 需要vip？\nVS Code**一行script**全搞定～～ <!-- more -->\n\n- 打开VS Code\n\n- 左侧 Extensions 搜索 Netease Music (VSC Netease Music) 或者点[这里](https://marketplace.visualstudio.com/items?itemName=nondanee.vsc-netease-music)\n\n- 点击 Install 后重启 VS Code\n\n- 作者提供了基于 VS Code 自身插件工具 Webview 实现的通过替换 electron 动态链接库翻墙的插件, 有详细的[中文文档](https://marketplace.visualstudio.com/items?itemName=nondanee.vsc-netease-music), 这里 **Unix Shell** 用户（包括 **MacOS**）可以直接在 **Terminal** （任意dir）输入以下script:\n```\ncurl https://gist.githubusercontent.com/nondanee/f157bbbccecfe29e48d87273cd02e213/raw | python\n```\n  script 输出结果为:\n```\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  3060  100  3060    0     0   8496      0 --:--:-- --:--:-- --:--:--  8476  \nvscode 1.34.0 x64\nelectron 3.1.8\ndownload well\nreplace done\nremove temp\n```\n\n- 替换完毕后打开 VS Code, 上方工具栏 Go -> Go to file... 或者 `Command(⌘) P`, 输入：\n```\n>NeteaseMusic: Start\n```\n  等待 editor 跳出 **Please preserve this webview tab** 后就可以使用所有网易云的功能了, 注意这个tab页面必须要保留（不能关闭）\n\n- 使用时直接`Command(⌘) P**`后输入命令即可, 命令都是以 `>NeteaseMusic`起头, 输入 `>ne` 后VS Code会自动跳出并补齐可行指令, 甚至还可以登陆收藏查看评论","slug":"如何用VS-Code免翻墙听网易云音乐","published":1,"updated":"2019-05-22T08:47:42.391Z","comments":1,"layout":"post","link":"","_id":"cjyedivus000at8ao0b5j0ykq","content":"<p>在墙外打开网易云音乐发现全是灰色的？ 抱歉您所在区域无法播放？ 该资源暂无版权？ 需要vip？<br>VS Code<strong>一行script</strong>全搞定～～ <a id=\"more\"></a></p>\n<ul>\n<li><p>打开VS Code</p>\n</li>\n<li><p>左侧 Extensions 搜索 Netease Music (VSC Netease Music) 或者点<a href=\"https://marketplace.visualstudio.com/items?itemName=nondanee.vsc-netease-music\" target=\"_blank\" rel=\"noopener\">这里</a></p>\n</li>\n<li><p>点击 Install 后重启 VS Code</p>\n</li>\n<li><p>作者提供了基于 VS Code 自身插件工具 Webview 实现的通过替换 electron 动态链接库翻墙的插件, 有详细的<a href=\"https://marketplace.visualstudio.com/items?itemName=nondanee.vsc-netease-music\" target=\"_blank\" rel=\"noopener\">中文文档</a>, 这里 <strong>Unix Shell</strong> 用户（包括 <strong>MacOS</strong>）可以直接在 <strong>Terminal</strong> （任意dir）输入以下script:</p>\n<pre><code>curl https://gist.githubusercontent.com/nondanee/f157bbbccecfe29e48d87273cd02e213/raw | python\n</code></pre><p>script 输出结果为:</p>\n<pre><code>% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                               Dload  Upload   Total   Spent    Left  Speed\n100  3060  100  3060    0     0   8496      0 --:--:-- --:--:-- --:--:--  8476  \nvscode 1.34.0 x64\nelectron 3.1.8\ndownload well\nreplace done\nremove temp\n</code></pre></li>\n<li><p>替换完毕后打开 VS Code, 上方工具栏 Go -&gt; Go to file… 或者 <code>Command(⌘) P</code>, 输入：</p>\n<pre><code>&gt;NeteaseMusic: Start\n</code></pre><p>等待 editor 跳出 <strong>Please preserve this webview tab</strong> 后就可以使用所有网易云的功能了, 注意这个tab页面必须要保留（不能关闭）</p>\n</li>\n<li><p>使用时直接<code>Command(⌘) P**</code>后输入命令即可, 命令都是以 <code>&gt;NeteaseMusic</code>起头, 输入 <code>&gt;ne</code> 后VS Code会自动跳出并补齐可行指令, 甚至还可以登陆收藏查看评论</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>在墙外打开网易云音乐发现全是灰色的？ 抱歉您所在区域无法播放？ 该资源暂无版权？ 需要vip？<br>VS Code<strong>一行script</strong>全搞定～～</p>","more":"<p></p>\n<ul>\n<li><p>打开VS Code</p>\n</li>\n<li><p>左侧 Extensions 搜索 Netease Music (VSC Netease Music) 或者点<a href=\"https://marketplace.visualstudio.com/items?itemName=nondanee.vsc-netease-music\" target=\"_blank\" rel=\"noopener\">这里</a></p>\n</li>\n<li><p>点击 Install 后重启 VS Code</p>\n</li>\n<li><p>作者提供了基于 VS Code 自身插件工具 Webview 实现的通过替换 electron 动态链接库翻墙的插件, 有详细的<a href=\"https://marketplace.visualstudio.com/items?itemName=nondanee.vsc-netease-music\" target=\"_blank\" rel=\"noopener\">中文文档</a>, 这里 <strong>Unix Shell</strong> 用户（包括 <strong>MacOS</strong>）可以直接在 <strong>Terminal</strong> （任意dir）输入以下script:</p>\n<pre><code>curl https://gist.githubusercontent.com/nondanee/f157bbbccecfe29e48d87273cd02e213/raw | python\n</code></pre><p>script 输出结果为:</p>\n<pre><code>% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                               Dload  Upload   Total   Spent    Left  Speed\n100  3060  100  3060    0     0   8496      0 --:--:-- --:--:-- --:--:--  8476  \nvscode 1.34.0 x64\nelectron 3.1.8\ndownload well\nreplace done\nremove temp\n</code></pre></li>\n<li><p>替换完毕后打开 VS Code, 上方工具栏 Go -&gt; Go to file… 或者 <code>Command(⌘) P</code>, 输入：</p>\n<pre><code>&gt;NeteaseMusic: Start\n</code></pre><p>等待 editor 跳出 <strong>Please preserve this webview tab</strong> 后就可以使用所有网易云的功能了, 注意这个tab页面必须要保留（不能关闭）</p>\n</li>\n<li><p>使用时直接<code>Command(⌘) P**</code>后输入命令即可, 命令都是以 <code>&gt;NeteaseMusic</code>起头, 输入 <code>&gt;ne</code> 后VS Code会自动跳出并补齐可行指令, 甚至还可以登陆收藏查看评论</p>\n</li>\n</ul>"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).<!-- more -->\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).<!-- more -->\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2019-04-27T07:51:40.239Z","updated":"2019-04-29T07:18:13.367Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjyedivuz000ct8aofjfg3ljl","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.<a id=\"more\"></a></p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre><code class=\"bash\">$ hexo new &quot;My New Post&quot;\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre><code class=\"bash\">$ hexo server\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre><code class=\"bash\">$ hexo generate\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre><code class=\"bash\">$ hexo deploy\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>","more":"<p></p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre><code class=\"bash\">$ hexo new &quot;My New Post&quot;\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre><code class=\"bash\">$ hexo server\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre><code class=\"bash\">$ hexo generate\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre><code class=\"bash\">$ hexo deploy\n</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>"},{"title":"A Brief Introduce to Redux Saga","date":"2019-04-28T05:35:50.000Z","photos":["../images/redux-saga.JPG"],"_content":"If you are quite experienced with redux, which is a predictable state container for JavaScript applications (**Note:** even thouth React and Redux is a popular combination to build fast and powerful apps, Redux is not necessarily combined with React), you are definitely feeling comfortable with its powerful store which manages all the global states and provides much cleaner logic flows to change them. If you are new to redux, [here](https://redux.js.org/introduction/getting-started) is the guide to dive before we start our topic.<!-- more -->\n\nIn a complex javascript application, asynchronous function is always one of the most annoying part where encounters tons of bugs. If not handle them properly, the app usually ends up with ***call back hell***.\n</br>\n\n## **Haven't heard of *CallBack Hell*?**\nWell, in javascript, the only way you can suspend a computation and have the rest operations doing later is to put the rest operations into a callback function. This callback function usually returns a **`Promise`** (And has a type of **`Promise<any>`**). In order to easily mark those async functions, after ***ES6*** javascript provides extra modifiers **`async`** and **`await`**, which actually wraps up the original utilities of promise and makes it more readable to programmers. Hummm, sounds like things are going better... ~~**NO!! It doesn't resolve anything!**~~ The core problem leads to a callback hell is the hierarchical async calls, for example\n\nyou have some simple synchronous functions which are in a chain to accomplish some logics:\n```javascript\na = getSomething();\nb = getMore(a);\nc = getMoreAndMore(b);\n...\n```\nIt looks fine for now, but what if they all turn out to be async? Then you have to follow the callback style to make them operate one right after another is done:\n```javascript\ngetSomthing(function(a) {\n    getMore(a, function(b) {\n        getMoreAndMore(b, function(c) {\n            //keep going...\n        });\n    });\n});\n```\nOr you prefer ***ES6***:\n```javascript\nasync function getSomething(a) {\n    await b = ToDo(a);\n    return await getMore((b) => {\n        return await ToDo(b);\n    }).then((c) => {\n        return await ToDo(c);\n    }).then(...);\n}\n```\nLooks really confused? This will getting even uglier if we are using callbacks in loops. \n</br>\n## Redux Thunks\nBack to our redux app, we usually want to update some states after an async call to inform the UI that the data is ready to be fetched. That is always achieved by dispatching an action from the component to the reducer:\n```javascript\nasync const callAPI = () => {\n    ...\n    return response;\n};\n...\nasync const updateUI = (...params) => {\n    const res = await callAPI();\n    if (res.status === 200)\n        dispatch({type: \"UPDATE\", isSuccess: true});\n};\n...\nrender({\n    ...\n    this.props.isSuccess?\n        showData() : showError()\n});\n```\nThis isn't bad, but we are always looking for something better. An advanced way to rewrite it is using redux middleware. ***Middleware*** is somewhere you can put the code after the framework receives a request and before it generates a response. For example, we want to add a logger in the redux store so that when the store catches an action, before it returns the new state, the logger can log the previous state and the new generated state. This is what can be added as a middleware:\n```javascript\nfunction logger(store) {\n    return function wrapDispatch (next) {\n        return function dispatchAndLog (action) {\n            console.log(\"dispatching.. \", action);\n            let result = next(action);\n            console.log(\"new state\", store.getState());\n            return result;\n        }\n    }\n}\n```\nThere are more advanced ways to add a logger. If you are interested, please refer to the [offical documentation](https://redux.js.org/advanced/middleware). With our middleware, the previous example can be written in a cleaner way:\n```javascript\nconst callAPI = () => {\n    return((dispatch) => {\n        dispatch(startCallingApiAction);\n        actualCallApi().then(data => {\n            dispatch(successAction(data));\n        }).fail(err => {\n            dispatch(failedAction(err));\n        });\n    });\n};\n```\nThe successful response data is wrapped in the payload of the action, sent to the reducer. Once the store updates the data, it will be mapped as a prop back to the component and request for a rerender. This middleware is also called ***thunk***. By applying thunk to decouple the presentation layer, we can get rid of most of the side effects in components, instead, managing and orchestrating side effects in thunks.\n\nThis is great, so why are we even considering ***saga***? Well, one of the advantages of middleware is that it can be chained. Every middleware mounted in redux store starts an individual thread (or something really looks like a thread in ***NodeJS***). When a middleware captures an action and handles its side effect, it can dispatch a new action to another middleware to do nested logics. This behavior of middleware indicates that thunks can be chained as well, for example thunkA forwards its return payload to thunkB and thunkB forwards its return payload to... **Wait! That sounds quite familiar!! Is that the case of callback hell??** Unfortunately, a good thing plus another good feature doesn't always end up with something better. ~~It could be some shit as well (笑)~~ In this case, true, this is exactly the callback hell.\n</br>\n\n## Redux Saga\nTo handle the possible endless callback functions and also to make it more easily to test in a component which has complicated logics, we need to change our previous thoughts. Just like shifting from Process Oriented Programming to Object Oriented Programming, instead of telling the application how to handle the side effects, suppose it already knows how to call a function and how to dispatch an action, all we need to do is to **give instructions about what to do next** and we don't care about how those instructions will be executed (Saga handles the executions).\n\nThen the thunks example can be changed as following:\n```javascript\nexport function* apiSideEffect(action) {\n    try{\n        const data = yield call(actualCallApi);\n        yield put({ type:\"SUCCESS\", payload: data });\n    } catch(err) {\n        yield put({ type:\"FAILED\", payload: err });\n    }\n}\n\nexport function* apiSaga() {\n    yield takeEvery(\"CLICK_TO_CALL_API\", apiSideEffect);\n}\n```\nThere are serval fucntions already being integrated in Saga:\n>**`Call`**: the method call will return only a plain object describing the operation so redux-saga can take care of the invocation and returns the result to the generator. The first parameter is the generator function ready to be called and the rest params are all the arguments in the generator.\n\n>**`Put`**: Instead of dispatching an action inside the generator (Don't ever ever do that), ***put*** Returns an object with instructions for the middleware to dispatch the action.\n\n>**`Select`**: Returns value from the selector function, similar with **`getState()`**. ***Note:*** It is not recommended to use this function because it returns the value corresponding to the contents of the store state tree, which is most likely a plain Javascript object and is **mutable** (Redux wants you to handle state immutably, which means return a new state instead of changing the old one).\n\n>**`Take`**: It creates a command object that tells the middleware to wait for a specific action. The resulting behavior of the call Effect is the same as when the middleware suspends the generator until a ***promise*** resolves. In the take case, it'll suspend the generator until a matching action is dispatched\n\nBy working with Saga, we make the side effects to be ***declarative*** rather than ***imperative***.\n>***Declarative:*** describing what the program must accomplish, rather than describe how to accomplish it\n\n>***Imperative:*** consists of commands for the computer to perform, focuses on describing how a program operates\n\nIn the case of take, the control is inverted. Instead of the actions being pushed to the handler tasks, the **Saga is pulling the action by itself**. An additional generator, known as ***watcher*** which contains ***take*** has to be created to watch a specific action and being triggered once the following action is dispatched in the application. There are two ways to create a watcher, one is using the buid-in functions (***Saga Helper***):\n```javascript\nfunction* watchFetchData() {\n    yield takeEvery(\"FETCH_REQUEST\", callFetchDataApi);\n}\n```\n***takeEvery*** allows multiple request to be proceeding at the same time. Or if you just want the latest request to be fired (the older one will be overrided during each time the watcher is triggered\n```javascript\nfunction* watchFetchData() {\n    yield takeLatest(\"FETCH_REQUEST\", callFetchDataApi);\n}\n```\nHowever by using ***take***, it is possible to fully control an action observation process to build complex control flow:\n```javascript\nfunction* watchFetchData() {\n    while(true) {\n        const action = yield take(\"FETCH_REQUEST\");\n        console.log(action);\n        yield call(callFetchDataApi, action.payload);\n    }\n}\n```\nAll right, now you have been exposed to everything you need to know before start trying redux saga on your own. Here is a short overall example that may also help:\nStore:\n```javascript\nconst sagaMiddleware = createSagaMiddleware();\nconst store = createStore(rootReducer, appluMiddleware(sagaMiddleware));    \nsagaMiddleware.run(watchFetch);\n```\nSagas:\n```javascript\nfunction* watchFetch(): Generator<*, *, *> {\n    yield takeEvery(\"FETCH_ACTION\", callFetchAPI);\n}\n\nfunction* callFetchAPI(): Generator<*, *, *> {\n    try {\n        yield put({ type: \"FETCHING\", payload: ... });\n        const data = yield call(actualCallApi);\n        yield put({ type: \"FETCH_SUCCESS\", payload: data });\n    } catch(err) {\n        yield put({ type: \"FETCH_FAILED\", payload: err });\n    }\n}\n```\nReducer:\n```javascript\nconst reducer = (state = initState, action) => {\n    switch(action) {\n        case \"FETCHING\":\n            return { loading: true, ...state };\n        case \"FETCH_SUCCESS\":\n            return { loading: false, success: true, data: action.payload, ...state };   \n        case \"FETCH_FAILED\":\n            return { loading: false, success: false, error: true, ...state };\n        default:\n            return { ...state };\n    }\n};\n```\nComponent:\n```javascript\nclass myComponent extends React.Component {\n    const mapStateToProps = ...\n    const mapDispatchToProps = ...\n    render() {\n        return (\n            <button onClick = { () => this.props.dispatch({type: \"FETCH_ACTION\"}) }/>   \n            {\n                this.props.loading?\n                    <p>Loading..</p> : this.props.error?\n                        <p>Error!</p> : <p>{this.props.data}</p>\n            }\n        );\n    }\n}\nexport default connect(mapStateToProps, mapDispatchToProps)(myComponent);\n\n```\nFor more advanced concepts, there is a well-organized [Saga offical documentation](https://redux-saga.js.org/docs/advanced/) you can refer to if you want to dive deeper.\n</br>\n\n## How to test Saga?\nA function that returns a simple object is easier to test than a function that directly makes an asynchronous call. For redux saga, each time you yield a function call will return a plain javascript object which makes the workflow much easier to test. You don’t need to use the real API, fake it, or mock it, instead just iterating over the generator function, asserting for equality on the values yielded.\n```javascript\ndescribe(\"fetch work flow\", () => {\n    const generator = cloneableGenerator(callFetchAPI)({ type: \"FETCH_ACTION\" });\n    expect(generator.next().value).toEqual(put({ type: \"FETCHING\", payload: ... }));    \n\n    test(\"fetch success\", () => {\n        const clone = generator.clone();\n        expect(clone.next().value).toEqual(put({ type: \"FETCH_SUCCESS\" }));\n        expect(generator.next().done).toEqual(true);\n    });\n});\n```\nIn the above example, we use **`clone()`** to test different control flows and **`next()`** to iterate to the next function ready be yielded. The mock return value can also be injected as an argument of **`next()`**:\n```javascript\nexpect(clone.next(false).value).toEqual( put(fetchFailedAction()) );      \n```\n</br>\n\n## Saga vs Observables\nRedux saga is not the only solution to our apps which may have complex control flows, they are other helpful tools providing different trade-offs which can also resolve the async problems. Here are some good [code snippets](https://hackmd.io/s/H1xLHUQ8e) of saga vs observables that can open your mind :D\n\n</br>\n</br>\n## References:\nhttps://redux-saga.js.org/\nhttps://stackoverflow.com/questions/25098066/what-is-callback-hell-and-how-and-why-rx-solves-it\nhttps://redux.js.org/advanced/middleware\nhttps://pub.dartlang.org/packages/redux_thunk\nhttps://codeburst.io/how-i-test-redux-saga-fcc425cda018\nhttps://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1\nhttps://www.sitepoint.com/redux-without-react-state-management-vanilla-javascript/\nhttps://redux.js.org/introduction/getting-started\nhttps://blog.logrocket.com/understanding-redux-saga-from-action-creators-to-sagas-2587298b5e71\n","source":"_posts/A-Brief-Introduce-to-Redux-Saga.md","raw":"---\ntitle: A Brief Introduce to Redux Saga\ndate: 2019-04-28 14:35:50\ntags: [Redux, Saga, React]\nphotos: [\"../images/redux-saga.JPG\"]\n---\nIf you are quite experienced with redux, which is a predictable state container for JavaScript applications (**Note:** even thouth React and Redux is a popular combination to build fast and powerful apps, Redux is not necessarily combined with React), you are definitely feeling comfortable with its powerful store which manages all the global states and provides much cleaner logic flows to change them. If you are new to redux, [here](https://redux.js.org/introduction/getting-started) is the guide to dive before we start our topic.<!-- more -->\n\nIn a complex javascript application, asynchronous function is always one of the most annoying part where encounters tons of bugs. If not handle them properly, the app usually ends up with ***call back hell***.\n</br>\n\n## **Haven't heard of *CallBack Hell*?**\nWell, in javascript, the only way you can suspend a computation and have the rest operations doing later is to put the rest operations into a callback function. This callback function usually returns a **`Promise`** (And has a type of **`Promise<any>`**). In order to easily mark those async functions, after ***ES6*** javascript provides extra modifiers **`async`** and **`await`**, which actually wraps up the original utilities of promise and makes it more readable to programmers. Hummm, sounds like things are going better... ~~**NO!! It doesn't resolve anything!**~~ The core problem leads to a callback hell is the hierarchical async calls, for example\n\nyou have some simple synchronous functions which are in a chain to accomplish some logics:\n```javascript\na = getSomething();\nb = getMore(a);\nc = getMoreAndMore(b);\n...\n```\nIt looks fine for now, but what if they all turn out to be async? Then you have to follow the callback style to make them operate one right after another is done:\n```javascript\ngetSomthing(function(a) {\n    getMore(a, function(b) {\n        getMoreAndMore(b, function(c) {\n            //keep going...\n        });\n    });\n});\n```\nOr you prefer ***ES6***:\n```javascript\nasync function getSomething(a) {\n    await b = ToDo(a);\n    return await getMore((b) => {\n        return await ToDo(b);\n    }).then((c) => {\n        return await ToDo(c);\n    }).then(...);\n}\n```\nLooks really confused? This will getting even uglier if we are using callbacks in loops. \n</br>\n## Redux Thunks\nBack to our redux app, we usually want to update some states after an async call to inform the UI that the data is ready to be fetched. That is always achieved by dispatching an action from the component to the reducer:\n```javascript\nasync const callAPI = () => {\n    ...\n    return response;\n};\n...\nasync const updateUI = (...params) => {\n    const res = await callAPI();\n    if (res.status === 200)\n        dispatch({type: \"UPDATE\", isSuccess: true});\n};\n...\nrender({\n    ...\n    this.props.isSuccess?\n        showData() : showError()\n});\n```\nThis isn't bad, but we are always looking for something better. An advanced way to rewrite it is using redux middleware. ***Middleware*** is somewhere you can put the code after the framework receives a request and before it generates a response. For example, we want to add a logger in the redux store so that when the store catches an action, before it returns the new state, the logger can log the previous state and the new generated state. This is what can be added as a middleware:\n```javascript\nfunction logger(store) {\n    return function wrapDispatch (next) {\n        return function dispatchAndLog (action) {\n            console.log(\"dispatching.. \", action);\n            let result = next(action);\n            console.log(\"new state\", store.getState());\n            return result;\n        }\n    }\n}\n```\nThere are more advanced ways to add a logger. If you are interested, please refer to the [offical documentation](https://redux.js.org/advanced/middleware). With our middleware, the previous example can be written in a cleaner way:\n```javascript\nconst callAPI = () => {\n    return((dispatch) => {\n        dispatch(startCallingApiAction);\n        actualCallApi().then(data => {\n            dispatch(successAction(data));\n        }).fail(err => {\n            dispatch(failedAction(err));\n        });\n    });\n};\n```\nThe successful response data is wrapped in the payload of the action, sent to the reducer. Once the store updates the data, it will be mapped as a prop back to the component and request for a rerender. This middleware is also called ***thunk***. By applying thunk to decouple the presentation layer, we can get rid of most of the side effects in components, instead, managing and orchestrating side effects in thunks.\n\nThis is great, so why are we even considering ***saga***? Well, one of the advantages of middleware is that it can be chained. Every middleware mounted in redux store starts an individual thread (or something really looks like a thread in ***NodeJS***). When a middleware captures an action and handles its side effect, it can dispatch a new action to another middleware to do nested logics. This behavior of middleware indicates that thunks can be chained as well, for example thunkA forwards its return payload to thunkB and thunkB forwards its return payload to... **Wait! That sounds quite familiar!! Is that the case of callback hell??** Unfortunately, a good thing plus another good feature doesn't always end up with something better. ~~It could be some shit as well (笑)~~ In this case, true, this is exactly the callback hell.\n</br>\n\n## Redux Saga\nTo handle the possible endless callback functions and also to make it more easily to test in a component which has complicated logics, we need to change our previous thoughts. Just like shifting from Process Oriented Programming to Object Oriented Programming, instead of telling the application how to handle the side effects, suppose it already knows how to call a function and how to dispatch an action, all we need to do is to **give instructions about what to do next** and we don't care about how those instructions will be executed (Saga handles the executions).\n\nThen the thunks example can be changed as following:\n```javascript\nexport function* apiSideEffect(action) {\n    try{\n        const data = yield call(actualCallApi);\n        yield put({ type:\"SUCCESS\", payload: data });\n    } catch(err) {\n        yield put({ type:\"FAILED\", payload: err });\n    }\n}\n\nexport function* apiSaga() {\n    yield takeEvery(\"CLICK_TO_CALL_API\", apiSideEffect);\n}\n```\nThere are serval fucntions already being integrated in Saga:\n>**`Call`**: the method call will return only a plain object describing the operation so redux-saga can take care of the invocation and returns the result to the generator. The first parameter is the generator function ready to be called and the rest params are all the arguments in the generator.\n\n>**`Put`**: Instead of dispatching an action inside the generator (Don't ever ever do that), ***put*** Returns an object with instructions for the middleware to dispatch the action.\n\n>**`Select`**: Returns value from the selector function, similar with **`getState()`**. ***Note:*** It is not recommended to use this function because it returns the value corresponding to the contents of the store state tree, which is most likely a plain Javascript object and is **mutable** (Redux wants you to handle state immutably, which means return a new state instead of changing the old one).\n\n>**`Take`**: It creates a command object that tells the middleware to wait for a specific action. The resulting behavior of the call Effect is the same as when the middleware suspends the generator until a ***promise*** resolves. In the take case, it'll suspend the generator until a matching action is dispatched\n\nBy working with Saga, we make the side effects to be ***declarative*** rather than ***imperative***.\n>***Declarative:*** describing what the program must accomplish, rather than describe how to accomplish it\n\n>***Imperative:*** consists of commands for the computer to perform, focuses on describing how a program operates\n\nIn the case of take, the control is inverted. Instead of the actions being pushed to the handler tasks, the **Saga is pulling the action by itself**. An additional generator, known as ***watcher*** which contains ***take*** has to be created to watch a specific action and being triggered once the following action is dispatched in the application. There are two ways to create a watcher, one is using the buid-in functions (***Saga Helper***):\n```javascript\nfunction* watchFetchData() {\n    yield takeEvery(\"FETCH_REQUEST\", callFetchDataApi);\n}\n```\n***takeEvery*** allows multiple request to be proceeding at the same time. Or if you just want the latest request to be fired (the older one will be overrided during each time the watcher is triggered\n```javascript\nfunction* watchFetchData() {\n    yield takeLatest(\"FETCH_REQUEST\", callFetchDataApi);\n}\n```\nHowever by using ***take***, it is possible to fully control an action observation process to build complex control flow:\n```javascript\nfunction* watchFetchData() {\n    while(true) {\n        const action = yield take(\"FETCH_REQUEST\");\n        console.log(action);\n        yield call(callFetchDataApi, action.payload);\n    }\n}\n```\nAll right, now you have been exposed to everything you need to know before start trying redux saga on your own. Here is a short overall example that may also help:\nStore:\n```javascript\nconst sagaMiddleware = createSagaMiddleware();\nconst store = createStore(rootReducer, appluMiddleware(sagaMiddleware));    \nsagaMiddleware.run(watchFetch);\n```\nSagas:\n```javascript\nfunction* watchFetch(): Generator<*, *, *> {\n    yield takeEvery(\"FETCH_ACTION\", callFetchAPI);\n}\n\nfunction* callFetchAPI(): Generator<*, *, *> {\n    try {\n        yield put({ type: \"FETCHING\", payload: ... });\n        const data = yield call(actualCallApi);\n        yield put({ type: \"FETCH_SUCCESS\", payload: data });\n    } catch(err) {\n        yield put({ type: \"FETCH_FAILED\", payload: err });\n    }\n}\n```\nReducer:\n```javascript\nconst reducer = (state = initState, action) => {\n    switch(action) {\n        case \"FETCHING\":\n            return { loading: true, ...state };\n        case \"FETCH_SUCCESS\":\n            return { loading: false, success: true, data: action.payload, ...state };   \n        case \"FETCH_FAILED\":\n            return { loading: false, success: false, error: true, ...state };\n        default:\n            return { ...state };\n    }\n};\n```\nComponent:\n```javascript\nclass myComponent extends React.Component {\n    const mapStateToProps = ...\n    const mapDispatchToProps = ...\n    render() {\n        return (\n            <button onClick = { () => this.props.dispatch({type: \"FETCH_ACTION\"}) }/>   \n            {\n                this.props.loading?\n                    <p>Loading..</p> : this.props.error?\n                        <p>Error!</p> : <p>{this.props.data}</p>\n            }\n        );\n    }\n}\nexport default connect(mapStateToProps, mapDispatchToProps)(myComponent);\n\n```\nFor more advanced concepts, there is a well-organized [Saga offical documentation](https://redux-saga.js.org/docs/advanced/) you can refer to if you want to dive deeper.\n</br>\n\n## How to test Saga?\nA function that returns a simple object is easier to test than a function that directly makes an asynchronous call. For redux saga, each time you yield a function call will return a plain javascript object which makes the workflow much easier to test. You don’t need to use the real API, fake it, or mock it, instead just iterating over the generator function, asserting for equality on the values yielded.\n```javascript\ndescribe(\"fetch work flow\", () => {\n    const generator = cloneableGenerator(callFetchAPI)({ type: \"FETCH_ACTION\" });\n    expect(generator.next().value).toEqual(put({ type: \"FETCHING\", payload: ... }));    \n\n    test(\"fetch success\", () => {\n        const clone = generator.clone();\n        expect(clone.next().value).toEqual(put({ type: \"FETCH_SUCCESS\" }));\n        expect(generator.next().done).toEqual(true);\n    });\n});\n```\nIn the above example, we use **`clone()`** to test different control flows and **`next()`** to iterate to the next function ready be yielded. The mock return value can also be injected as an argument of **`next()`**:\n```javascript\nexpect(clone.next(false).value).toEqual( put(fetchFailedAction()) );      \n```\n</br>\n\n## Saga vs Observables\nRedux saga is not the only solution to our apps which may have complex control flows, they are other helpful tools providing different trade-offs which can also resolve the async problems. Here are some good [code snippets](https://hackmd.io/s/H1xLHUQ8e) of saga vs observables that can open your mind :D\n\n</br>\n</br>\n## References:\nhttps://redux-saga.js.org/\nhttps://stackoverflow.com/questions/25098066/what-is-callback-hell-and-how-and-why-rx-solves-it\nhttps://redux.js.org/advanced/middleware\nhttps://pub.dartlang.org/packages/redux_thunk\nhttps://codeburst.io/how-i-test-redux-saga-fcc425cda018\nhttps://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1\nhttps://www.sitepoint.com/redux-without-react-state-management-vanilla-javascript/\nhttps://redux.js.org/introduction/getting-started\nhttps://blog.logrocket.com/understanding-redux-saga-from-action-creators-to-sagas-2587298b5e71\n","slug":"A-Brief-Introduce-to-Redux-Saga","published":1,"updated":"2019-05-10T10:01:52.394Z","comments":1,"layout":"post","link":"","_id":"cjyedivvr0018t8aoodq7yluq","content":"<p>If you are quite experienced with redux, which is a predictable state container for JavaScript applications (<strong>Note:</strong> even thouth React and Redux is a popular combination to build fast and powerful apps, Redux is not necessarily combined with React), you are definitely feeling comfortable with its powerful store which manages all the global states and provides much cleaner logic flows to change them. If you are new to redux, <a href=\"https://redux.js.org/introduction/getting-started\" target=\"_blank\" rel=\"noopener\">here</a> is the guide to dive before we start our topic.<a id=\"more\"></a></p>\n<p>In a complex javascript application, asynchronous function is always one of the most annoying part where encounters tons of bugs. If not handle them properly, the app usually ends up with <strong><em>call back hell</em></strong>.<br><br></p>\n<h2 id=\"Haven’t-heard-of-CallBack-Hell\"><a href=\"#Haven’t-heard-of-CallBack-Hell\" class=\"headerlink\" title=\"Haven’t heard of CallBack Hell?\"></a><strong>Haven’t heard of <em>CallBack Hell</em>?</strong></h2><p>Well, in javascript, the only way you can suspend a computation and have the rest operations doing later is to put the rest operations into a callback function. This callback function usually returns a <strong><code>Promise</code></strong> (And has a type of <strong><code>Promise&lt;any&gt;</code></strong>). In order to easily mark those async functions, after <strong><em>ES6</em></strong> javascript provides extra modifiers <strong><code>async</code></strong> and <strong><code>await</code></strong>, which actually wraps up the original utilities of promise and makes it more readable to programmers. Hummm, sounds like things are going better… <del><strong>NO!! It doesn’t resolve anything!</strong></del> The core problem leads to a callback hell is the hierarchical async calls, for example</p>\n<p>you have some simple synchronous functions which are in a chain to accomplish some logics:</p>\n<pre><code class=\"javascript\">a = getSomething();\nb = getMore(a);\nc = getMoreAndMore(b);\n...\n</code></pre>\n<p>It looks fine for now, but what if they all turn out to be async? Then you have to follow the callback style to make them operate one right after another is done:</p>\n<pre><code class=\"javascript\">getSomthing(function(a) {\n    getMore(a, function(b) {\n        getMoreAndMore(b, function(c) {\n            //keep going...\n        });\n    });\n});\n</code></pre>\n<p>Or you prefer <strong><em>ES6</em></strong>:</p>\n<pre><code class=\"javascript\">async function getSomething(a) {\n    await b = ToDo(a);\n    return await getMore((b) =&gt; {\n        return await ToDo(b);\n    }).then((c) =&gt; {\n        return await ToDo(c);\n    }).then(...);\n}\n</code></pre>\n<p>Looks really confused? This will getting even uglier if we are using callbacks in loops.<br><br></p>\n<h2 id=\"Redux-Thunks\"><a href=\"#Redux-Thunks\" class=\"headerlink\" title=\"Redux Thunks\"></a>Redux Thunks</h2><p>Back to our redux app, we usually want to update some states after an async call to inform the UI that the data is ready to be fetched. That is always achieved by dispatching an action from the component to the reducer:</p>\n<pre><code class=\"javascript\">async const callAPI = () =&gt; {\n    ...\n    return response;\n};\n...\nasync const updateUI = (...params) =&gt; {\n    const res = await callAPI();\n    if (res.status === 200)\n        dispatch({type: &quot;UPDATE&quot;, isSuccess: true});\n};\n...\nrender({\n    ...\n    this.props.isSuccess?\n        showData() : showError()\n});\n</code></pre>\n<p>This isn’t bad, but we are always looking for something better. An advanced way to rewrite it is using redux middleware. <strong><em>Middleware</em></strong> is somewhere you can put the code after the framework receives a request and before it generates a response. For example, we want to add a logger in the redux store so that when the store catches an action, before it returns the new state, the logger can log the previous state and the new generated state. This is what can be added as a middleware:</p>\n<pre><code class=\"javascript\">function logger(store) {\n    return function wrapDispatch (next) {\n        return function dispatchAndLog (action) {\n            console.log(&quot;dispatching.. &quot;, action);\n            let result = next(action);\n            console.log(&quot;new state&quot;, store.getState());\n            return result;\n        }\n    }\n}\n</code></pre>\n<p>There are more advanced ways to add a logger. If you are interested, please refer to the <a href=\"https://redux.js.org/advanced/middleware\" target=\"_blank\" rel=\"noopener\">offical documentation</a>. With our middleware, the previous example can be written in a cleaner way:</p>\n<pre><code class=\"javascript\">const callAPI = () =&gt; {\n    return((dispatch) =&gt; {\n        dispatch(startCallingApiAction);\n        actualCallApi().then(data =&gt; {\n            dispatch(successAction(data));\n        }).fail(err =&gt; {\n            dispatch(failedAction(err));\n        });\n    });\n};\n</code></pre>\n<p>The successful response data is wrapped in the payload of the action, sent to the reducer. Once the store updates the data, it will be mapped as a prop back to the component and request for a rerender. This middleware is also called <strong><em>thunk</em></strong>. By applying thunk to decouple the presentation layer, we can get rid of most of the side effects in components, instead, managing and orchestrating side effects in thunks.</p>\n<p>This is great, so why are we even considering <strong><em>saga</em></strong>? Well, one of the advantages of middleware is that it can be chained. Every middleware mounted in redux store starts an individual thread (or something really looks like a thread in <strong><em>NodeJS</em></strong>). When a middleware captures an action and handles its side effect, it can dispatch a new action to another middleware to do nested logics. This behavior of middleware indicates that thunks can be chained as well, for example thunkA forwards its return payload to thunkB and thunkB forwards its return payload to… <strong>Wait! That sounds quite familiar!! Is that the case of callback hell??</strong> Unfortunately, a good thing plus another good feature doesn’t always end up with something better. <del>It could be some shit as well (笑)</del> In this case, true, this is exactly the callback hell.<br><br></p>\n<h2 id=\"Redux-Saga\"><a href=\"#Redux-Saga\" class=\"headerlink\" title=\"Redux Saga\"></a>Redux Saga</h2><p>To handle the possible endless callback functions and also to make it more easily to test in a component which has complicated logics, we need to change our previous thoughts. Just like shifting from Process Oriented Programming to Object Oriented Programming, instead of telling the application how to handle the side effects, suppose it already knows how to call a function and how to dispatch an action, all we need to do is to <strong>give instructions about what to do next</strong> and we don’t care about how those instructions will be executed (Saga handles the executions).</p>\n<p>Then the thunks example can be changed as following:</p>\n<pre><code class=\"javascript\">export function* apiSideEffect(action) {\n    try{\n        const data = yield call(actualCallApi);\n        yield put({ type:&quot;SUCCESS&quot;, payload: data });\n    } catch(err) {\n        yield put({ type:&quot;FAILED&quot;, payload: err });\n    }\n}\n\nexport function* apiSaga() {\n    yield takeEvery(&quot;CLICK_TO_CALL_API&quot;, apiSideEffect);\n}\n</code></pre>\n<p>There are serval fucntions already being integrated in Saga:</p>\n<blockquote>\n<p><strong><code>Call</code></strong>: the method call will return only a plain object describing the operation so redux-saga can take care of the invocation and returns the result to the generator. The first parameter is the generator function ready to be called and the rest params are all the arguments in the generator.</p>\n</blockquote>\n<blockquote>\n<p><strong><code>Put</code></strong>: Instead of dispatching an action inside the generator (Don’t ever ever do that), <strong><em>put</em></strong> Returns an object with instructions for the middleware to dispatch the action.</p>\n</blockquote>\n<blockquote>\n<p><strong><code>Select</code></strong>: Returns value from the selector function, similar with <strong><code>getState()</code></strong>. <strong><em>Note:</em></strong> It is not recommended to use this function because it returns the value corresponding to the contents of the store state tree, which is most likely a plain Javascript object and is <strong>mutable</strong> (Redux wants you to handle state immutably, which means return a new state instead of changing the old one).</p>\n</blockquote>\n<blockquote>\n<p><strong><code>Take</code></strong>: It creates a command object that tells the middleware to wait for a specific action. The resulting behavior of the call Effect is the same as when the middleware suspends the generator until a <strong><em>promise</em></strong> resolves. In the take case, it’ll suspend the generator until a matching action is dispatched</p>\n</blockquote>\n<p>By working with Saga, we make the side effects to be <strong><em>declarative</em></strong> rather than <strong><em>imperative</em></strong>.</p>\n<blockquote>\n<p><strong><em>Declarative:</em></strong> describing what the program must accomplish, rather than describe how to accomplish it</p>\n</blockquote>\n<blockquote>\n<p><strong><em>Imperative:</em></strong> consists of commands for the computer to perform, focuses on describing how a program operates</p>\n</blockquote>\n<p>In the case of take, the control is inverted. Instead of the actions being pushed to the handler tasks, the <strong>Saga is pulling the action by itself</strong>. An additional generator, known as <strong><em>watcher</em></strong> which contains <strong><em>take</em></strong> has to be created to watch a specific action and being triggered once the following action is dispatched in the application. There are two ways to create a watcher, one is using the buid-in functions (<strong><em>Saga Helper</em></strong>):</p>\n<pre><code class=\"javascript\">function* watchFetchData() {\n    yield takeEvery(&quot;FETCH_REQUEST&quot;, callFetchDataApi);\n}\n</code></pre>\n<p><strong><em>takeEvery</em></strong> allows multiple request to be proceeding at the same time. Or if you just want the latest request to be fired (the older one will be overrided during each time the watcher is triggered</p>\n<pre><code class=\"javascript\">function* watchFetchData() {\n    yield takeLatest(&quot;FETCH_REQUEST&quot;, callFetchDataApi);\n}\n</code></pre>\n<p>However by using <strong><em>take</em></strong>, it is possible to fully control an action observation process to build complex control flow:</p>\n<pre><code class=\"javascript\">function* watchFetchData() {\n    while(true) {\n        const action = yield take(&quot;FETCH_REQUEST&quot;);\n        console.log(action);\n        yield call(callFetchDataApi, action.payload);\n    }\n}\n</code></pre>\n<p>All right, now you have been exposed to everything you need to know before start trying redux saga on your own. Here is a short overall example that may also help:<br>Store:</p>\n<pre><code class=\"javascript\">const sagaMiddleware = createSagaMiddleware();\nconst store = createStore(rootReducer, appluMiddleware(sagaMiddleware));    \nsagaMiddleware.run(watchFetch);\n</code></pre>\n<p>Sagas:</p>\n<pre><code class=\"javascript\">function* watchFetch(): Generator&lt;*, *, *&gt; {\n    yield takeEvery(&quot;FETCH_ACTION&quot;, callFetchAPI);\n}\n\nfunction* callFetchAPI(): Generator&lt;*, *, *&gt; {\n    try {\n        yield put({ type: &quot;FETCHING&quot;, payload: ... });\n        const data = yield call(actualCallApi);\n        yield put({ type: &quot;FETCH_SUCCESS&quot;, payload: data });\n    } catch(err) {\n        yield put({ type: &quot;FETCH_FAILED&quot;, payload: err });\n    }\n}\n</code></pre>\n<p>Reducer:</p>\n<pre><code class=\"javascript\">const reducer = (state = initState, action) =&gt; {\n    switch(action) {\n        case &quot;FETCHING&quot;:\n            return { loading: true, ...state };\n        case &quot;FETCH_SUCCESS&quot;:\n            return { loading: false, success: true, data: action.payload, ...state };   \n        case &quot;FETCH_FAILED&quot;:\n            return { loading: false, success: false, error: true, ...state };\n        default:\n            return { ...state };\n    }\n};\n</code></pre>\n<p>Component:</p>\n<pre><code class=\"javascript\">class myComponent extends React.Component {\n    const mapStateToProps = ...\n    const mapDispatchToProps = ...\n    render() {\n        return (\n            &lt;button onClick = { () =&gt; this.props.dispatch({type: &quot;FETCH_ACTION&quot;}) }/&gt;   \n            {\n                this.props.loading?\n                    &lt;p&gt;Loading..&lt;/p&gt; : this.props.error?\n                        &lt;p&gt;Error!&lt;/p&gt; : &lt;p&gt;{this.props.data}&lt;/p&gt;\n            }\n        );\n    }\n}\nexport default connect(mapStateToProps, mapDispatchToProps)(myComponent);\n\n</code></pre>\n<p>For more advanced concepts, there is a well-organized <a href=\"https://redux-saga.js.org/docs/advanced/\" target=\"_blank\" rel=\"noopener\">Saga offical documentation</a> you can refer to if you want to dive deeper.<br><br></p>\n<h2 id=\"How-to-test-Saga\"><a href=\"#How-to-test-Saga\" class=\"headerlink\" title=\"How to test Saga?\"></a>How to test Saga?</h2><p>A function that returns a simple object is easier to test than a function that directly makes an asynchronous call. For redux saga, each time you yield a function call will return a plain javascript object which makes the workflow much easier to test. You don’t need to use the real API, fake it, or mock it, instead just iterating over the generator function, asserting for equality on the values yielded.</p>\n<pre><code class=\"javascript\">describe(&quot;fetch work flow&quot;, () =&gt; {\n    const generator = cloneableGenerator(callFetchAPI)({ type: &quot;FETCH_ACTION&quot; });\n    expect(generator.next().value).toEqual(put({ type: &quot;FETCHING&quot;, payload: ... }));    \n\n    test(&quot;fetch success&quot;, () =&gt; {\n        const clone = generator.clone();\n        expect(clone.next().value).toEqual(put({ type: &quot;FETCH_SUCCESS&quot; }));\n        expect(generator.next().done).toEqual(true);\n    });\n});\n</code></pre>\n<p>In the above example, we use <strong><code>clone()</code></strong> to test different control flows and <strong><code>next()</code></strong> to iterate to the next function ready be yielded. The mock return value can also be injected as an argument of <strong><code>next()</code></strong>:</p>\n<pre><code class=\"javascript\">expect(clone.next(false).value).toEqual( put(fetchFailedAction()) );      \n</code></pre>\n<p><br></p>\n<h2 id=\"Saga-vs-Observables\"><a href=\"#Saga-vs-Observables\" class=\"headerlink\" title=\"Saga vs Observables\"></a>Saga vs Observables</h2><p>Redux saga is not the only solution to our apps which may have complex control flows, they are other helpful tools providing different trade-offs which can also resolve the async problems. Here are some good <a href=\"https://hackmd.io/s/H1xLHUQ8e\" target=\"_blank\" rel=\"noopener\">code snippets</a> of saga vs observables that can open your mind :D</p>\n<p><br><br><br></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References:\"></a>References:</h2><p><a href=\"https://redux-saga.js.org/\" target=\"_blank\" rel=\"noopener\">https://redux-saga.js.org/</a><br><a href=\"https://stackoverflow.com/questions/25098066/what-is-callback-hell-and-how-and-why-rx-solves-it\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/questions/25098066/what-is-callback-hell-and-how-and-why-rx-solves-it</a><br><a href=\"https://redux.js.org/advanced/middleware\" target=\"_blank\" rel=\"noopener\">https://redux.js.org/advanced/middleware</a><br><a href=\"https://pub.dartlang.org/packages/redux_thunk\" target=\"_blank\" rel=\"noopener\">https://pub.dartlang.org/packages/redux_thunk</a><br><a href=\"https://codeburst.io/how-i-test-redux-saga-fcc425cda018\" target=\"_blank\" rel=\"noopener\">https://codeburst.io/how-i-test-redux-saga-fcc425cda018</a><br><a href=\"https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1\" target=\"_blank\" rel=\"noopener\">https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1</a><br><a href=\"https://www.sitepoint.com/redux-without-react-state-management-vanilla-javascript/\" target=\"_blank\" rel=\"noopener\">https://www.sitepoint.com/redux-without-react-state-management-vanilla-javascript/</a><br><a href=\"https://redux.js.org/introduction/getting-started\" target=\"_blank\" rel=\"noopener\">https://redux.js.org/introduction/getting-started</a><br><a href=\"https://blog.logrocket.com/understanding-redux-saga-from-action-creators-to-sagas-2587298b5e71\" target=\"_blank\" rel=\"noopener\">https://blog.logrocket.com/understanding-redux-saga-from-action-creators-to-sagas-2587298b5e71</a></p>\n","site":{"data":{}},"excerpt":"<p>If you are quite experienced with redux, which is a predictable state container for JavaScript applications (<strong>Note:</strong> even thouth React and Redux is a popular combination to build fast and powerful apps, Redux is not necessarily combined with React), you are definitely feeling comfortable with its powerful store which manages all the global states and provides much cleaner logic flows to change them. If you are new to redux, <a href=\"https://redux.js.org/introduction/getting-started\" target=\"_blank\" rel=\"noopener\">here</a> is the guide to dive before we start our topic.</p>","more":"<p></p>\n<p>In a complex javascript application, asynchronous function is always one of the most annoying part where encounters tons of bugs. If not handle them properly, the app usually ends up with <strong><em>call back hell</em></strong>.<br><br></p>\n<h2 id=\"Haven’t-heard-of-CallBack-Hell\"><a href=\"#Haven’t-heard-of-CallBack-Hell\" class=\"headerlink\" title=\"Haven’t heard of CallBack Hell?\"></a><strong>Haven’t heard of <em>CallBack Hell</em>?</strong></h2><p>Well, in javascript, the only way you can suspend a computation and have the rest operations doing later is to put the rest operations into a callback function. This callback function usually returns a <strong><code>Promise</code></strong> (And has a type of <strong><code>Promise&lt;any&gt;</code></strong>). In order to easily mark those async functions, after <strong><em>ES6</em></strong> javascript provides extra modifiers <strong><code>async</code></strong> and <strong><code>await</code></strong>, which actually wraps up the original utilities of promise and makes it more readable to programmers. Hummm, sounds like things are going better… <del><strong>NO!! It doesn’t resolve anything!</strong></del> The core problem leads to a callback hell is the hierarchical async calls, for example</p>\n<p>you have some simple synchronous functions which are in a chain to accomplish some logics:</p>\n<pre><code class=\"javascript\">a = getSomething();\nb = getMore(a);\nc = getMoreAndMore(b);\n...\n</code></pre>\n<p>It looks fine for now, but what if they all turn out to be async? Then you have to follow the callback style to make them operate one right after another is done:</p>\n<pre><code class=\"javascript\">getSomthing(function(a) {\n    getMore(a, function(b) {\n        getMoreAndMore(b, function(c) {\n            //keep going...\n        });\n    });\n});\n</code></pre>\n<p>Or you prefer <strong><em>ES6</em></strong>:</p>\n<pre><code class=\"javascript\">async function getSomething(a) {\n    await b = ToDo(a);\n    return await getMore((b) =&gt; {\n        return await ToDo(b);\n    }).then((c) =&gt; {\n        return await ToDo(c);\n    }).then(...);\n}\n</code></pre>\n<p>Looks really confused? This will getting even uglier if we are using callbacks in loops.<br><br></p>\n<h2 id=\"Redux-Thunks\"><a href=\"#Redux-Thunks\" class=\"headerlink\" title=\"Redux Thunks\"></a>Redux Thunks</h2><p>Back to our redux app, we usually want to update some states after an async call to inform the UI that the data is ready to be fetched. That is always achieved by dispatching an action from the component to the reducer:</p>\n<pre><code class=\"javascript\">async const callAPI = () =&gt; {\n    ...\n    return response;\n};\n...\nasync const updateUI = (...params) =&gt; {\n    const res = await callAPI();\n    if (res.status === 200)\n        dispatch({type: &quot;UPDATE&quot;, isSuccess: true});\n};\n...\nrender({\n    ...\n    this.props.isSuccess?\n        showData() : showError()\n});\n</code></pre>\n<p>This isn’t bad, but we are always looking for something better. An advanced way to rewrite it is using redux middleware. <strong><em>Middleware</em></strong> is somewhere you can put the code after the framework receives a request and before it generates a response. For example, we want to add a logger in the redux store so that when the store catches an action, before it returns the new state, the logger can log the previous state and the new generated state. This is what can be added as a middleware:</p>\n<pre><code class=\"javascript\">function logger(store) {\n    return function wrapDispatch (next) {\n        return function dispatchAndLog (action) {\n            console.log(&quot;dispatching.. &quot;, action);\n            let result = next(action);\n            console.log(&quot;new state&quot;, store.getState());\n            return result;\n        }\n    }\n}\n</code></pre>\n<p>There are more advanced ways to add a logger. If you are interested, please refer to the <a href=\"https://redux.js.org/advanced/middleware\" target=\"_blank\" rel=\"noopener\">offical documentation</a>. With our middleware, the previous example can be written in a cleaner way:</p>\n<pre><code class=\"javascript\">const callAPI = () =&gt; {\n    return((dispatch) =&gt; {\n        dispatch(startCallingApiAction);\n        actualCallApi().then(data =&gt; {\n            dispatch(successAction(data));\n        }).fail(err =&gt; {\n            dispatch(failedAction(err));\n        });\n    });\n};\n</code></pre>\n<p>The successful response data is wrapped in the payload of the action, sent to the reducer. Once the store updates the data, it will be mapped as a prop back to the component and request for a rerender. This middleware is also called <strong><em>thunk</em></strong>. By applying thunk to decouple the presentation layer, we can get rid of most of the side effects in components, instead, managing and orchestrating side effects in thunks.</p>\n<p>This is great, so why are we even considering <strong><em>saga</em></strong>? Well, one of the advantages of middleware is that it can be chained. Every middleware mounted in redux store starts an individual thread (or something really looks like a thread in <strong><em>NodeJS</em></strong>). When a middleware captures an action and handles its side effect, it can dispatch a new action to another middleware to do nested logics. This behavior of middleware indicates that thunks can be chained as well, for example thunkA forwards its return payload to thunkB and thunkB forwards its return payload to… <strong>Wait! That sounds quite familiar!! Is that the case of callback hell??</strong> Unfortunately, a good thing plus another good feature doesn’t always end up with something better. <del>It could be some shit as well (笑)</del> In this case, true, this is exactly the callback hell.<br><br></p>\n<h2 id=\"Redux-Saga\"><a href=\"#Redux-Saga\" class=\"headerlink\" title=\"Redux Saga\"></a>Redux Saga</h2><p>To handle the possible endless callback functions and also to make it more easily to test in a component which has complicated logics, we need to change our previous thoughts. Just like shifting from Process Oriented Programming to Object Oriented Programming, instead of telling the application how to handle the side effects, suppose it already knows how to call a function and how to dispatch an action, all we need to do is to <strong>give instructions about what to do next</strong> and we don’t care about how those instructions will be executed (Saga handles the executions).</p>\n<p>Then the thunks example can be changed as following:</p>\n<pre><code class=\"javascript\">export function* apiSideEffect(action) {\n    try{\n        const data = yield call(actualCallApi);\n        yield put({ type:&quot;SUCCESS&quot;, payload: data });\n    } catch(err) {\n        yield put({ type:&quot;FAILED&quot;, payload: err });\n    }\n}\n\nexport function* apiSaga() {\n    yield takeEvery(&quot;CLICK_TO_CALL_API&quot;, apiSideEffect);\n}\n</code></pre>\n<p>There are serval fucntions already being integrated in Saga:</p>\n<blockquote>\n<p><strong><code>Call</code></strong>: the method call will return only a plain object describing the operation so redux-saga can take care of the invocation and returns the result to the generator. The first parameter is the generator function ready to be called and the rest params are all the arguments in the generator.</p>\n</blockquote>\n<blockquote>\n<p><strong><code>Put</code></strong>: Instead of dispatching an action inside the generator (Don’t ever ever do that), <strong><em>put</em></strong> Returns an object with instructions for the middleware to dispatch the action.</p>\n</blockquote>\n<blockquote>\n<p><strong><code>Select</code></strong>: Returns value from the selector function, similar with <strong><code>getState()</code></strong>. <strong><em>Note:</em></strong> It is not recommended to use this function because it returns the value corresponding to the contents of the store state tree, which is most likely a plain Javascript object and is <strong>mutable</strong> (Redux wants you to handle state immutably, which means return a new state instead of changing the old one).</p>\n</blockquote>\n<blockquote>\n<p><strong><code>Take</code></strong>: It creates a command object that tells the middleware to wait for a specific action. The resulting behavior of the call Effect is the same as when the middleware suspends the generator until a <strong><em>promise</em></strong> resolves. In the take case, it’ll suspend the generator until a matching action is dispatched</p>\n</blockquote>\n<p>By working with Saga, we make the side effects to be <strong><em>declarative</em></strong> rather than <strong><em>imperative</em></strong>.</p>\n<blockquote>\n<p><strong><em>Declarative:</em></strong> describing what the program must accomplish, rather than describe how to accomplish it</p>\n</blockquote>\n<blockquote>\n<p><strong><em>Imperative:</em></strong> consists of commands for the computer to perform, focuses on describing how a program operates</p>\n</blockquote>\n<p>In the case of take, the control is inverted. Instead of the actions being pushed to the handler tasks, the <strong>Saga is pulling the action by itself</strong>. An additional generator, known as <strong><em>watcher</em></strong> which contains <strong><em>take</em></strong> has to be created to watch a specific action and being triggered once the following action is dispatched in the application. There are two ways to create a watcher, one is using the buid-in functions (<strong><em>Saga Helper</em></strong>):</p>\n<pre><code class=\"javascript\">function* watchFetchData() {\n    yield takeEvery(&quot;FETCH_REQUEST&quot;, callFetchDataApi);\n}\n</code></pre>\n<p><strong><em>takeEvery</em></strong> allows multiple request to be proceeding at the same time. Or if you just want the latest request to be fired (the older one will be overrided during each time the watcher is triggered</p>\n<pre><code class=\"javascript\">function* watchFetchData() {\n    yield takeLatest(&quot;FETCH_REQUEST&quot;, callFetchDataApi);\n}\n</code></pre>\n<p>However by using <strong><em>take</em></strong>, it is possible to fully control an action observation process to build complex control flow:</p>\n<pre><code class=\"javascript\">function* watchFetchData() {\n    while(true) {\n        const action = yield take(&quot;FETCH_REQUEST&quot;);\n        console.log(action);\n        yield call(callFetchDataApi, action.payload);\n    }\n}\n</code></pre>\n<p>All right, now you have been exposed to everything you need to know before start trying redux saga on your own. Here is a short overall example that may also help:<br>Store:</p>\n<pre><code class=\"javascript\">const sagaMiddleware = createSagaMiddleware();\nconst store = createStore(rootReducer, appluMiddleware(sagaMiddleware));    \nsagaMiddleware.run(watchFetch);\n</code></pre>\n<p>Sagas:</p>\n<pre><code class=\"javascript\">function* watchFetch(): Generator&lt;*, *, *&gt; {\n    yield takeEvery(&quot;FETCH_ACTION&quot;, callFetchAPI);\n}\n\nfunction* callFetchAPI(): Generator&lt;*, *, *&gt; {\n    try {\n        yield put({ type: &quot;FETCHING&quot;, payload: ... });\n        const data = yield call(actualCallApi);\n        yield put({ type: &quot;FETCH_SUCCESS&quot;, payload: data });\n    } catch(err) {\n        yield put({ type: &quot;FETCH_FAILED&quot;, payload: err });\n    }\n}\n</code></pre>\n<p>Reducer:</p>\n<pre><code class=\"javascript\">const reducer = (state = initState, action) =&gt; {\n    switch(action) {\n        case &quot;FETCHING&quot;:\n            return { loading: true, ...state };\n        case &quot;FETCH_SUCCESS&quot;:\n            return { loading: false, success: true, data: action.payload, ...state };   \n        case &quot;FETCH_FAILED&quot;:\n            return { loading: false, success: false, error: true, ...state };\n        default:\n            return { ...state };\n    }\n};\n</code></pre>\n<p>Component:</p>\n<pre><code class=\"javascript\">class myComponent extends React.Component {\n    const mapStateToProps = ...\n    const mapDispatchToProps = ...\n    render() {\n        return (\n            &lt;button onClick = { () =&gt; this.props.dispatch({type: &quot;FETCH_ACTION&quot;}) }/&gt;   \n            {\n                this.props.loading?\n                    &lt;p&gt;Loading..&lt;/p&gt; : this.props.error?\n                        &lt;p&gt;Error!&lt;/p&gt; : &lt;p&gt;{this.props.data}&lt;/p&gt;\n            }\n        );\n    }\n}\nexport default connect(mapStateToProps, mapDispatchToProps)(myComponent);\n\n</code></pre>\n<p>For more advanced concepts, there is a well-organized <a href=\"https://redux-saga.js.org/docs/advanced/\" target=\"_blank\" rel=\"noopener\">Saga offical documentation</a> you can refer to if you want to dive deeper.<br><br></p>\n<h2 id=\"How-to-test-Saga\"><a href=\"#How-to-test-Saga\" class=\"headerlink\" title=\"How to test Saga?\"></a>How to test Saga?</h2><p>A function that returns a simple object is easier to test than a function that directly makes an asynchronous call. For redux saga, each time you yield a function call will return a plain javascript object which makes the workflow much easier to test. You don’t need to use the real API, fake it, or mock it, instead just iterating over the generator function, asserting for equality on the values yielded.</p>\n<pre><code class=\"javascript\">describe(&quot;fetch work flow&quot;, () =&gt; {\n    const generator = cloneableGenerator(callFetchAPI)({ type: &quot;FETCH_ACTION&quot; });\n    expect(generator.next().value).toEqual(put({ type: &quot;FETCHING&quot;, payload: ... }));    \n\n    test(&quot;fetch success&quot;, () =&gt; {\n        const clone = generator.clone();\n        expect(clone.next().value).toEqual(put({ type: &quot;FETCH_SUCCESS&quot; }));\n        expect(generator.next().done).toEqual(true);\n    });\n});\n</code></pre>\n<p>In the above example, we use <strong><code>clone()</code></strong> to test different control flows and <strong><code>next()</code></strong> to iterate to the next function ready be yielded. The mock return value can also be injected as an argument of <strong><code>next()</code></strong>:</p>\n<pre><code class=\"javascript\">expect(clone.next(false).value).toEqual( put(fetchFailedAction()) );      \n</code></pre>\n<p><br></p>\n<h2 id=\"Saga-vs-Observables\"><a href=\"#Saga-vs-Observables\" class=\"headerlink\" title=\"Saga vs Observables\"></a>Saga vs Observables</h2><p>Redux saga is not the only solution to our apps which may have complex control flows, they are other helpful tools providing different trade-offs which can also resolve the async problems. Here are some good <a href=\"https://hackmd.io/s/H1xLHUQ8e\" target=\"_blank\" rel=\"noopener\">code snippets</a> of saga vs observables that can open your mind :D</p>\n<p><br><br><br></p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References:\"></a>References:</h2><p><a href=\"https://redux-saga.js.org/\" target=\"_blank\" rel=\"noopener\">https://redux-saga.js.org/</a><br><a href=\"https://stackoverflow.com/questions/25098066/what-is-callback-hell-and-how-and-why-rx-solves-it\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/questions/25098066/what-is-callback-hell-and-how-and-why-rx-solves-it</a><br><a href=\"https://redux.js.org/advanced/middleware\" target=\"_blank\" rel=\"noopener\">https://redux.js.org/advanced/middleware</a><br><a href=\"https://pub.dartlang.org/packages/redux_thunk\" target=\"_blank\" rel=\"noopener\">https://pub.dartlang.org/packages/redux_thunk</a><br><a href=\"https://codeburst.io/how-i-test-redux-saga-fcc425cda018\" target=\"_blank\" rel=\"noopener\">https://codeburst.io/how-i-test-redux-saga-fcc425cda018</a><br><a href=\"https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1\" target=\"_blank\" rel=\"noopener\">https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1</a><br><a href=\"https://www.sitepoint.com/redux-without-react-state-management-vanilla-javascript/\" target=\"_blank\" rel=\"noopener\">https://www.sitepoint.com/redux-without-react-state-management-vanilla-javascript/</a><br><a href=\"https://redux.js.org/introduction/getting-started\" target=\"_blank\" rel=\"noopener\">https://redux.js.org/introduction/getting-started</a><br><a href=\"https://blog.logrocket.com/understanding-redux-saga-from-action-creators-to-sagas-2587298b5e71\" target=\"_blank\" rel=\"noopener\">https://blog.logrocket.com/understanding-redux-saga-from-action-creators-to-sagas-2587298b5e71</a></p>"},{"title":"Implement Zero Data Loss in Spark Streaming","date":"2019-05-22T09:19:14.000Z","photos":["../images/spark_kafka.JPG"],"_content":"This is a log from my own experience in spark streaming during my work. Base on different environment and servers, the strategy may vary. For here, I am working with **Kafka** and Google Cloud **PubSub**. <!-- more -->\n\n## Background\nI have such workflow that the spark streaming receives dataset from both Kafka and PubSub, after doing some clean up and modeling, push the data to the Cloud Datastore.\n![my Workflow](sparkworkflow.JPG)\nIt works fine when the data stream is small and reports me the expected values; however while the number of end users is growing large, especially when the data stream turns to be erratic and sometimes considerably large if end users interact frequently with our UI pages, the spark streaming will get a lot of uncertain runtime errors. Those errors are most likely caused by the finite number of workers in our cluster. Also when there are too much stream rushing over to the server, the limited memory in cache will cause failures. After we upgraded the cluster on cloud, the number of errors is significantly reduced.\n![CPU Utilization](cpudiagram.JPG)\nBut the data processed during the errors was permanently lost and cannot be recovered. The data running on spark is buffered in memory (cache) and will be cleared meanwhile a failure occurs. This is not desired since some valuable KPIs may be lost as well. To prevent such data loss, I tried different strategies.\n</br>\n\n## Checkpoint\nCheckpointing is a process supported by spark streaming after version which will save RDDs in log after being checkpointed. There are two level of checkpoints: reliable and local. **Reliable checkpoint** ensures that the data is stored permanentlly on HDFS, S3 or other distributed filesystems. Each job on cluster will create a directory for saving checkpointed logs, which will look pretty much like the directory below:\n```\n├── \"SomeUUID\"\n│   └── rdd-#\n│       ├── part-timestamp1\n│       ├── .part-timestamp1.crc\n│       ├── part-timestamp2\n│       └── .part-timestamp2.crc\n```\nSince the data stream will be replicated on disk, the performance will slow down due to file I/O. **Local checkpoint** privileges performance over fault-tolerance which will persist RDDs on local storage in the executors. Read or write will be faster in this case; however if a driver fails, the data not yet executed may not be recoverable. As default, the data storeage level is set to `MEMORY_AND_DISK` which saves data in cache and disk (some in cache and some in disk). For here I changed to `MEMORY_AND_DISK_SER_2` (more details can be referred to [here](https://stackoverflow.com/questions/30520428/what-is-the-difference-between-memory-only-and-memory-and-disk-caching-level-in)). The different is that, unlike **cache** only, the checkpoints doesn't save DAG with all the parents of RDDs; instead, they only save particular RDDs and remain for a longer time than cache. The time of persistance is strictly related to the executed computation and ends with the end of the spark application. To apply the checkpoint machanism, you just simply need to set the checkpoint directory when you are creating the **StreamingContext**\n```scala\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(\"KpiAnalysis\")   \n     val ssc = new StreamingContext(sparkConf, Seconds(1))\n     ...\n     ssc.checkpoint(CHECK_POINT_DIR)\n     ssc\n}\n```\nand before an action is operated on RDD:\n```scala\n    sRDD.checkpoint()\n    sRDD.foreachPartition { partitionOfRecords => {     \n        ...\n        }\n    }\n```\nand `sRDD.isCheckpointed()` will return **true**. For cleaning, the RDDs stored in cache will be cleaned with all other memory after the whole spark application is finished or terminated; the reliable RDDs stored on disk can be cleaned manually or set \n`spark.cleaner.referenceTracking.cleanCheckpoints` property to be **true** to enable automatic cleaning. This driver recovery mechanism is sufficient to ensure zero data loss if all data was reliably store in the system. However for my circumstance, the data is read from **kafka** and some of the data buffered in memory could be lost. If the driver process fails, all the executors running will be killed as well, along with any data in their memory. This pushes me to look for other mechanisms which are more advanced.\n</br>\n\n## Write Ahead Logs\nWrite Ahead Logs are used in database and file systems to ensure the durability of any data operations. The intention of the operation is first written down into a durable log , and then the operation is applied to the data. If the system fails in the middle of applying the operation, it can recover by reading the log and reapplying the operations it had intended to do.\n\nSpark streaming uses **Receiver** to read data from **Kafka**. They run as long-running tasks in the executors and store the revecived data in the memory of the executors. If you enable the **checkpoint**, the data will be checkpointed either in cache or disk **in executors** before porceed into the application drivers. Unlike **checkpoint**, applying **WAL** will instead backup the recevied data in an **external fault-tolerant filesystem**. And after the executor batches the received data and sends to the driver, **WAL** supports another log to store the block metadata into external filesystem before being executed.\n![https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html](wal_spark.JPG)\n(diagram from https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html)\nSpark streaming starts supporting WAL after version 1.2 and can be enabled by setting the config:\n\n```scala\nval ssc = StreamingContext.createContext(...params)\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(\"KpiAnalysis\")\n                                   .set(\"spark.streaming.receiver.writeAheadLog.enable\",\"true\")    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n```\nSet the spark automatically clean checkpoints to release disk memory:\n```scala\nval ssc = StreamingContext.createContext(...params)\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(\"KpiAnalysis\")\n                                   .set(\"spark.streaming.receiver.writeAheadLog.enable\",\"true\")\n                                   .set(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\")    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n\n```\nThis will not clean the latest checkpoint as it is still referred to by the application to recover from possible failures. If you expect to restart the application driver if it crashed due to some errors and exactly start from where it crashed last time instead of performing the whole operation once again, you can create **StreamingContext** from the previous checkpoint by using the method `getOrCreate()`:\n```scala\nval ssc = StreamingContext.getOrCreate(CHECK_POINT_DIR, () => createContext(...params))\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(\"KpiAnalysis\")\n                                   .set(\"spark.streaming.receiver.writeAheadLog.enable\",\"true\")\n                                   .set(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\")    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n```\nHowever after I tried switching to `getOrCreate()`, I had the following exception:\n```\nException in thread \"main\" org.apache.spark.SparkException: Failed to read checkpoint from directory checkpointDir\n    at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:368)\n\tat org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)\n    ...\nCaused by: java.io.IOException: java.lang.ClassCastException: cannot assign instance of com.some.project$$anonfun$1 to field org.apache.spark.streaming.dstream.MappedDStream.org$apache$spark$streaming$dstream$MappedDStream$$mapFunc of type scala.Function1 in instance of org.apache.spark.streaming.dstream.MappedDStream     \n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)\n\tat org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    ...\n```\nIt is because this is not the first time I use checkpoints, and there already exsists some other checkpoints in the same root directory (etc, the simple checkpoint discussed previously). Since the application tried to read from those unrelated checkpoints and expected to cast them to generate new context, it would throw such **ClassCastException**. This can be easily solved by deleting original checkpoints in HDFS locals (manually).\n\nWAL is an advanced checkpoint mechanism and also simple to be applied. Compared with checkpoint, it saves data into an external filesystem so that even though if the executor is terminated and the data in memory is clean-uped, data still can be recovered from the external filesystem. However. it can only recover the data which is logged in the filesystem, if the drivers fail due to some error, the executor will be terminated as well, so as the WAL writer. Then the rest incomming data will not be logged into the filesystem and hence, is not recoverable. It can be tested by calling **stop** to the **StreamContext**:\n```scala\ndef stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit\n```\nOnce it is called, the following console log interpretes that the **WAL Writer** is interrupted as well:\n```\nERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\nWARN BlockGenerator: Cannot stop BlockGenerator as its not in the Active state [state = StoppedAll]     \nWARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n```\nAnd also the data will not be recoverable across applications or Spark upgrades and hence not very reliable\n</br>\n\n## Kafka Direct API\nThis mechanism is only available when you data source is **Kafka**. Kafka supports a commit strategy which is able to help you manage offsets of each topics. Each offset points to a slot in a topic. When the data stored in this slot is consumed by any receiver, Kafka will be acknowledged by this consumption and moves the index to the next data slot.\n![https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/](Spark-Streaming-flow-for-offsets.JPG)\n(diagram from https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/)\nWhen `enable.auto.commit` is set to be true, as soon as any receiver retrieves the data from the offset datastore in Kafka (here I use Kafka to store offsets), the receiver will automatically commit, which doesn't ensure that the data is successfully executed in the spark streaming. Therefore, we have to disbale the auto-commit when we are creating DStream from Kafka. After the data is successfully processed, we manually commit the offset to the datastore by calling the Kafka direct API:\n```scala\nstream.foreachRDD { rdd =>\n      //get current offset\n      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n\n      //process data\n\n      //store data in some datastore\n      DataStoreDB.push(...param)\n\n      //commit offset to kafka\n      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)\n    }\n  }\n```\nIf an error occurs at any point during the execution, the offset won;t be committed to the kafka offset store and hence the same data will be resent by kafka, which ensures that the data will be executed only once.\n","source":"_posts/Implement-Zero-Data-Loss-in-Spark-Streaming.md","raw":"---\ntitle: Implement Zero Data Loss in Spark Streaming\ndate: 2019-05-22 18:19:14\ntags: [Spark, Kafka]\nphotos: [\"../images/spark_kafka.JPG\"]\n---\nThis is a log from my own experience in spark streaming during my work. Base on different environment and servers, the strategy may vary. For here, I am working with **Kafka** and Google Cloud **PubSub**. <!-- more -->\n\n## Background\nI have such workflow that the spark streaming receives dataset from both Kafka and PubSub, after doing some clean up and modeling, push the data to the Cloud Datastore.\n![my Workflow](sparkworkflow.JPG)\nIt works fine when the data stream is small and reports me the expected values; however while the number of end users is growing large, especially when the data stream turns to be erratic and sometimes considerably large if end users interact frequently with our UI pages, the spark streaming will get a lot of uncertain runtime errors. Those errors are most likely caused by the finite number of workers in our cluster. Also when there are too much stream rushing over to the server, the limited memory in cache will cause failures. After we upgraded the cluster on cloud, the number of errors is significantly reduced.\n![CPU Utilization](cpudiagram.JPG)\nBut the data processed during the errors was permanently lost and cannot be recovered. The data running on spark is buffered in memory (cache) and will be cleared meanwhile a failure occurs. This is not desired since some valuable KPIs may be lost as well. To prevent such data loss, I tried different strategies.\n</br>\n\n## Checkpoint\nCheckpointing is a process supported by spark streaming after version which will save RDDs in log after being checkpointed. There are two level of checkpoints: reliable and local. **Reliable checkpoint** ensures that the data is stored permanentlly on HDFS, S3 or other distributed filesystems. Each job on cluster will create a directory for saving checkpointed logs, which will look pretty much like the directory below:\n```\n├── \"SomeUUID\"\n│   └── rdd-#\n│       ├── part-timestamp1\n│       ├── .part-timestamp1.crc\n│       ├── part-timestamp2\n│       └── .part-timestamp2.crc\n```\nSince the data stream will be replicated on disk, the performance will slow down due to file I/O. **Local checkpoint** privileges performance over fault-tolerance which will persist RDDs on local storage in the executors. Read or write will be faster in this case; however if a driver fails, the data not yet executed may not be recoverable. As default, the data storeage level is set to `MEMORY_AND_DISK` which saves data in cache and disk (some in cache and some in disk). For here I changed to `MEMORY_AND_DISK_SER_2` (more details can be referred to [here](https://stackoverflow.com/questions/30520428/what-is-the-difference-between-memory-only-and-memory-and-disk-caching-level-in)). The different is that, unlike **cache** only, the checkpoints doesn't save DAG with all the parents of RDDs; instead, they only save particular RDDs and remain for a longer time than cache. The time of persistance is strictly related to the executed computation and ends with the end of the spark application. To apply the checkpoint machanism, you just simply need to set the checkpoint directory when you are creating the **StreamingContext**\n```scala\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(\"KpiAnalysis\")   \n     val ssc = new StreamingContext(sparkConf, Seconds(1))\n     ...\n     ssc.checkpoint(CHECK_POINT_DIR)\n     ssc\n}\n```\nand before an action is operated on RDD:\n```scala\n    sRDD.checkpoint()\n    sRDD.foreachPartition { partitionOfRecords => {     \n        ...\n        }\n    }\n```\nand `sRDD.isCheckpointed()` will return **true**. For cleaning, the RDDs stored in cache will be cleaned with all other memory after the whole spark application is finished or terminated; the reliable RDDs stored on disk can be cleaned manually or set \n`spark.cleaner.referenceTracking.cleanCheckpoints` property to be **true** to enable automatic cleaning. This driver recovery mechanism is sufficient to ensure zero data loss if all data was reliably store in the system. However for my circumstance, the data is read from **kafka** and some of the data buffered in memory could be lost. If the driver process fails, all the executors running will be killed as well, along with any data in their memory. This pushes me to look for other mechanisms which are more advanced.\n</br>\n\n## Write Ahead Logs\nWrite Ahead Logs are used in database and file systems to ensure the durability of any data operations. The intention of the operation is first written down into a durable log , and then the operation is applied to the data. If the system fails in the middle of applying the operation, it can recover by reading the log and reapplying the operations it had intended to do.\n\nSpark streaming uses **Receiver** to read data from **Kafka**. They run as long-running tasks in the executors and store the revecived data in the memory of the executors. If you enable the **checkpoint**, the data will be checkpointed either in cache or disk **in executors** before porceed into the application drivers. Unlike **checkpoint**, applying **WAL** will instead backup the recevied data in an **external fault-tolerant filesystem**. And after the executor batches the received data and sends to the driver, **WAL** supports another log to store the block metadata into external filesystem before being executed.\n![https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html](wal_spark.JPG)\n(diagram from https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html)\nSpark streaming starts supporting WAL after version 1.2 and can be enabled by setting the config:\n\n```scala\nval ssc = StreamingContext.createContext(...params)\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(\"KpiAnalysis\")\n                                   .set(\"spark.streaming.receiver.writeAheadLog.enable\",\"true\")    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n```\nSet the spark automatically clean checkpoints to release disk memory:\n```scala\nval ssc = StreamingContext.createContext(...params)\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(\"KpiAnalysis\")\n                                   .set(\"spark.streaming.receiver.writeAheadLog.enable\",\"true\")\n                                   .set(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\")    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n\n```\nThis will not clean the latest checkpoint as it is still referred to by the application to recover from possible failures. If you expect to restart the application driver if it crashed due to some errors and exactly start from where it crashed last time instead of performing the whole operation once again, you can create **StreamingContext** from the previous checkpoint by using the method `getOrCreate()`:\n```scala\nval ssc = StreamingContext.getOrCreate(CHECK_POINT_DIR, () => createContext(...params))\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(\"KpiAnalysis\")\n                                   .set(\"spark.streaming.receiver.writeAheadLog.enable\",\"true\")\n                                   .set(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\")    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n```\nHowever after I tried switching to `getOrCreate()`, I had the following exception:\n```\nException in thread \"main\" org.apache.spark.SparkException: Failed to read checkpoint from directory checkpointDir\n    at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:368)\n\tat org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)\n    ...\nCaused by: java.io.IOException: java.lang.ClassCastException: cannot assign instance of com.some.project$$anonfun$1 to field org.apache.spark.streaming.dstream.MappedDStream.org$apache$spark$streaming$dstream$MappedDStream$$mapFunc of type scala.Function1 in instance of org.apache.spark.streaming.dstream.MappedDStream     \n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)\n\tat org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    ...\n```\nIt is because this is not the first time I use checkpoints, and there already exsists some other checkpoints in the same root directory (etc, the simple checkpoint discussed previously). Since the application tried to read from those unrelated checkpoints and expected to cast them to generate new context, it would throw such **ClassCastException**. This can be easily solved by deleting original checkpoints in HDFS locals (manually).\n\nWAL is an advanced checkpoint mechanism and also simple to be applied. Compared with checkpoint, it saves data into an external filesystem so that even though if the executor is terminated and the data in memory is clean-uped, data still can be recovered from the external filesystem. However. it can only recover the data which is logged in the filesystem, if the drivers fail due to some error, the executor will be terminated as well, so as the WAL writer. Then the rest incomming data will not be logged into the filesystem and hence, is not recoverable. It can be tested by calling **stop** to the **StreamContext**:\n```scala\ndef stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit\n```\nOnce it is called, the following console log interpretes that the **WAL Writer** is interrupted as well:\n```\nERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\nWARN BlockGenerator: Cannot stop BlockGenerator as its not in the Active state [state = StoppedAll]     \nWARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n```\nAnd also the data will not be recoverable across applications or Spark upgrades and hence not very reliable\n</br>\n\n## Kafka Direct API\nThis mechanism is only available when you data source is **Kafka**. Kafka supports a commit strategy which is able to help you manage offsets of each topics. Each offset points to a slot in a topic. When the data stored in this slot is consumed by any receiver, Kafka will be acknowledged by this consumption and moves the index to the next data slot.\n![https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/](Spark-Streaming-flow-for-offsets.JPG)\n(diagram from https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/)\nWhen `enable.auto.commit` is set to be true, as soon as any receiver retrieves the data from the offset datastore in Kafka (here I use Kafka to store offsets), the receiver will automatically commit, which doesn't ensure that the data is successfully executed in the spark streaming. Therefore, we have to disbale the auto-commit when we are creating DStream from Kafka. After the data is successfully processed, we manually commit the offset to the datastore by calling the Kafka direct API:\n```scala\nstream.foreachRDD { rdd =>\n      //get current offset\n      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n\n      //process data\n\n      //store data in some datastore\n      DataStoreDB.push(...param)\n\n      //commit offset to kafka\n      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)\n    }\n  }\n```\nIf an error occurs at any point during the execution, the offset won;t be committed to the kafka offset store and hence the same data will be resent by kafka, which ensures that the data will be executed only once.\n","slug":"Implement-Zero-Data-Loss-in-Spark-Streaming","published":1,"updated":"2019-07-18T14:13:16.399Z","comments":1,"layout":"post","link":"","_id":"cjyedivvs0019t8ao1t2ylzbe","content":"<p>This is a log from my own experience in spark streaming during my work. Base on different environment and servers, the strategy may vary. For here, I am working with <strong>Kafka</strong> and Google Cloud <strong>PubSub</strong>. <a id=\"more\"></a></p>\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><p>I have such workflow that the spark streaming receives dataset from both Kafka and PubSub, after doing some clean up and modeling, push the data to the Cloud Datastore.<br><img src=\"/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/sparkworkflow.JPG\" alt=\"my Workflow\"><br>It works fine when the data stream is small and reports me the expected values; however while the number of end users is growing large, especially when the data stream turns to be erratic and sometimes considerably large if end users interact frequently with our UI pages, the spark streaming will get a lot of uncertain runtime errors. Those errors are most likely caused by the finite number of workers in our cluster. Also when there are too much stream rushing over to the server, the limited memory in cache will cause failures. After we upgraded the cluster on cloud, the number of errors is significantly reduced.<br><img src=\"/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/cpudiagram.JPG\" alt=\"CPU Utilization\"><br>But the data processed during the errors was permanently lost and cannot be recovered. The data running on spark is buffered in memory (cache) and will be cleared meanwhile a failure occurs. This is not desired since some valuable KPIs may be lost as well. To prevent such data loss, I tried different strategies.<br><br></p>\n<h2 id=\"Checkpoint\"><a href=\"#Checkpoint\" class=\"headerlink\" title=\"Checkpoint\"></a>Checkpoint</h2><p>Checkpointing is a process supported by spark streaming after version which will save RDDs in log after being checkpointed. There are two level of checkpoints: reliable and local. <strong>Reliable checkpoint</strong> ensures that the data is stored permanentlly on HDFS, S3 or other distributed filesystems. Each job on cluster will create a directory for saving checkpointed logs, which will look pretty much like the directory below:</p>\n<pre><code>├── &quot;SomeUUID&quot;\n│   └── rdd-#\n│       ├── part-timestamp1\n│       ├── .part-timestamp1.crc\n│       ├── part-timestamp2\n│       └── .part-timestamp2.crc\n</code></pre><p>Since the data stream will be replicated on disk, the performance will slow down due to file I/O. <strong>Local checkpoint</strong> privileges performance over fault-tolerance which will persist RDDs on local storage in the executors. Read or write will be faster in this case; however if a driver fails, the data not yet executed may not be recoverable. As default, the data storeage level is set to <code>MEMORY_AND_DISK</code> which saves data in cache and disk (some in cache and some in disk). For here I changed to <code>MEMORY_AND_DISK_SER_2</code> (more details can be referred to <a href=\"https://stackoverflow.com/questions/30520428/what-is-the-difference-between-memory-only-and-memory-and-disk-caching-level-in\" target=\"_blank\" rel=\"noopener\">here</a>). The different is that, unlike <strong>cache</strong> only, the checkpoints doesn’t save DAG with all the parents of RDDs; instead, they only save particular RDDs and remain for a longer time than cache. The time of persistance is strictly related to the executed computation and ends with the end of the spark application. To apply the checkpoint machanism, you just simply need to set the checkpoint directory when you are creating the <strong>StreamingContext</strong></p>\n<pre><code class=\"scala\">def createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;)   \n     val ssc = new StreamingContext(sparkConf, Seconds(1))\n     ...\n     ssc.checkpoint(CHECK_POINT_DIR)\n     ssc\n}\n</code></pre>\n<p>and before an action is operated on RDD:</p>\n<pre><code class=\"scala\">    sRDD.checkpoint()\n    sRDD.foreachPartition { partitionOfRecords =&gt; {     \n        ...\n        }\n    }\n</code></pre>\n<p>and <code>sRDD.isCheckpointed()</code> will return <strong>true</strong>. For cleaning, the RDDs stored in cache will be cleaned with all other memory after the whole spark application is finished or terminated; the reliable RDDs stored on disk can be cleaned manually or set<br><code>spark.cleaner.referenceTracking.cleanCheckpoints</code> property to be <strong>true</strong> to enable automatic cleaning. This driver recovery mechanism is sufficient to ensure zero data loss if all data was reliably store in the system. However for my circumstance, the data is read from <strong>kafka</strong> and some of the data buffered in memory could be lost. If the driver process fails, all the executors running will be killed as well, along with any data in their memory. This pushes me to look for other mechanisms which are more advanced.<br><br></p>\n<h2 id=\"Write-Ahead-Logs\"><a href=\"#Write-Ahead-Logs\" class=\"headerlink\" title=\"Write Ahead Logs\"></a>Write Ahead Logs</h2><p>Write Ahead Logs are used in database and file systems to ensure the durability of any data operations. The intention of the operation is first written down into a durable log , and then the operation is applied to the data. If the system fails in the middle of applying the operation, it can recover by reading the log and reapplying the operations it had intended to do.</p>\n<p>Spark streaming uses <strong>Receiver</strong> to read data from <strong>Kafka</strong>. They run as long-running tasks in the executors and store the revecived data in the memory of the executors. If you enable the <strong>checkpoint</strong>, the data will be checkpointed either in cache or disk <strong>in executors</strong> before porceed into the application drivers. Unlike <strong>checkpoint</strong>, applying <strong>WAL</strong> will instead backup the recevied data in an <strong>external fault-tolerant filesystem</strong>. And after the executor batches the received data and sends to the driver, <strong>WAL</strong> supports another log to store the block metadata into external filesystem before being executed.<br><img src=\"/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/wal_spark.JPG\" alt=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\"><br>(diagram from <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\" target=\"_blank\" rel=\"noopener\">https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html</a>)<br>Spark streaming starts supporting WAL after version 1.2 and can be enabled by setting the config:</p>\n<pre><code class=\"scala\">val ssc = StreamingContext.createContext(...params)\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;)\n                                   .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;)    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n</code></pre>\n<p>Set the spark automatically clean checkpoints to release disk memory:</p>\n<pre><code class=\"scala\">val ssc = StreamingContext.createContext(...params)\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;)\n                                   .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;)\n                                   .set(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, &quot;true&quot;)    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n\n</code></pre>\n<p>This will not clean the latest checkpoint as it is still referred to by the application to recover from possible failures. If you expect to restart the application driver if it crashed due to some errors and exactly start from where it crashed last time instead of performing the whole operation once again, you can create <strong>StreamingContext</strong> from the previous checkpoint by using the method <code>getOrCreate()</code>:</p>\n<pre><code class=\"scala\">val ssc = StreamingContext.getOrCreate(CHECK_POINT_DIR, () =&gt; createContext(...params))\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;)\n                                   .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;)\n                                   .set(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, &quot;true&quot;)    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n</code></pre>\n<p>However after I tried switching to <code>getOrCreate()</code>, I had the following exception:</p>\n<pre><code>Exception in thread &quot;main&quot; org.apache.spark.SparkException: Failed to read checkpoint from directory checkpointDir\n    at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:368)\n    at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)\n    ...\nCaused by: java.io.IOException: java.lang.ClassCastException: cannot assign instance of com.some.project$$anonfun$1 to field org.apache.spark.streaming.dstream.MappedDStream.org$apache$spark$streaming$dstream$MappedDStream$$mapFunc of type scala.Function1 in instance of org.apache.spark.streaming.dstream.MappedDStream     \n    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)\n    at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    ...\n</code></pre><p>It is because this is not the first time I use checkpoints, and there already exsists some other checkpoints in the same root directory (etc, the simple checkpoint discussed previously). Since the application tried to read from those unrelated checkpoints and expected to cast them to generate new context, it would throw such <strong>ClassCastException</strong>. This can be easily solved by deleting original checkpoints in HDFS locals (manually).</p>\n<p>WAL is an advanced checkpoint mechanism and also simple to be applied. Compared with checkpoint, it saves data into an external filesystem so that even though if the executor is terminated and the data in memory is clean-uped, data still can be recovered from the external filesystem. However. it can only recover the data which is logged in the filesystem, if the drivers fail due to some error, the executor will be terminated as well, so as the WAL writer. Then the rest incomming data will not be logged into the filesystem and hence, is not recoverable. It can be tested by calling <strong>stop</strong> to the <strong>StreamContext</strong>:</p>\n<pre><code class=\"scala\">def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit\n</code></pre>\n<p>Once it is called, the following console log interpretes that the <strong>WAL Writer</strong> is interrupted as well:</p>\n<pre><code>ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\nWARN BlockGenerator: Cannot stop BlockGenerator as its not in the Active state [state = StoppedAll]     \nWARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n</code></pre><p>And also the data will not be recoverable across applications or Spark upgrades and hence not very reliable<br><br></p>\n<h2 id=\"Kafka-Direct-API\"><a href=\"#Kafka-Direct-API\" class=\"headerlink\" title=\"Kafka Direct API\"></a>Kafka Direct API</h2><p>This mechanism is only available when you data source is <strong>Kafka</strong>. Kafka supports a commit strategy which is able to help you manage offsets of each topics. Each offset points to a slot in a topic. When the data stored in this slot is consumed by any receiver, Kafka will be acknowledged by this consumption and moves the index to the next data slot.<br><img src=\"/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/Spark-Streaming-flow-for-offsets.JPG\" alt=\"https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/\"><br>(diagram from <a href=\"https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/\" target=\"_blank\" rel=\"noopener\">https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/</a>)<br>When <code>enable.auto.commit</code> is set to be true, as soon as any receiver retrieves the data from the offset datastore in Kafka (here I use Kafka to store offsets), the receiver will automatically commit, which doesn’t ensure that the data is successfully executed in the spark streaming. Therefore, we have to disbale the auto-commit when we are creating DStream from Kafka. After the data is successfully processed, we manually commit the offset to the datastore by calling the Kafka direct API:</p>\n<pre><code class=\"scala\">stream.foreachRDD { rdd =&gt;\n      //get current offset\n      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n\n      //process data\n\n      //store data in some datastore\n      DataStoreDB.push(...param)\n\n      //commit offset to kafka\n      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)\n    }\n  }\n</code></pre>\n<p>If an error occurs at any point during the execution, the offset won;t be committed to the kafka offset store and hence the same data will be resent by kafka, which ensures that the data will be executed only once.</p>\n","site":{"data":{}},"excerpt":"<p>This is a log from my own experience in spark streaming during my work. Base on different environment and servers, the strategy may vary. For here, I am working with <strong>Kafka</strong> and Google Cloud <strong>PubSub</strong>.</p>","more":"<p></p>\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><p>I have such workflow that the spark streaming receives dataset from both Kafka and PubSub, after doing some clean up and modeling, push the data to the Cloud Datastore.<br><img src=\"/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/sparkworkflow.JPG\" alt=\"my Workflow\"><br>It works fine when the data stream is small and reports me the expected values; however while the number of end users is growing large, especially when the data stream turns to be erratic and sometimes considerably large if end users interact frequently with our UI pages, the spark streaming will get a lot of uncertain runtime errors. Those errors are most likely caused by the finite number of workers in our cluster. Also when there are too much stream rushing over to the server, the limited memory in cache will cause failures. After we upgraded the cluster on cloud, the number of errors is significantly reduced.<br><img src=\"/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/cpudiagram.JPG\" alt=\"CPU Utilization\"><br>But the data processed during the errors was permanently lost and cannot be recovered. The data running on spark is buffered in memory (cache) and will be cleared meanwhile a failure occurs. This is not desired since some valuable KPIs may be lost as well. To prevent such data loss, I tried different strategies.<br><br></p>\n<h2 id=\"Checkpoint\"><a href=\"#Checkpoint\" class=\"headerlink\" title=\"Checkpoint\"></a>Checkpoint</h2><p>Checkpointing is a process supported by spark streaming after version which will save RDDs in log after being checkpointed. There are two level of checkpoints: reliable and local. <strong>Reliable checkpoint</strong> ensures that the data is stored permanentlly on HDFS, S3 or other distributed filesystems. Each job on cluster will create a directory for saving checkpointed logs, which will look pretty much like the directory below:</p>\n<pre><code>├── &quot;SomeUUID&quot;\n│   └── rdd-#\n│       ├── part-timestamp1\n│       ├── .part-timestamp1.crc\n│       ├── part-timestamp2\n│       └── .part-timestamp2.crc\n</code></pre><p>Since the data stream will be replicated on disk, the performance will slow down due to file I/O. <strong>Local checkpoint</strong> privileges performance over fault-tolerance which will persist RDDs on local storage in the executors. Read or write will be faster in this case; however if a driver fails, the data not yet executed may not be recoverable. As default, the data storeage level is set to <code>MEMORY_AND_DISK</code> which saves data in cache and disk (some in cache and some in disk). For here I changed to <code>MEMORY_AND_DISK_SER_2</code> (more details can be referred to <a href=\"https://stackoverflow.com/questions/30520428/what-is-the-difference-between-memory-only-and-memory-and-disk-caching-level-in\" target=\"_blank\" rel=\"noopener\">here</a>). The different is that, unlike <strong>cache</strong> only, the checkpoints doesn’t save DAG with all the parents of RDDs; instead, they only save particular RDDs and remain for a longer time than cache. The time of persistance is strictly related to the executed computation and ends with the end of the spark application. To apply the checkpoint machanism, you just simply need to set the checkpoint directory when you are creating the <strong>StreamingContext</strong></p>\n<pre><code class=\"scala\">def createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;)   \n     val ssc = new StreamingContext(sparkConf, Seconds(1))\n     ...\n     ssc.checkpoint(CHECK_POINT_DIR)\n     ssc\n}\n</code></pre>\n<p>and before an action is operated on RDD:</p>\n<pre><code class=\"scala\">    sRDD.checkpoint()\n    sRDD.foreachPartition { partitionOfRecords =&gt; {     \n        ...\n        }\n    }\n</code></pre>\n<p>and <code>sRDD.isCheckpointed()</code> will return <strong>true</strong>. For cleaning, the RDDs stored in cache will be cleaned with all other memory after the whole spark application is finished or terminated; the reliable RDDs stored on disk can be cleaned manually or set<br><code>spark.cleaner.referenceTracking.cleanCheckpoints</code> property to be <strong>true</strong> to enable automatic cleaning. This driver recovery mechanism is sufficient to ensure zero data loss if all data was reliably store in the system. However for my circumstance, the data is read from <strong>kafka</strong> and some of the data buffered in memory could be lost. If the driver process fails, all the executors running will be killed as well, along with any data in their memory. This pushes me to look for other mechanisms which are more advanced.<br><br></p>\n<h2 id=\"Write-Ahead-Logs\"><a href=\"#Write-Ahead-Logs\" class=\"headerlink\" title=\"Write Ahead Logs\"></a>Write Ahead Logs</h2><p>Write Ahead Logs are used in database and file systems to ensure the durability of any data operations. The intention of the operation is first written down into a durable log , and then the operation is applied to the data. If the system fails in the middle of applying the operation, it can recover by reading the log and reapplying the operations it had intended to do.</p>\n<p>Spark streaming uses <strong>Receiver</strong> to read data from <strong>Kafka</strong>. They run as long-running tasks in the executors and store the revecived data in the memory of the executors. If you enable the <strong>checkpoint</strong>, the data will be checkpointed either in cache or disk <strong>in executors</strong> before porceed into the application drivers. Unlike <strong>checkpoint</strong>, applying <strong>WAL</strong> will instead backup the recevied data in an <strong>external fault-tolerant filesystem</strong>. And after the executor batches the received data and sends to the driver, <strong>WAL</strong> supports another log to store the block metadata into external filesystem before being executed.<br><img src=\"/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/wal_spark.JPG\" alt=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\"><br>(diagram from <a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\" target=\"_blank\" rel=\"noopener\">https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html</a>)<br>Spark streaming starts supporting WAL after version 1.2 and can be enabled by setting the config:</p>\n<pre><code class=\"scala\">val ssc = StreamingContext.createContext(...params)\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;)\n                                   .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;)    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n</code></pre>\n<p>Set the spark automatically clean checkpoints to release disk memory:</p>\n<pre><code class=\"scala\">val ssc = StreamingContext.createContext(...params)\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;)\n                                   .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;)\n                                   .set(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, &quot;true&quot;)    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n\n</code></pre>\n<p>This will not clean the latest checkpoint as it is still referred to by the application to recover from possible failures. If you expect to restart the application driver if it crashed due to some errors and exactly start from where it crashed last time instead of performing the whole operation once again, you can create <strong>StreamingContext</strong> from the previous checkpoint by using the method <code>getOrCreate()</code>:</p>\n<pre><code class=\"scala\">val ssc = StreamingContext.getOrCreate(CHECK_POINT_DIR, () =&gt; createContext(...params))\n\ndef createContext(...params): StreamingContext = {\n    val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;)\n                                   .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;)\n                                   .set(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, &quot;true&quot;)    \n    val ssc = new StreamingContext(sparkConf, Seconds(1))\n    ...\n    ssc.checkpoint(CHECK_POINT_DIR)\n    ssc\n}\n</code></pre>\n<p>However after I tried switching to <code>getOrCreate()</code>, I had the following exception:</p>\n<pre><code>Exception in thread &quot;main&quot; org.apache.spark.SparkException: Failed to read checkpoint from directory checkpointDir\n    at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:368)\n    at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)\n    ...\nCaused by: java.io.IOException: java.lang.ClassCastException: cannot assign instance of com.some.project$$anonfun$1 to field org.apache.spark.streaming.dstream.MappedDStream.org$apache$spark$streaming$dstream$MappedDStream$$mapFunc of type scala.Function1 in instance of org.apache.spark.streaming.dstream.MappedDStream     \n    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)\n    at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    ...\n</code></pre><p>It is because this is not the first time I use checkpoints, and there already exsists some other checkpoints in the same root directory (etc, the simple checkpoint discussed previously). Since the application tried to read from those unrelated checkpoints and expected to cast them to generate new context, it would throw such <strong>ClassCastException</strong>. This can be easily solved by deleting original checkpoints in HDFS locals (manually).</p>\n<p>WAL is an advanced checkpoint mechanism and also simple to be applied. Compared with checkpoint, it saves data into an external filesystem so that even though if the executor is terminated and the data in memory is clean-uped, data still can be recovered from the external filesystem. However. it can only recover the data which is logged in the filesystem, if the drivers fail due to some error, the executor will be terminated as well, so as the WAL writer. Then the rest incomming data will not be logged into the filesystem and hence, is not recoverable. It can be tested by calling <strong>stop</strong> to the <strong>StreamContext</strong>:</p>\n<pre><code class=\"scala\">def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit\n</code></pre>\n<p>Once it is called, the following console log interpretes that the <strong>WAL Writer</strong> is interrupted as well:</p>\n<pre><code>ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\nWARN BlockGenerator: Cannot stop BlockGenerator as its not in the Active state [state = StoppedAll]     \nWARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n</code></pre><p>And also the data will not be recoverable across applications or Spark upgrades and hence not very reliable<br><br></p>\n<h2 id=\"Kafka-Direct-API\"><a href=\"#Kafka-Direct-API\" class=\"headerlink\" title=\"Kafka Direct API\"></a>Kafka Direct API</h2><p>This mechanism is only available when you data source is <strong>Kafka</strong>. Kafka supports a commit strategy which is able to help you manage offsets of each topics. Each offset points to a slot in a topic. When the data stored in this slot is consumed by any receiver, Kafka will be acknowledged by this consumption and moves the index to the next data slot.<br><img src=\"/2019/05/22/Implement-Zero-Data-Loss-in-Spark-Streaming/Spark-Streaming-flow-for-offsets.JPG\" alt=\"https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/\"><br>(diagram from <a href=\"https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/\" target=\"_blank\" rel=\"noopener\">https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/</a>)<br>When <code>enable.auto.commit</code> is set to be true, as soon as any receiver retrieves the data from the offset datastore in Kafka (here I use Kafka to store offsets), the receiver will automatically commit, which doesn’t ensure that the data is successfully executed in the spark streaming. Therefore, we have to disbale the auto-commit when we are creating DStream from Kafka. After the data is successfully processed, we manually commit the offset to the datastore by calling the Kafka direct API:</p>\n<pre><code class=\"scala\">stream.foreachRDD { rdd =&gt;\n      //get current offset\n      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n\n      //process data\n\n      //store data in some datastore\n      DataStoreDB.push(...param)\n\n      //commit offset to kafka\n      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)\n    }\n  }\n</code></pre>\n<p>If an error occurs at any point during the execution, the offset won;t be committed to the kafka offset store and hence the same data will be resent by kafka, which ensures that the data will be executed only once.</p>"},{"title":"Memory Leaks in Serveral Commonly Used Programming Languages","date":"2019-05-05T06:37:22.000Z","photos":["../images/Memory-Leaks.JPG"],"_content":"Usually when we talk about memory leaks we are actually talking about the memory leaks in heap memory. When an object is initialized, it will be dynamically allocated to a piece of memory in the heap and ready to be manipulated. After we perform some operations and the whole procedure is finished, the object stored in heap should also be erased; however in the case of memory leak, that piece of memory is not released but still held in the heap, marked as occupied but no reference refers to it.<!-- more -->\n\nWiki's Def:\n>[**Memory leak**](https://en.wikipedia.org/wiki/Memory_leak) is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that: \n- memory which is no longer needed is not released\n- an object is stored in memory but cannot be accessed by the running code\n\nWe usually encounter this issue in programming languages that don't have [**GC**](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science), for example C++ and C. For such languages, we have to manage the memory by ourselves which, if not done properly, will expose the risks of memory leaks.\n</br>\n\n## This is really common in C++\nLet's take a look in C++. There are literally hundreds of ways that can cause memory leaks and most of them won't be detected during compilation and even in runtime. Only a few leaks will not have any impact on the system; however if we are running a huge application and those leaks accumulate, that will significantly reduce the real runtime performance of the whole system.\n\nWe all know that when we allocate an object, we have to release the memory if this object is not used anymore. The way we release the memory is simply call the buid-in function **`free()`** or **`delete[]`**. However in C++ the procedure can exit anywhere. An exception can be thrown in the half way so that the code doesn't ever reach the line to release memory:\n```c++\nint sample(int n) {\n    void  *ptr = malloc(16);\n    if (n)\n        return -1; //memory leak here\n    free(ptr);\n    return 0;\n}\n```\nor:\n```c++\nclass Sample {\n    public:\n        init() { int *n = new int;  throw any_exception(); }\n        ~init() { delete n; }\n    private:\n        int *n;\n};\nSample *n = new Sample; //memory leak here\n```\nThe solution to the above examples is also really simple: check control flows and **do remember to call the destructor before anywhere the procedure may exit**. Well if you want to do it in a fancy way, you can use ***smart pointer*** alternatively:\n```c++\nclass Sample {\n    public:\n        init() { n = std::make_shared<int>(new int) }\n        ~init() {}\n    private:\n        std::shared_ptr<int> n;\n};\n```\nSmart pointer helps you manage this object and if it is not referred anymore, release its memory.\n</br>\n\n## free( )/delete is not enough\nNow your program has such a concrete control flow that **free( )** or **delete** is called before all the possible drop out. That is great but still not enough. **free( )** and **delete** can **only release the memory where the pointer is currently pointing to but not the pointer itself!** The pointer will still point to the original memory address but the content has been already removed. In this circumstance, the value of the pointer does not equal to **NULL**, instead some random values that cannot be predicted.\n```c++\nint main() {\n    char *p = (char*) malloc(sizeof(char) * 100);\n    strcpy(p, \"hello\");\n    free(p);\n    if (p != NULL) //doesn't prevent issue\n        strcpy(p, \"world\"); // error\n}\n```\nThis pointer p is called [***dangling pointer***](https://en.wikipedia.org/wiki/Dangling_pointer) or [***wild pointer***](https://en.wikipedia.org/wiki/Dangling_pointer) and will only be erased after the whole procedure is finished or terminated. The wild pointer is really risky because of its random behavior. Imagine there is something in your room that sometimes can be observed sometimes cannot, randomly breaks your stuff but never leaves footprint. In programming it is called wild pointer, and in real life it is called [**cat**](https://en.wikipedia.org/wiki/Cat). To prevent it, we should **always set the pointer to be NULL when it is not used/the memory is released**.\n\n***Note***: when you define a pointer without setting up its initial value, that pointer will also be a **wild pointer** and has a value of some random number (which doesn't equal to **NULL**). Hence it is necessary to set the value of a pointer to be **NULL** if it cannot be asigned a value at the beginning.\n\nFor some simple pointers, they can be reasigned to **NULL** to prevent **wild pointer**, however for a pointer referring to a hierarchical object, simply setting to **NULL** cannot resolve the potential issues. For example, you are using a **`vector`** in C++ :\n```c++\nvector <string> v\nint main() {\n    for (int i=0; i<1000000; i++)\n        v.push_back(\"test\");\n    \n    cout << v.capacity() << endl;  //memory usage: 54M\n    v.clear();\n    cout << v.capacity() << endl;  //memory usage: still 54M\n}\n```\nEven though we have cleared the vector and all its elements were indeed released, the capacity of the vector is still unchanged. **`clear()`** removed all its element but cannot shrink the size of the container. The same thing happens to other containers such as **`deque`**. To handle this, before **C++ 11**, we can swap the pointers:\n```c++\nint main() {\n    ...\n    v.clear();\n    vector<string>(v).swap(v); //new a vector with the same content and swap    \n    cout << v.capacity() << endl;  //memory usage: 0\n}\n```\nafter C++ 11, it provides function **`shrink_to_fit()`** to remove the extra allocated memory.\n</br>\n\n## GC doesn't avoid memory leaks\nIt's not suprising that GC can prevent most cases of memory leaks because it is runnig in an individual thread, checking the memory regularly and removing the unreferred objects. It is so powerful that porgrammers rarely pay attention to memory management and be aware of the memory leaks. **Java** is such language which has powerful and unruly GC that can be hardly controlled (call **`System.gc()`** doesn't certainly invoke GC). It helps to manage the memory in jvm, but it cannot avoid memory leaks.\n\nThere are mainly two cases that can lead to memory leaks in Java. One is the object which has a longer lifecycle keeps a reference to another object which has a shorter lifecycle:\n```java\npublic class Sample {\n    Object object;\n    public void anymethod(){\n        object = new Object();\n        ...\n    }\n    ...\n}\n```\nIf ***object*** is only used inside ***anymethod( )***, then after stack pops ***anymethod( )***, the lifecycle of ***object*** should also be ended. But for here, because class ***Sample*** is still proceeding and keeps the reference of ***object***, ***object*** cannot be collected by GC and hence leaks the memory. The solution will be either init ***object*** inside ***anymethod( )*** (as a local varible) or set ***object*** to be ***null*** after ***anymethod*** is finished.\n\nAnother case is the use of **`HashSet`**. ***HashSet*** is the implementation of hash-table and it stores elements according to their different hash values. In order to push and withdraw the same object in the ***HashSet***, we need to override the method **`HashCode()`** so that the same object has the same hash vaule and being stored in the same place in ***HashSet***. However, if we push something into the ***HashSet*** and then change some properties of this object (those properties are most likely to be used to calculate the hashcode), the hashcode of this object may vary and when we refer this object back to our ***HashSet*** to do some operations, for example delete this object from the ***HashSet***, this object might not be found in the set and hence cannot be deleted:\n```java\n    HashSet<Obejct> set = new HashSet<Object>();\n    Object something = new Object();\n    set.add(something);\n    something.doSomethingChanges();\n    set.contains(something);  //this may return false\n    set.remove(something);  //'something' cannot be removed if the previous line returns false      \n```\n</br>\n\n## Python\n\n","source":"_posts/Memory-Leak-in-Serveral-Commonly-Used-Programming-Languages.md","raw":"---\ntitle: Memory Leaks in Serveral Commonly Used Programming Languages\ndate: 2019-05-05 15:37:22\ntags: [C++, Java, Python, NodeJS]\nphotos: [\"../images/Memory-Leaks.JPG\"]\n---\nUsually when we talk about memory leaks we are actually talking about the memory leaks in heap memory. When an object is initialized, it will be dynamically allocated to a piece of memory in the heap and ready to be manipulated. After we perform some operations and the whole procedure is finished, the object stored in heap should also be erased; however in the case of memory leak, that piece of memory is not released but still held in the heap, marked as occupied but no reference refers to it.<!-- more -->\n\nWiki's Def:\n>[**Memory leak**](https://en.wikipedia.org/wiki/Memory_leak) is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that: \n- memory which is no longer needed is not released\n- an object is stored in memory but cannot be accessed by the running code\n\nWe usually encounter this issue in programming languages that don't have [**GC**](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science), for example C++ and C. For such languages, we have to manage the memory by ourselves which, if not done properly, will expose the risks of memory leaks.\n</br>\n\n## This is really common in C++\nLet's take a look in C++. There are literally hundreds of ways that can cause memory leaks and most of them won't be detected during compilation and even in runtime. Only a few leaks will not have any impact on the system; however if we are running a huge application and those leaks accumulate, that will significantly reduce the real runtime performance of the whole system.\n\nWe all know that when we allocate an object, we have to release the memory if this object is not used anymore. The way we release the memory is simply call the buid-in function **`free()`** or **`delete[]`**. However in C++ the procedure can exit anywhere. An exception can be thrown in the half way so that the code doesn't ever reach the line to release memory:\n```c++\nint sample(int n) {\n    void  *ptr = malloc(16);\n    if (n)\n        return -1; //memory leak here\n    free(ptr);\n    return 0;\n}\n```\nor:\n```c++\nclass Sample {\n    public:\n        init() { int *n = new int;  throw any_exception(); }\n        ~init() { delete n; }\n    private:\n        int *n;\n};\nSample *n = new Sample; //memory leak here\n```\nThe solution to the above examples is also really simple: check control flows and **do remember to call the destructor before anywhere the procedure may exit**. Well if you want to do it in a fancy way, you can use ***smart pointer*** alternatively:\n```c++\nclass Sample {\n    public:\n        init() { n = std::make_shared<int>(new int) }\n        ~init() {}\n    private:\n        std::shared_ptr<int> n;\n};\n```\nSmart pointer helps you manage this object and if it is not referred anymore, release its memory.\n</br>\n\n## free( )/delete is not enough\nNow your program has such a concrete control flow that **free( )** or **delete** is called before all the possible drop out. That is great but still not enough. **free( )** and **delete** can **only release the memory where the pointer is currently pointing to but not the pointer itself!** The pointer will still point to the original memory address but the content has been already removed. In this circumstance, the value of the pointer does not equal to **NULL**, instead some random values that cannot be predicted.\n```c++\nint main() {\n    char *p = (char*) malloc(sizeof(char) * 100);\n    strcpy(p, \"hello\");\n    free(p);\n    if (p != NULL) //doesn't prevent issue\n        strcpy(p, \"world\"); // error\n}\n```\nThis pointer p is called [***dangling pointer***](https://en.wikipedia.org/wiki/Dangling_pointer) or [***wild pointer***](https://en.wikipedia.org/wiki/Dangling_pointer) and will only be erased after the whole procedure is finished or terminated. The wild pointer is really risky because of its random behavior. Imagine there is something in your room that sometimes can be observed sometimes cannot, randomly breaks your stuff but never leaves footprint. In programming it is called wild pointer, and in real life it is called [**cat**](https://en.wikipedia.org/wiki/Cat). To prevent it, we should **always set the pointer to be NULL when it is not used/the memory is released**.\n\n***Note***: when you define a pointer without setting up its initial value, that pointer will also be a **wild pointer** and has a value of some random number (which doesn't equal to **NULL**). Hence it is necessary to set the value of a pointer to be **NULL** if it cannot be asigned a value at the beginning.\n\nFor some simple pointers, they can be reasigned to **NULL** to prevent **wild pointer**, however for a pointer referring to a hierarchical object, simply setting to **NULL** cannot resolve the potential issues. For example, you are using a **`vector`** in C++ :\n```c++\nvector <string> v\nint main() {\n    for (int i=0; i<1000000; i++)\n        v.push_back(\"test\");\n    \n    cout << v.capacity() << endl;  //memory usage: 54M\n    v.clear();\n    cout << v.capacity() << endl;  //memory usage: still 54M\n}\n```\nEven though we have cleared the vector and all its elements were indeed released, the capacity of the vector is still unchanged. **`clear()`** removed all its element but cannot shrink the size of the container. The same thing happens to other containers such as **`deque`**. To handle this, before **C++ 11**, we can swap the pointers:\n```c++\nint main() {\n    ...\n    v.clear();\n    vector<string>(v).swap(v); //new a vector with the same content and swap    \n    cout << v.capacity() << endl;  //memory usage: 0\n}\n```\nafter C++ 11, it provides function **`shrink_to_fit()`** to remove the extra allocated memory.\n</br>\n\n## GC doesn't avoid memory leaks\nIt's not suprising that GC can prevent most cases of memory leaks because it is runnig in an individual thread, checking the memory regularly and removing the unreferred objects. It is so powerful that porgrammers rarely pay attention to memory management and be aware of the memory leaks. **Java** is such language which has powerful and unruly GC that can be hardly controlled (call **`System.gc()`** doesn't certainly invoke GC). It helps to manage the memory in jvm, but it cannot avoid memory leaks.\n\nThere are mainly two cases that can lead to memory leaks in Java. One is the object which has a longer lifecycle keeps a reference to another object which has a shorter lifecycle:\n```java\npublic class Sample {\n    Object object;\n    public void anymethod(){\n        object = new Object();\n        ...\n    }\n    ...\n}\n```\nIf ***object*** is only used inside ***anymethod( )***, then after stack pops ***anymethod( )***, the lifecycle of ***object*** should also be ended. But for here, because class ***Sample*** is still proceeding and keeps the reference of ***object***, ***object*** cannot be collected by GC and hence leaks the memory. The solution will be either init ***object*** inside ***anymethod( )*** (as a local varible) or set ***object*** to be ***null*** after ***anymethod*** is finished.\n\nAnother case is the use of **`HashSet`**. ***HashSet*** is the implementation of hash-table and it stores elements according to their different hash values. In order to push and withdraw the same object in the ***HashSet***, we need to override the method **`HashCode()`** so that the same object has the same hash vaule and being stored in the same place in ***HashSet***. However, if we push something into the ***HashSet*** and then change some properties of this object (those properties are most likely to be used to calculate the hashcode), the hashcode of this object may vary and when we refer this object back to our ***HashSet*** to do some operations, for example delete this object from the ***HashSet***, this object might not be found in the set and hence cannot be deleted:\n```java\n    HashSet<Obejct> set = new HashSet<Object>();\n    Object something = new Object();\n    set.add(something);\n    something.doSomethingChanges();\n    set.contains(something);  //this may return false\n    set.remove(something);  //'something' cannot be removed if the previous line returns false      \n```\n</br>\n\n## Python\n\n","slug":"Memory-Leak-in-Serveral-Commonly-Used-Programming-Languages","published":1,"updated":"2019-05-10T10:13:39.539Z","comments":1,"layout":"post","link":"","_id":"cjyedivvt001bt8aoxbe6a37t","content":"<p>Usually when we talk about memory leaks we are actually talking about the memory leaks in heap memory. When an object is initialized, it will be dynamically allocated to a piece of memory in the heap and ready to be manipulated. After we perform some operations and the whole procedure is finished, the object stored in heap should also be erased; however in the case of memory leak, that piece of memory is not released but still held in the heap, marked as occupied but no reference refers to it.<a id=\"more\"></a></p>\n<p>Wiki’s Def:</p>\n<blockquote>\n<p><a href=\"https://en.wikipedia.org/wiki/Memory_leak\" target=\"_blank\" rel=\"noopener\"><strong>Memory leak</strong></a> is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that: </p>\n<ul>\n<li>memory which is no longer needed is not released</li>\n<li>an object is stored in memory but cannot be accessed by the running code</li>\n</ul>\n</blockquote>\n<p>We usually encounter this issue in programming languages that don’t have <a href=\"https://en.wikipedia.org/wiki/Garbage_collection_(computer_science\" target=\"_blank\" rel=\"noopener\"><strong>GC</strong></a>, for example C++ and C. For such languages, we have to manage the memory by ourselves which, if not done properly, will expose the risks of memory leaks.<br><br></p>\n<h2 id=\"This-is-really-common-in-C\"><a href=\"#This-is-really-common-in-C\" class=\"headerlink\" title=\"This is really common in C++\"></a>This is really common in C++</h2><p>Let’s take a look in C++. There are literally hundreds of ways that can cause memory leaks and most of them won’t be detected during compilation and even in runtime. Only a few leaks will not have any impact on the system; however if we are running a huge application and those leaks accumulate, that will significantly reduce the real runtime performance of the whole system.</p>\n<p>We all know that when we allocate an object, we have to release the memory if this object is not used anymore. The way we release the memory is simply call the buid-in function <strong><code>free()</code></strong> or <strong><code>delete[]</code></strong>. However in C++ the procedure can exit anywhere. An exception can be thrown in the half way so that the code doesn’t ever reach the line to release memory:</p>\n<pre><code class=\"c++\">int sample(int n) {\n    void  *ptr = malloc(16);\n    if (n)\n        return -1; //memory leak here\n    free(ptr);\n    return 0;\n}\n</code></pre>\n<p>or:</p>\n<pre><code class=\"c++\">class Sample {\n    public:\n        init() { int *n = new int;  throw any_exception(); }\n        ~init() { delete n; }\n    private:\n        int *n;\n};\nSample *n = new Sample; //memory leak here\n</code></pre>\n<p>The solution to the above examples is also really simple: check control flows and <strong>do remember to call the destructor before anywhere the procedure may exit</strong>. Well if you want to do it in a fancy way, you can use <strong><em>smart pointer</em></strong> alternatively:</p>\n<pre><code class=\"c++\">class Sample {\n    public:\n        init() { n = std::make_shared&lt;int&gt;(new int) }\n        ~init() {}\n    private:\n        std::shared_ptr&lt;int&gt; n;\n};\n</code></pre>\n<p>Smart pointer helps you manage this object and if it is not referred anymore, release its memory.<br><br></p>\n<h2 id=\"free-delete-is-not-enough\"><a href=\"#free-delete-is-not-enough\" class=\"headerlink\" title=\"free( )/delete is not enough\"></a>free( )/delete is not enough</h2><p>Now your program has such a concrete control flow that <strong>free( )</strong> or <strong>delete</strong> is called before all the possible drop out. That is great but still not enough. <strong>free( )</strong> and <strong>delete</strong> can <strong>only release the memory where the pointer is currently pointing to but not the pointer itself!</strong> The pointer will still point to the original memory address but the content has been already removed. In this circumstance, the value of the pointer does not equal to <strong>NULL</strong>, instead some random values that cannot be predicted.</p>\n<pre><code class=\"c++\">int main() {\n    char *p = (char*) malloc(sizeof(char) * 100);\n    strcpy(p, &quot;hello&quot;);\n    free(p);\n    if (p != NULL) //doesn&#39;t prevent issue\n        strcpy(p, &quot;world&quot;); // error\n}\n</code></pre>\n<p>This pointer p is called <a href=\"https://en.wikipedia.org/wiki/Dangling_pointer\" target=\"_blank\" rel=\"noopener\"><strong><em>dangling pointer</em></strong></a> or <a href=\"https://en.wikipedia.org/wiki/Dangling_pointer\" target=\"_blank\" rel=\"noopener\"><strong><em>wild pointer</em></strong></a> and will only be erased after the whole procedure is finished or terminated. The wild pointer is really risky because of its random behavior. Imagine there is something in your room that sometimes can be observed sometimes cannot, randomly breaks your stuff but never leaves footprint. In programming it is called wild pointer, and in real life it is called <a href=\"https://en.wikipedia.org/wiki/Cat\" target=\"_blank\" rel=\"noopener\"><strong>cat</strong></a>. To prevent it, we should <strong>always set the pointer to be NULL when it is not used/the memory is released</strong>.</p>\n<p><strong><em>Note</em></strong>: when you define a pointer without setting up its initial value, that pointer will also be a <strong>wild pointer</strong> and has a value of some random number (which doesn’t equal to <strong>NULL</strong>). Hence it is necessary to set the value of a pointer to be <strong>NULL</strong> if it cannot be asigned a value at the beginning.</p>\n<p>For some simple pointers, they can be reasigned to <strong>NULL</strong> to prevent <strong>wild pointer</strong>, however for a pointer referring to a hierarchical object, simply setting to <strong>NULL</strong> cannot resolve the potential issues. For example, you are using a <strong><code>vector</code></strong> in C++ :</p>\n<pre><code class=\"c++\">vector &lt;string&gt; v\nint main() {\n    for (int i=0; i&lt;1000000; i++)\n        v.push_back(&quot;test&quot;);\n\n    cout &lt;&lt; v.capacity() &lt;&lt; endl;  //memory usage: 54M\n    v.clear();\n    cout &lt;&lt; v.capacity() &lt;&lt; endl;  //memory usage: still 54M\n}\n</code></pre>\n<p>Even though we have cleared the vector and all its elements were indeed released, the capacity of the vector is still unchanged. <strong><code>clear()</code></strong> removed all its element but cannot shrink the size of the container. The same thing happens to other containers such as <strong><code>deque</code></strong>. To handle this, before <strong>C++ 11</strong>, we can swap the pointers:</p>\n<pre><code class=\"c++\">int main() {\n    ...\n    v.clear();\n    vector&lt;string&gt;(v).swap(v); //new a vector with the same content and swap    \n    cout &lt;&lt; v.capacity() &lt;&lt; endl;  //memory usage: 0\n}\n</code></pre>\n<p>after C++ 11, it provides function <strong><code>shrink_to_fit()</code></strong> to remove the extra allocated memory.<br><br></p>\n<h2 id=\"GC-doesn’t-avoid-memory-leaks\"><a href=\"#GC-doesn’t-avoid-memory-leaks\" class=\"headerlink\" title=\"GC doesn’t avoid memory leaks\"></a>GC doesn’t avoid memory leaks</h2><p>It’s not suprising that GC can prevent most cases of memory leaks because it is runnig in an individual thread, checking the memory regularly and removing the unreferred objects. It is so powerful that porgrammers rarely pay attention to memory management and be aware of the memory leaks. <strong>Java</strong> is such language which has powerful and unruly GC that can be hardly controlled (call <strong><code>System.gc()</code></strong> doesn’t certainly invoke GC). It helps to manage the memory in jvm, but it cannot avoid memory leaks.</p>\n<p>There are mainly two cases that can lead to memory leaks in Java. One is the object which has a longer lifecycle keeps a reference to another object which has a shorter lifecycle:</p>\n<pre><code class=\"java\">public class Sample {\n    Object object;\n    public void anymethod(){\n        object = new Object();\n        ...\n    }\n    ...\n}\n</code></pre>\n<p>If <strong><em>object</em></strong> is only used inside <strong><em>anymethod( )</em></strong>, then after stack pops <strong><em>anymethod( )</em></strong>, the lifecycle of <strong><em>object</em></strong> should also be ended. But for here, because class <strong><em>Sample</em></strong> is still proceeding and keeps the reference of <strong><em>object</em></strong>, <strong><em>object</em></strong> cannot be collected by GC and hence leaks the memory. The solution will be either init <strong><em>object</em></strong> inside <strong><em>anymethod( )</em></strong> (as a local varible) or set <strong><em>object</em></strong> to be <strong><em>null</em></strong> after <strong><em>anymethod</em></strong> is finished.</p>\n<p>Another case is the use of <strong><code>HashSet</code></strong>. <strong><em>HashSet</em></strong> is the implementation of hash-table and it stores elements according to their different hash values. In order to push and withdraw the same object in the <strong><em>HashSet</em></strong>, we need to override the method <strong><code>HashCode()</code></strong> so that the same object has the same hash vaule and being stored in the same place in <strong><em>HashSet</em></strong>. However, if we push something into the <strong><em>HashSet</em></strong> and then change some properties of this object (those properties are most likely to be used to calculate the hashcode), the hashcode of this object may vary and when we refer this object back to our <strong><em>HashSet</em></strong> to do some operations, for example delete this object from the <strong><em>HashSet</em></strong>, this object might not be found in the set and hence cannot be deleted:</p>\n<pre><code class=\"java\">    HashSet&lt;Obejct&gt; set = new HashSet&lt;Object&gt;();\n    Object something = new Object();\n    set.add(something);\n    something.doSomethingChanges();\n    set.contains(something);  //this may return false\n    set.remove(something);  //&#39;something&#39; cannot be removed if the previous line returns false      \n</code></pre>\n<p><br></p>\n<h2 id=\"Python\"><a href=\"#Python\" class=\"headerlink\" title=\"Python\"></a>Python</h2>","site":{"data":{}},"excerpt":"<p>Usually when we talk about memory leaks we are actually talking about the memory leaks in heap memory. When an object is initialized, it will be dynamically allocated to a piece of memory in the heap and ready to be manipulated. After we perform some operations and the whole procedure is finished, the object stored in heap should also be erased; however in the case of memory leak, that piece of memory is not released but still held in the heap, marked as occupied but no reference refers to it.</p>","more":"<p></p>\n<p>Wiki’s Def:</p>\n<blockquote>\n<p><a href=\"https://en.wikipedia.org/wiki/Memory_leak\" target=\"_blank\" rel=\"noopener\"><strong>Memory leak</strong></a> is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that: </p>\n<ul>\n<li>memory which is no longer needed is not released</li>\n<li>an object is stored in memory but cannot be accessed by the running code</li>\n</ul>\n</blockquote>\n<p>We usually encounter this issue in programming languages that don’t have <a href=\"https://en.wikipedia.org/wiki/Garbage_collection_(computer_science\" target=\"_blank\" rel=\"noopener\"><strong>GC</strong></a>, for example C++ and C. For such languages, we have to manage the memory by ourselves which, if not done properly, will expose the risks of memory leaks.<br><br></p>\n<h2 id=\"This-is-really-common-in-C\"><a href=\"#This-is-really-common-in-C\" class=\"headerlink\" title=\"This is really common in C++\"></a>This is really common in C++</h2><p>Let’s take a look in C++. There are literally hundreds of ways that can cause memory leaks and most of them won’t be detected during compilation and even in runtime. Only a few leaks will not have any impact on the system; however if we are running a huge application and those leaks accumulate, that will significantly reduce the real runtime performance of the whole system.</p>\n<p>We all know that when we allocate an object, we have to release the memory if this object is not used anymore. The way we release the memory is simply call the buid-in function <strong><code>free()</code></strong> or <strong><code>delete[]</code></strong>. However in C++ the procedure can exit anywhere. An exception can be thrown in the half way so that the code doesn’t ever reach the line to release memory:</p>\n<pre><code class=\"c++\">int sample(int n) {\n    void  *ptr = malloc(16);\n    if (n)\n        return -1; //memory leak here\n    free(ptr);\n    return 0;\n}\n</code></pre>\n<p>or:</p>\n<pre><code class=\"c++\">class Sample {\n    public:\n        init() { int *n = new int;  throw any_exception(); }\n        ~init() { delete n; }\n    private:\n        int *n;\n};\nSample *n = new Sample; //memory leak here\n</code></pre>\n<p>The solution to the above examples is also really simple: check control flows and <strong>do remember to call the destructor before anywhere the procedure may exit</strong>. Well if you want to do it in a fancy way, you can use <strong><em>smart pointer</em></strong> alternatively:</p>\n<pre><code class=\"c++\">class Sample {\n    public:\n        init() { n = std::make_shared&lt;int&gt;(new int) }\n        ~init() {}\n    private:\n        std::shared_ptr&lt;int&gt; n;\n};\n</code></pre>\n<p>Smart pointer helps you manage this object and if it is not referred anymore, release its memory.<br><br></p>\n<h2 id=\"free-delete-is-not-enough\"><a href=\"#free-delete-is-not-enough\" class=\"headerlink\" title=\"free( )/delete is not enough\"></a>free( )/delete is not enough</h2><p>Now your program has such a concrete control flow that <strong>free( )</strong> or <strong>delete</strong> is called before all the possible drop out. That is great but still not enough. <strong>free( )</strong> and <strong>delete</strong> can <strong>only release the memory where the pointer is currently pointing to but not the pointer itself!</strong> The pointer will still point to the original memory address but the content has been already removed. In this circumstance, the value of the pointer does not equal to <strong>NULL</strong>, instead some random values that cannot be predicted.</p>\n<pre><code class=\"c++\">int main() {\n    char *p = (char*) malloc(sizeof(char) * 100);\n    strcpy(p, &quot;hello&quot;);\n    free(p);\n    if (p != NULL) //doesn&#39;t prevent issue\n        strcpy(p, &quot;world&quot;); // error\n}\n</code></pre>\n<p>This pointer p is called <a href=\"https://en.wikipedia.org/wiki/Dangling_pointer\" target=\"_blank\" rel=\"noopener\"><strong><em>dangling pointer</em></strong></a> or <a href=\"https://en.wikipedia.org/wiki/Dangling_pointer\" target=\"_blank\" rel=\"noopener\"><strong><em>wild pointer</em></strong></a> and will only be erased after the whole procedure is finished or terminated. The wild pointer is really risky because of its random behavior. Imagine there is something in your room that sometimes can be observed sometimes cannot, randomly breaks your stuff but never leaves footprint. In programming it is called wild pointer, and in real life it is called <a href=\"https://en.wikipedia.org/wiki/Cat\" target=\"_blank\" rel=\"noopener\"><strong>cat</strong></a>. To prevent it, we should <strong>always set the pointer to be NULL when it is not used/the memory is released</strong>.</p>\n<p><strong><em>Note</em></strong>: when you define a pointer without setting up its initial value, that pointer will also be a <strong>wild pointer</strong> and has a value of some random number (which doesn’t equal to <strong>NULL</strong>). Hence it is necessary to set the value of a pointer to be <strong>NULL</strong> if it cannot be asigned a value at the beginning.</p>\n<p>For some simple pointers, they can be reasigned to <strong>NULL</strong> to prevent <strong>wild pointer</strong>, however for a pointer referring to a hierarchical object, simply setting to <strong>NULL</strong> cannot resolve the potential issues. For example, you are using a <strong><code>vector</code></strong> in C++ :</p>\n<pre><code class=\"c++\">vector &lt;string&gt; v\nint main() {\n    for (int i=0; i&lt;1000000; i++)\n        v.push_back(&quot;test&quot;);\n\n    cout &lt;&lt; v.capacity() &lt;&lt; endl;  //memory usage: 54M\n    v.clear();\n    cout &lt;&lt; v.capacity() &lt;&lt; endl;  //memory usage: still 54M\n}\n</code></pre>\n<p>Even though we have cleared the vector and all its elements were indeed released, the capacity of the vector is still unchanged. <strong><code>clear()</code></strong> removed all its element but cannot shrink the size of the container. The same thing happens to other containers such as <strong><code>deque</code></strong>. To handle this, before <strong>C++ 11</strong>, we can swap the pointers:</p>\n<pre><code class=\"c++\">int main() {\n    ...\n    v.clear();\n    vector&lt;string&gt;(v).swap(v); //new a vector with the same content and swap    \n    cout &lt;&lt; v.capacity() &lt;&lt; endl;  //memory usage: 0\n}\n</code></pre>\n<p>after C++ 11, it provides function <strong><code>shrink_to_fit()</code></strong> to remove the extra allocated memory.<br><br></p>\n<h2 id=\"GC-doesn’t-avoid-memory-leaks\"><a href=\"#GC-doesn’t-avoid-memory-leaks\" class=\"headerlink\" title=\"GC doesn’t avoid memory leaks\"></a>GC doesn’t avoid memory leaks</h2><p>It’s not suprising that GC can prevent most cases of memory leaks because it is runnig in an individual thread, checking the memory regularly and removing the unreferred objects. It is so powerful that porgrammers rarely pay attention to memory management and be aware of the memory leaks. <strong>Java</strong> is such language which has powerful and unruly GC that can be hardly controlled (call <strong><code>System.gc()</code></strong> doesn’t certainly invoke GC). It helps to manage the memory in jvm, but it cannot avoid memory leaks.</p>\n<p>There are mainly two cases that can lead to memory leaks in Java. One is the object which has a longer lifecycle keeps a reference to another object which has a shorter lifecycle:</p>\n<pre><code class=\"java\">public class Sample {\n    Object object;\n    public void anymethod(){\n        object = new Object();\n        ...\n    }\n    ...\n}\n</code></pre>\n<p>If <strong><em>object</em></strong> is only used inside <strong><em>anymethod( )</em></strong>, then after stack pops <strong><em>anymethod( )</em></strong>, the lifecycle of <strong><em>object</em></strong> should also be ended. But for here, because class <strong><em>Sample</em></strong> is still proceeding and keeps the reference of <strong><em>object</em></strong>, <strong><em>object</em></strong> cannot be collected by GC and hence leaks the memory. The solution will be either init <strong><em>object</em></strong> inside <strong><em>anymethod( )</em></strong> (as a local varible) or set <strong><em>object</em></strong> to be <strong><em>null</em></strong> after <strong><em>anymethod</em></strong> is finished.</p>\n<p>Another case is the use of <strong><code>HashSet</code></strong>. <strong><em>HashSet</em></strong> is the implementation of hash-table and it stores elements according to their different hash values. In order to push and withdraw the same object in the <strong><em>HashSet</em></strong>, we need to override the method <strong><code>HashCode()</code></strong> so that the same object has the same hash vaule and being stored in the same place in <strong><em>HashSet</em></strong>. However, if we push something into the <strong><em>HashSet</em></strong> and then change some properties of this object (those properties are most likely to be used to calculate the hashcode), the hashcode of this object may vary and when we refer this object back to our <strong><em>HashSet</em></strong> to do some operations, for example delete this object from the <strong><em>HashSet</em></strong>, this object might not be found in the set and hence cannot be deleted:</p>\n<pre><code class=\"java\">    HashSet&lt;Obejct&gt; set = new HashSet&lt;Object&gt;();\n    Object something = new Object();\n    set.add(something);\n    something.doSomethingChanges();\n    set.contains(something);  //this may return false\n    set.remove(something);  //&#39;something&#39; cannot be removed if the previous line returns false      \n</code></pre>\n<p><br></p>\n<h2 id=\"Python\"><a href=\"#Python\" class=\"headerlink\" title=\"Python\"></a>Python</h2>"}],"PostAsset":[{"_id":"source/_posts/Suffix-Tree/Banana_SuffixTree.JPG","slug":"Banana_SuffixTree.JPG","post":"cjyedivup0007t8aonhzgs8ph","modified":0,"renderable":0},{"_id":"source/_posts/Suffix-Tree/Compressed_PrefixTree.JPG","slug":"Compressed_PrefixTree.JPG","post":"cjyedivup0007t8aonhzgs8ph","modified":0,"renderable":0},{"_id":"source/_posts/Prefix-Tree/prefix_tree.JPG","slug":"prefix_tree.JPG","post":"cjyedivum0005t8aoelix8c8k","modified":0,"renderable":0},{"_id":"source/_posts/Suffix-Tree/Banana_PrefixTree.JPG","slug":"Banana_PrefixTree.JPG","post":"cjyedivup0007t8aonhzgs8ph","modified":0,"renderable":0},{"_id":"source/_posts/Suffix-Tree/PrefixTree_StringProblem.JPG","slug":"PrefixTree_StringProblem.JPG","post":"cjyedivup0007t8aonhzgs8ph","modified":0,"renderable":0},{"_id":"source/_posts/Suffix-Tree/prefix_tree.JPG","slug":"prefix_tree.JPG","post":"cjyedivup0007t8aonhzgs8ph","modified":0,"renderable":0},{"_id":"source/_posts/Implement-Zero-Data-Loss-in-Spark-Streaming/Spark-Streaming-flow-for-offsets.JPG","slug":"Spark-Streaming-flow-for-offsets.JPG","post":"cjyedivvs0019t8ao1t2ylzbe","modified":0,"renderable":0},{"_id":"source/_posts/Implement-Zero-Data-Loss-in-Spark-Streaming/wal_spark.JPG","slug":"wal_spark.JPG","post":"cjyedivvs0019t8ao1t2ylzbe","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"cjyedivub0000t8aogl3u33ce","tag_id":"cjyedivuk0004t8aon6bg81rz","_id":"cjyedivv5000et8ao0rwywjj0"},{"post_id":"cjyedivub0000t8aogl3u33ce","tag_id":"cjyedivuq0008t8aocjpb2qsr","_id":"cjyedivv6000ft8aov0llmhe3"},{"post_id":"cjyedivub0000t8aogl3u33ce","tag_id":"cjyedivuv000bt8ao9ppyka9e","_id":"cjyedivv8000ht8aoxld6hfa7"},{"post_id":"cjyedivuh0002t8aozzxepfjl","tag_id":"cjyedivv5000dt8ao41tehg5d","_id":"cjyedivva000jt8aozt6bxhoi"},{"post_id":"cjyedivuh0002t8aozzxepfjl","tag_id":"cjyedivv6000gt8ao114kxu67","_id":"cjyedivva000kt8aouq2dny57"},{"post_id":"cjyedivum0005t8aoelix8c8k","tag_id":"cjyedivv9000it8aopd47z69o","_id":"cjyedivvb000mt8aoxyeduudj"},{"post_id":"cjyedivun0006t8aogaj6kwbl","tag_id":"cjyedivva000lt8aoph995vl0","_id":"cjyedivvc000rt8aow96kw6fo"},{"post_id":"cjyedivun0006t8aogaj6kwbl","tag_id":"cjyedivvb000nt8ao2xfu9s9v","_id":"cjyedivvc000st8aobrrzyss6"},{"post_id":"cjyedivun0006t8aogaj6kwbl","tag_id":"cjyedivvc000ot8aotpsyfy2x","_id":"cjyedivvc000ut8aop25lvav9"},{"post_id":"cjyedivun0006t8aogaj6kwbl","tag_id":"cjyedivvc000pt8aoxj5obghh","_id":"cjyedivvd000vt8aomgk3dc6x"},{"post_id":"cjyedivup0007t8aonhzgs8ph","tag_id":"cjyedivvc000qt8aof5ek3fbc","_id":"cjyedivvd000xt8aogq45h9oq"},{"post_id":"cjyedivur0009t8ao5963efhv","tag_id":"cjyedivvc000tt8aoqlyg1s3u","_id":"cjyedivvf0010t8ao8ix5ndhc"},{"post_id":"cjyedivur0009t8ao5963efhv","tag_id":"cjyedivvd000wt8aorpehuxvk","_id":"cjyedivvf0011t8aon126laky"},{"post_id":"cjyedivur0009t8ao5963efhv","tag_id":"cjyedivvd000yt8aom98xp2j7","_id":"cjyedivvf0013t8aocb9jwkb2"},{"post_id":"cjyedivus000at8ao0b5j0ykq","tag_id":"cjyedivv5000dt8ao41tehg5d","_id":"cjyedivvf0015t8aoqyyr8s26"},{"post_id":"cjyedivus000at8ao0b5j0ykq","tag_id":"cjyedivvf0012t8aoxughd14l","_id":"cjyedivvg0016t8ao9a95y2jx"},{"post_id":"cjyedivus000at8ao0b5j0ykq","tag_id":"cjyedivvf0014t8ao0qpwmb53","_id":"cjyedivvg0017t8aoezurfn4f"},{"post_id":"cjyedivvr0018t8aoodq7yluq","tag_id":"cjyedivvt001at8aoqzr1rr09","_id":"cjyedivvw001ft8aoiv4ep2e0"},{"post_id":"cjyedivvr0018t8aoodq7yluq","tag_id":"cjyedivvv001ct8ao6redms5d","_id":"cjyedivvx001gt8ao3wp7sdtp"},{"post_id":"cjyedivvr0018t8aoodq7yluq","tag_id":"cjyedivvw001dt8aohop2980y","_id":"cjyedivvx001it8aodh4l6jps"},{"post_id":"cjyedivvs0019t8ao1t2ylzbe","tag_id":"cjyedivvw001et8aos4ypnmgz","_id":"cjyedivvy001kt8aou4wnm7zc"},{"post_id":"cjyedivvs0019t8ao1t2ylzbe","tag_id":"cjyedivvx001ht8ao54x4q53j","_id":"cjyedivvy001lt8aofywnmzid"},{"post_id":"cjyedivvt001bt8aoxbe6a37t","tag_id":"cjyedivvx001jt8aojlrv9l4b","_id":"cjyedivw0001ot8ao3fkjiwan"},{"post_id":"cjyedivvt001bt8aoxbe6a37t","tag_id":"cjyedivvy001mt8aoo2yl223j","_id":"cjyedivw1001pt8ao6sou0oph"},{"post_id":"cjyedivvt001bt8aoxbe6a37t","tag_id":"cjyedivvz001nt8ao79x2gt5z","_id":"cjyedivw1001qt8ao2bbhkh7v"},{"post_id":"cjyedivvt001bt8aoxbe6a37t","tag_id":"cjyedivv6000gt8ao114kxu67","_id":"cjyedivw1001rt8aofhbgwwrg"}],"Tag":[{"name":"CLI","_id":"cjyedivuk0004t8aon6bg81rz"},{"name":"Mac OS","_id":"cjyedivuq0008t8aocjpb2qsr"},{"name":"port","_id":"cjyedivuv000bt8ao9ppyka9e"},{"name":"VS Code","_id":"cjyedivv5000dt8ao41tehg5d"},{"name":"NodeJS","_id":"cjyedivv6000gt8ao114kxu67"},{"name":"Prefix Tree","_id":"cjyedivv9000it8aopd47z69o"},{"name":"Lisp","_id":"cjyedivva000lt8aoph995vl0"},{"name":"Scheme","_id":"cjyedivvb000nt8ao2xfu9s9v"},{"name":"Prefix Notation","_id":"cjyedivvc000ot8aotpsyfy2x"},{"name":"Functional Programming","_id":"cjyedivvc000pt8aoxj5obghh"},{"name":"Suffix Tree","_id":"cjyedivvc000qt8aof5ek3fbc"},{"name":"Golang","_id":"cjyedivvc000tt8aoqlyg1s3u"},{"name":"Array","_id":"cjyedivvd000wt8aorpehuxvk"},{"name":"Slice","_id":"cjyedivvd000yt8aom98xp2j7"},{"name":"NeteaseMusic","_id":"cjyedivvf0012t8aoxughd14l"},{"name":"网易云","_id":"cjyedivvf0014t8ao0qpwmb53"},{"name":"Redux","_id":"cjyedivvt001at8aoqzr1rr09"},{"name":"Saga","_id":"cjyedivvv001ct8ao6redms5d"},{"name":"React","_id":"cjyedivvw001dt8aohop2980y"},{"name":"Spark","_id":"cjyedivvw001et8aos4ypnmgz"},{"name":"Kafka","_id":"cjyedivvx001ht8ao54x4q53j"},{"name":"C++","_id":"cjyedivvx001jt8aojlrv9l4b"},{"name":"Java","_id":"cjyedivvy001mt8aoo2yl223j"},{"name":"Python","_id":"cjyedivvz001nt8ao79x2gt5z"}]}}