<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Implement Zero Data Loss in Spark Streaming]]></title>
    <url>%2F2019%2F05%2F22%2FImplement-Zero-Data-Loss-in-Spark-Streaming%2F</url>
    <content type="text"><![CDATA[This is a log from my own experience in spark streaming during my work. Base on different environment and servers, the strategy may vary. For here, I am working with Kafka and Google Cloud PubSub. BackgroundI have such workflow that the spark streaming receives dataset from both Kafka and PubSub, after doing some clean up and modeling, push the data to the Cloud Datastore.It works fine when the data stream is small and reports me the expected values; however while the number of end users is growing large, especially when the data stream turns to be erratic and sometimes considerably large if end users interact frequently with our UI pages, the spark streaming will get a lot of uncertain runtime errors. Those errors are most likely caused by the finite number of workers in our cluster. Also when there are too much stream rushing over to the server, the limited memory in cache will cause failures. After we upgraded the cluster on cloud, the number of errors is significantly reduced.But the data processed during the errors was permanently lost and cannot be recovered. The data running on spark is buffered in memory (cache) and will be cleared meanwhile a failure occurs. This is not desired since some valuable KPIs may be lost as well. To prevent such data loss, I tried different strategies. CheckpointCheckpointing is a process supported by spark streaming after version which will save RDDs in log after being checkpointed. There are two level of checkpoints: reliable and local. Reliable checkpoint ensures that the data is stored permanentlly on HDFS, S3 or other distributed filesystems. Each job on cluster will create a directory for saving checkpointed logs, which will look pretty much like the directory below: ├── &quot;SomeUUID&quot; │ └── rdd-# │ ├── part-timestamp1 │ ├── .part-timestamp1.crc │ ├── part-timestamp2 │ └── .part-timestamp2.crc Since the data stream will be replicated on disk, the performance will slow down due to file I/O. Local checkpoint privileges performance over fault-tolerance which will persist RDDs on local storage in the executors. Read or write will be faster in this case; however if a driver fails, the data not yet executed may not be recoverable. As default, the data storeage level is set to MEMORY_AND_DISK which saves data in cache and disk (some in cache and some in disk). For here I changed to MEMORY_AND_DISK_SER_2 (more details can be referred to here). The different is that, unlike cache only, the checkpoints doesn’t save DAG with all the parents of RDDs; instead, they only save particular RDDs and remain for a longer time than cache. The time of persistance is strictly related to the executed computation and ends with the end of the spark application. To apply the checkpoint machanism, you just simply need to set the checkpoint directory when you are creating the StreamingContext def createContext(...params): StreamingContext = { val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ... ssc.checkpoint(CHECK_POINT_DIR) ssc } and before an action is operated on RDD: sRDD.checkpoint() sRDD.foreachPartition { partitionOfRecords =&gt; { ... } } and sRDD.isCheckpointed() will return true. For cleaning, the RDDs stored in cache will be cleaned with all other memory after the whole spark application is finished or terminated; the reliable RDDs stored on disk can be cleaned manually or setspark.cleaner.referenceTracking.cleanCheckpoints property to be true to enable automatic cleaning. This driver recovery mechanism is sufficient to ensure zero data loss if all data was reliably store in the system. However for my circumstance, the data is read from kafka and some of the data buffered in memory could be lost. If the driver process fails, all the executors running will be killed as well, along with any data in their memory. This pushes me to look for other mechanisms which are more advanced. Write Ahead LogsWrite Ahead Logs are used in database and file systems to ensure the durability of any data operations. The intention of the operation is first written down into a durable log , and then the operation is applied to the data. If the system fails in the middle of applying the operation, it can recover by reading the log and reapplying the operations it had intended to do. Spark streaming uses Receiver to read data from Kafka. They run as long-running tasks in the executors and store the revecived data in the memory of the executors. If you enable the checkpoint, the data will be checkpointed either in cache or disk in executors before porceed into the application drivers. Unlike checkpoint, applying WAL will instead backup the recevied data in an external fault-tolerant filesystem. And after the executor batches the received data and sends to the driver, WAL supports another log to store the block metadata into external filesystem before being executed.(diagram from https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html)Spark streaming starts supporting WAL after version 1.2 and can be enabled by setting the config: val ssc = StreamingContext.createContext(...params) def createContext(...params): StreamingContext = { val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;) .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ... ssc.checkpoint(CHECK_POINT_DIR) ssc } Set the spark automatically clean checkpoints to release disk memory: val ssc = StreamingContext.createContext(...params) def createContext(...params): StreamingContext = { val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;) .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) .set(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, &quot;true&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ... ssc.checkpoint(CHECK_POINT_DIR) ssc } This will not clean the latest checkpoint as it is still referred to by the application to recover from possible failures. If you expect to restart the application driver if it crashed due to some errors and exactly start from where it crashed last time instead of performing the whole operation once again, you can create StreamingContext from the previous checkpoint by using the method getOrCreate(): val ssc = StreamingContext.getOrCreate(CHECK_POINT_DIR, () =&gt; createContext(...params)) def createContext(...params): StreamingContext = { val sparkConf = new SparkConf().setAppName(&quot;KpiAnalysis&quot;) .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;) .set(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, &quot;true&quot;) val ssc = new StreamingContext(sparkConf, Seconds(1)) ... ssc.checkpoint(CHECK_POINT_DIR) ssc } However after I tried switching to getOrCreate(), I had the following exception: Exception in thread &quot;main&quot; org.apache.spark.SparkException: Failed to read checkpoint from directory checkpointDir at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:368) at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827) ... Caused by: java.io.IOException: java.lang.ClassCastException: cannot assign instance of com.some.project$$anonfun$1 to field org.apache.spark.streaming.dstream.MappedDStream.org$apache$spark$streaming$dstream$MappedDStream$$mapFunc of type scala.Function1 in instance of org.apache.spark.streaming.dstream.MappedDStream at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310) at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ... It is because this is not the first time I use checkpoints, and there already exsists some other checkpoints in the same root directory (etc, the simple checkpoint discussed previously). Since the application tried to read from those unrelated checkpoints and expected to cast them to generate new context, it would throw such ClassCastException. This can be easily solved by deleting original checkpoints in HDFS locals (manually). WAL is an advanced checkpoint mechanism and also simple to be applied. Compared with checkpoint, it saves data into an external filesystem so that even though if the executor is terminated and the data in memory is clean-uped, data still can be recovered from the external filesystem. However. it can only recover the data which is logged in the filesystem, if the drivers fail due to some error, the executor will be terminated as well, so as the WAL writer. Then the rest incomming data will not be logged into the filesystem and hence, is not recoverable. It can be tested by calling stop to the StreamContext: def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit Once it is called, the following console log interpretes that the WAL Writer is interrupted as well: ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver WARN BlockGenerator: Cannot stop BlockGenerator as its not in the Active state [state = StoppedAll] WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted. And also the data will not be recoverable across applications or Spark upgrades and hence not very reliable Kafka Direct APIThis mechanism is only available when you data source is Kafka. Kafka supports a commit strategy which is able to help you manage offsets of each topics. Each offset points to a slot in a topic. When the data stored in this slot is consumed by any receiver, Kafka will be acknowledged by this consumption and moves the index to the next data slot.(diagram from https://blog.cloudera.com/blog/2017/06/offset-management-for-apache-kafka-with-apache-spark-streaming/)When enable.auto.commit is set to be true, as soon as any reveiver retrieves the data from the Offset datastore in Kafka (here I use Kafka to store offsets), the receiver will automatically commit, which doesn’t ensure that the data is successfully executed in the spark streaming. Therefore, we have to disbale the auto-commit when we are creating DStream from Kafka. After the data is successfully processed, we manually commit the offset to the datastore by calling the Kafka direct API: stream.foreachRDD { rdd =&gt; //get current offset val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges //process data //store data in some datastore DataStoreDB.push(...param) //commit offset to kafka stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges) } } This ensures that the data will be executed only once.]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何用VS Code免翻墙听网易云音乐]]></title>
    <url>%2F2019%2F05%2F22%2F%E5%A6%82%E4%BD%95%E7%94%A8VS-Code%E5%85%8D%E7%BF%BB%E5%A2%99%E5%90%AC%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%2F</url>
    <content type="text"><![CDATA[在墙外打开网易云音乐发现全是灰色的？ 抱歉您所在区域无法播放？ 该资源暂无版权？ 需要vip？VS Code一行script全搞定～～ 打开VS Code 左侧 Extensions 搜索 Netease Music (VSC Netease Music) 或者点这里 点击 Install 后重启 VS Code 作者提供了基于 VS Code 自身插件工具 Webview 实现的通过替换 electron 动态链接库翻墙的插件, 有详细的中文文档, 这里 Unix Shell 用户（包括 MacOS）可以直接在 Terminal （任意dir）输入以下script: curl https://gist.githubusercontent.com/nondanee/f157bbbccecfe29e48d87273cd02e213/raw | python script 输出结果为: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 3060 100 3060 0 0 8496 0 --:--:-- --:--:-- --:--:-- 8476 vscode 1.34.0 x64 electron 3.1.8 download well replace done remove temp 替换完毕后打开 VS Code, 上方工具栏 Go -&gt; Go to file… 或者 Command(⌘) P, 输入： &gt;NeteaseMusic: Start 等待 editor 跳出 Please preserve this webview tab 后就可以使用所有网易云的功能了, 注意这个tab页面必须要保留（不能关闭） 使用时直接Command(⌘) P**后输入命令即可, 命令都是以 &gt;NeteaseMusic起头, 输入 &gt;ne 后VS Code会自动跳出并补齐可行指令, 甚至还可以登陆收藏查看评论]]></content>
      <tags>
        <tag>VS Code</tag>
        <tag>NeteaseMusic</tag>
        <tag>网易云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Slice in Golang]]></title>
    <url>%2F2019%2F05%2F12%2FType-slice-in-Golang%2F</url>
    <content type="text"><![CDATA[This article is a summary from Andrew Gerrand’s blog Golang has an unique type slice which is an abstraction built on top of Go’s array type. They are really similar but providing different means of working with sequences of typed data. So to understand slices we must first understand arrays. Arrays in GoAn array in Go has to specify its length and element type. The size of the array is fixed and its length is part of its type. For example [4]int and [5]int are distinct and have different types even though they all store integers. And contrary to C/C++, the initial value of an array will be filled with 0 if it is not initialized. var a [4]int a[0] = 1 i := a[0] j := a[1] //i == 1 //j == 0 Go’s arrays are values. An array variable denotes the entire array; it is not a pointer to the first array element (as would be the case in C/C++). This means that when you assign or pass around an array value you will make a copy of its contents. (To avoid the copy you could pass a pointer to the array, but then that’s a pointer to an array, not an array) An array literal can be specified like so: b := [2]string{&quot;aa&quot;, &quot;bb&quot;} Or, you can have the compiler counting the array elements for you: b := [...]string{&quot;aa&quot;, &quot;bb&quot;} In both cases, the type of b is [2]string. Slices in GoArrays are a bit inflexible, so you don’t see them too often in the code. Slices, though, are everywhere. Unlike an array type, a slice type has no specific length: b := []string{&quot;aa&quot;, &quot;bb&quot;} We can use build-in function make() to define a slice: func make([]T, len, cap) []T T represent the type of the elements. Function make accepts type, length and capacity(optional) as parameters. When it is called, make will allocate an array and returns a slice that refers to that array var s []byte s = make([]byte, 5, 5) //s == []byte{0, 0, 0, 0, 0} If cap is not specified, it will be init as the value of len. We can use the build-in functions len() and cap() to check the length and capacity of a slice: len(s) == 5 cap(s) == 5 The zero value of a slice is nil. The len and cap functions will both return 0 for a nil slice. A slice can also be formed by “slicing” an existing slice or array, for example, the expression b[1:4] creates a slice including elements 1 through 3 of b: b := []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;} // b[1:4] == []byte{&#39;b&#39;, &#39;c&#39;, &#39;d&#39;}, sharing the same storage as b The start and end indices of a slice expression are optional; they default to zero and the slice’s length respectively: // b[:2] == []byte{&#39;a&#39;, &#39;b&#39;} // b[2:] == []byte{&#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;} // b[:] == b This is also the syntax to create a slice given an array: x := [3]string{&quot;Лайка&quot;, &quot;Белка&quot;, &quot;Стрелка&quot;} s := x[:] // a slice referencing the storage of x Slicing does not copy the slice’s data. It creates a new slice value that points to the original array. This makes slice operations as efficient as manipulating array indices. Therefore, modifying the elements of a re-slice modifies the elements of the original slice: d := []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;} e := d[2:] // e == []byte{&#39;c&#39;, &#39;d&#39;} // now change the re-slice will also change the original slice e[1] = &#39;m&#39; // e == []byte{&#39;c&#39;, &#39;m&#39;} // d == []byte{&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;m&#39;} A slice cannot be grown beyond its capacity. Attempting to do so will cause a runtime panic, just as when indexing outside the bounds of a slice or array. Similarly, slices cannot be re-sliced below zero to access earlier elements in the array. Double the capacity of a sliceTo increase the capacity of a slice, we must create a new, larger slice and copy the contents of the original slice into it. The belowing example shows how to create a new slice t whihc doubles the capacity of s: t := make([]byte, len(s), (cap(s) * 2)) for i:= range s { t[i] = s[i] } s = t //reassign s to t The loop can be replaced by the build-in function copy(), which copies the data from source and returns the number of elements copied: func copy(dst, src []T) int The function copy supports copying between slices of different lengths (it will copy only up to the smaller number of elements) and the case that two slices refer to the same array. Using copy, the above double size code snippet can be rewritten as: t := make([]byte, len(s), (cap(s) * 2)) copy(t, s) s = t A common operation is to append new data to the tail of a slice: func AppendByte(slice []byte, data ...type) []byte { m := len(slice) n := m + len(data) if n &gt; cap(slice) { //if the original capacity is not big enough newSlice := make([]byte, (n + 1) * 2) copy(newSlice, slice) slice = newSlice } slice = slice[0:n] //shrink the capacity to the length of data copy(slice[m:n], data) return slice } This customized AppendByte function is really useful because we can fully control the size of a slice. However most programs do need such complete control. Go provides a build-in function append() which appends slice x to the end of slice s, expanding s if needed: func append(s []T, x ...T) []T Using … to append one slice to the end of another: a := []string{&quot;aa&quot;, &quot;bb&quot;} b := []string{&quot;cc&quot;, &quot;dd&quot;} a = append(a, b...) //same as append(a, b[0], b[1], b[2]) Another example of append: func Filter(s []int, fn func(int) bool) []int { var p []int // p == nil for _, v := range s { if fn(v) { p = append(p, v) } } return p }]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>Array</tag>
        <tag>Slice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to debug NodeJS on VS Code]]></title>
    <url>%2F2019%2F05%2F08%2FHow-to-debug-NodeJS-on-VS-Code%2F</url>
    <content type="text"><![CDATA[Here are the steps to start debug mode in VS Code: On the left side bar, click “debug” icon to switch to debug viewlet On the top left, click the gear icon Then launch.json will be opened in the editor Replace the content of the file to be: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;type&quot;: &quot;node&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;name&quot;: &quot;Launch app.js&quot;, &quot;program&quot;: &quot;${workspaceRoot}/app.js&quot;, &quot;stopOnEntry&quot;: true, &quot;args&quot;: [ &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot; ] } ] } Replace the command line arguments to whatever you need Start the debugger or press F5 You are all good to go! If your program reads from stdin, please add a “console” attribute to the launch config: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;type&quot;: &quot;node&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;name&quot;: &quot;Launch app.js&quot;, &quot;program&quot;: &quot;${workspaceRoot}/app.js&quot;, &quot;stopOnEntry&quot;: true, &quot;args&quot;: [ &quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot; ], &quot;console&quot;: &quot;integratedTerminal&quot; } ] } If you are running the program in the terminal, you can change the content alternatively to be: { &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;type&quot;: &quot;node&quot;, &quot;request&quot;: &quot;attach&quot;, &quot;name&quot;: &quot;Attach to app.js&quot;, &quot;port&quot;: &quot;5858&quot; } ] } The port is the debug port and it has nothing to do with your program (no matter it is a service or not). Then in the terminal, run the command: node --debug-brk app.js arg1 arg2 arg3... The --debug-brk lets your program wait for the debugger to attach to. So there is no problem that it terminates before the debugger could attach. Running such command, you may encounter a warning below: (node:31245) [DEP0062] DeprecationWarning: `node --inspect --debug-brk` is deprecated. Please use `node --inspect-brk` instead. As discussed in microsoft github offical repository, currently there is no way to prevent this happening. The reason why using --inspect --debug-brk is explained here: This combination of args is the only way to enter debug mode across all node versions. At some point I’ll switch to inspect-brk if we don’t want to support node 6.x anymore, or will do version detection for it and do something for runtimeExecutable scenarios. The problem is that we do not really know what version of node a user is using, so we cannot adapt the flags we use to the node version in order to minimize the resulting deprecation warnings.]]></content>
      <tags>
        <tag>VS Code</tag>
        <tag>NodeJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Check Open TCP/IP Ports in Mac OS X]]></title>
    <url>%2F2019%2F05%2F06%2FHow-to-Check-Open-TCP-IP-Ports-in-Mac-OS-X%2F</url>
    <content type="text"><![CDATA[The core of Mac OS is Darwin and we can use most of the CLI tools in Mac OS just like how it feels like in Linux. If we want to check out the current ports in usage, the command netstat is useful: netstat -ap tcp | grep -i &quot;listen&quot; That will print out something like this in the console: Achive Internet connections(including servers) Proto Recv-Q Send-Q Local Address Foreign Address (state) tcp4 0 0 localhost.25035 *.* LISTEN That works but the problem is that it doesn’t show up the names of the procedures which occupy the ports. Sometimes we want to know precisely which program is exposing the port. Then found out that there is another command lsof: sudo lsof -nP -iTCP:PortNumber -sTCP:LISTEN which prints out all the processes running in a given port with specific names: COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME syslogd 350 root 5w VREG 222,5 0 440818 /var/adm/messages syslogd 350 root 6w VREG 222,5 339098 6248 /var/log/syslog cron 353 root cwd VDIR 222,5 512 254550 /var -- atjobs -n : No dns (no host name)-P : List port number instead of its name-i : Lists IP sockets To view the port associated with a daemon: lsof -i -n -P | grep python If we just want to see the name: sudo lsof -i :PortNumber | grep LISTEN Get all running PID in a specific port: sudo lsof -i :PortNumber| grep LISTEN | awk &#39;{ print $2; }&#39; | head -n 2 | grep -v PID And then we can kill all the processes: sudo kill -9 $(sudo lsof -i :PortNumber| grep LISTEN | awk &#39;{ print $2; }&#39; | head -n 2 | grep -v PID) list all commands: lsof -h]]></content>
      <tags>
        <tag>CLI</tag>
        <tag>Mac OS</tag>
        <tag>port</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Memory Leaks in Serveral Commonly Used Programming Languages]]></title>
    <url>%2F2019%2F05%2F05%2FMemory-Leak-in-Serveral-Commonly-Used-Programming-Languages%2F</url>
    <content type="text"><![CDATA[Usually when we talk about memory leaks we are actually talking about the memory leaks in heap memory. When an object is initialized, it will be dynamically allocated to a piece of memory in the heap and ready to be manipulated. After we perform some operations and the whole procedure is finished, the object stored in heap should also be erased; however in the case of memory leak, that piece of memory is not released but still held in the heap, marked as occupied but no reference refers to it. Wiki’s Def: Memory leak is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that: memory which is no longer needed is not released an object is stored in memory but cannot be accessed by the running code We usually encounter this issue in programming languages that don’t have GC, for example C++ and C. For such languages, we have to manage the memory by ourselves which, if not done properly, will expose the risks of memory leaks. This is really common in C++Let’s take a look in C++. There are literally hundreds of ways that can cause memory leaks and most of them won’t be detected during compilation and even in runtime. Only a few leaks will not have any impact on the system; however if we are running a huge application and those leaks accumulate, that will significantly reduce the real runtime performance of the whole system. We all know that when we allocate an object, we have to release the memory if this object is not used anymore. The way we release the memory is simply call the buid-in function free() or delete[]. However in C++ the procedure can exit anywhere. An exception can be thrown in the half way so that the code doesn’t ever reach the line to release memory: int sample(int n) { void *ptr = malloc(16); if (n) return -1; //memory leak here free(ptr); return 0; } or: class Sample { public: init() { int *n = new int; throw any_exception(); } ~init() { delete n; } private: int *n; }; Sample *n = new Sample; //memory leak here The solution to the above examples is also really simple: check control flows and do remember to call the destructor before anywhere the procedure may exit. Well if you want to do it in a fancy way, you can use smart pointer alternatively: class Sample { public: init() { n = std::make_shared&lt;int&gt;(new int) } ~init() {} private: std::shared_ptr&lt;int&gt; n; }; Smart pointer helps you manage this object and if it is not referred anymore, release its memory. free( )/delete is not enoughNow your program has such a concrete control flow that free( ) or delete is called before all the possible drop out. That is great but still not enough. free( ) and delete can only release the memory where the pointer is currently pointing to but not the pointer itself! The pointer will still point to the original memory address but the content has been already removed. In this circumstance, the value of the pointer does not equal to NULL, instead some random values that cannot be predicted. int main() { char *p = (char*) malloc(sizeof(char) * 100); strcpy(p, &quot;hello&quot;); free(p); if (p != NULL) //doesn&#39;t prevent issue strcpy(p, &quot;world&quot;); // error } This pointer p is called dangling pointer or wild pointer and will only be erased after the whole procedure is finished or terminated. The wild pointer is really risky because of its random behavior. Imagine there is something in your room that sometimes can be observed sometimes cannot, randomly breaks your stuff but never leaves footprint. In programming it is called wild pointer, and in real life it is called cat. To prevent it, we should always set the pointer to be NULL when it is not used/the memory is released. Note: when you define a pointer without setting up its initial value, that pointer will also be a wild pointer and has a value of some random number (which doesn’t equal to NULL). Hence it is necessary to set the value of a pointer to be NULL if it cannot be asigned a value at the beginning. For some simple pointers, they can be reasigned to NULL to prevent wild pointer, however for a pointer referring to a hierarchical object, simply setting to NULL cannot resolve the potential issues. For example, you are using a vector in C++ : vector &lt;string&gt; v int main() { for (int i=0; i&lt;1000000; i++) v.push_back(&quot;test&quot;); cout &lt;&lt; v.capacity() &lt;&lt; endl; //memory usage: 54M v.clear(); cout &lt;&lt; v.capacity() &lt;&lt; endl; //memory usage: still 54M } Even though we have cleared the vector and all its elements were indeed released, the capacity of the vector is still unchanged. clear() removed all its element but cannot shrink the size of the container. The same thing happens to other containers such as deque. To handle this, before C++ 11, we can swap the pointers: int main() { ... v.clear(); vector&lt;string&gt;(v).swap(v); //new a vector with the same content and swap cout &lt;&lt; v.capacity() &lt;&lt; endl; //memory usage: 0 } after C++ 11, it provides function shrink_to_fit() to remove the extra allocated memory. GC doesn’t avoid memory leaksIt’s not suprising that GC can prevent most cases of memory leaks because it is runnig in an individual thread, checking the memory regularly and removing the unreferred objects. It is so powerful that porgrammers rarely pay attention to memory management and be aware of the memory leaks. Java is such language which has powerful and unruly GC that can be hardly controlled (call System.gc() doesn’t certainly invoke GC). It helps to manage the memory in jvm, but it cannot avoid memory leaks. There are mainly two cases that can lead to memory leaks in Java. One is the object which has a longer lifecycle keeps a reference to another object which has a shorter lifecycle: public class Sample { Object object; public void anymethod(){ object = new Object(); ... } ... } If object is only used inside anymethod( ), then after stack pops anymethod( ), the lifecycle of object should also be ended. But for here, because class Sample is still proceeding and keeps the reference of object, object cannot be collected by GC and hence leaks the memory. The solution will be either init object inside anymethod( ) (as a local varible) or set object to be null after anymethod is finished. Another case is the use of HashSet. HashSet is the implementation of hash-table and it stores elements according to their different hash values. In order to push and withdraw the same object in the HashSet, we need to override the method HashCode() so that the same object has the same hash vaule and being stored in the same place in HashSet. However, if we push something into the HashSet and then change some properties of this object (those properties are most likely to be used to calculate the hashcode), the hashcode of this object may vary and when we refer this object back to our HashSet to do some operations, for example delete this object from the HashSet, this object might not be found in the set and hence cannot be deleted: HashSet&lt;Obejct&gt; set = new HashSet&lt;Object&gt;(); Object something = new Object(); set.add(something); something.doSomethingChanges(); set.contains(something); //this may return false set.remove(something); //&#39;something&#39; cannot be removed if the previous line returns false Python]]></content>
      <tags>
        <tag>NodeJS</tag>
        <tag>C++</tag>
        <tag>Java</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prefix Notation]]></title>
    <url>%2F2019%2F04%2F30%2FPrefix-Notation%2F</url>
    <content type="text"><![CDATA[( 20 + 5 ) ( 16 / 4 ) Such expressions which denote procedures, are called combinations. The left and the right elements are called operands, and the element in the middle to indicate the operation is called operator. This is the most common style we have seen by now; however there is another way to construct a procedure known as prefix notation: ( + 20 5 ) ( / 16 4 ) Instead of injecting the operator between operands, which is a more human readable style, the prefix notation requires the operator always to be at the left most. conditions: ( define ( abs x ) ( cond (( &gt; x 0 ) x ) (( = x 0 ) 0 ) (( &lt; x 0 ) ( - x )))) The general form can be expressed as: ( cond (&lt;\P1&gt; &lt;\E1&gt;) (&lt;\P2&gt; &lt;\E2&gt;) … (&lt;\Pn&gt; &lt;\En&gt;)) If none of them is evaluated to be true, then the value of the cond will be undefined. It can also be simplified by using else: ( define ( abs x ) ( cond (( &lt; x 0 ) ( - x )) ( else x ))) If there is only two predicates (the expression to be interpreted as either true of false), then it can use a special form if: ( define ( abs x ) ( if ( &lt; x 0 ) ( - x ) x )) The general form of an if expression is: ( if &lt;\predicate&gt; &lt;\consequent&gt; &lt;\alternative&gt; ) The logic operators: ( and &lt;\E1&gt; … &lt;\En&gt; )( or &lt;\E1&gt; … &lt;\En&gt; )( not ) Then use the logic operators to define a predicate to evaluate if a number id larger or equal to the other one: ( define ( &gt;= x y ) ( or ( &gt; x y ) ( = x y )) That is all the syntax, there is no loop in a functional programming language! RecursionConsidering the factorial function: n! = n ⋅ (n-1) ⋅ (n-2) ⋅ … ⋅2⋅1 Which can be computed as: n! = n ⋅ (n-1)! If we end it up with 1!, then simply output 1. Then the factorial function can be implemented in linear recursion: ( define ( factorial n ) ( if ( = n 1 ) 1 ( * n ( factorial ( - n 1 ))))) Linear recursion defines that the computation chains of operations is proportional to n and hence grows linearly. There is also another pattern of recursion, known as Tree Recursion. The best example will be the Fibonacci series, in which each element is the sum of the previous two: ( define ( fib n ) ( cond ( = n 0 ) 0 ) ( = n 1 ) 1 ) ( else ( + ( fib ( - n 1 ) ) ( fib ( - n 2 ) ))))) You may find out that this procedure is not really efficient because to compute fib( - n 1), fib( - n 2) has to be computed one more time which causes duplicated work.Therefore, instead of Tree Recursion, let’s try to convert it to be Linear Recursion. Reasign the sum of a and b to a, and the previous a to b: ( define ( fib n ) ( iterate 1 0 n )) ( define ( iterate a b count ) ( if ( = count 0 ) b ( iterate ( + a b ) a ( - count 1 )))) LambdaInstead of defining some trivial procedures so that we can pass them as arguments of the other procedures, functional programming provides Lambda Expression: ( lambda ( &lt;\formal-param&gt; ) &lt;\body&gt; ) For instance, ( define ( Add a b ) ( + a b )) can be written as: ( define add ( lambda ( a b ) ( + a b ))) And operators can also be represented by Lambda Expression: (( lambda ( a b ) ( + ( * a a ) ( * b b ))) 2 3 ) Another use of Lambda Expression is creating local variables. An expression can be binded with a specific name by using keyword let. The above example then can be interpreted as: ( define ( sumsqr x y ) ( let ( a ( * x x )) ( b ( * y y )) ( + a b ))) Note: The scope of a variable specified by a let is only applied to the body of the let. For example, if the evalue of x is 2, then the expression: ( let (( x 3 ) ( y ( + x 2 ))) ( * x y )) The value of y will be 4 as being outside of the let body, and the output will be 3 * 4 = 12. It seems like let is really similar to define; however, in the most cases, we much prefer using let and only apply define to internal procedures.]]></content>
      <tags>
        <tag>Lisp</tag>
        <tag>Scheme</tag>
        <tag>Prefix Notation</tag>
        <tag>Functional Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Brief Introduce to Redux Saga]]></title>
    <url>%2F2019%2F04%2F28%2FA-Brief-Introduce-to-Redux-Saga%2F</url>
    <content type="text"><![CDATA[If you are quite experienced with redux, which is a predictable state container for JavaScript applications (Note: even thouth React and Redux is a popular combination to build fast and powerful apps, Redux is not necessarily combined with React), you are definitely feeling comfortable with its powerful store which manages all the global states and provides much cleaner logic flows to change them. If you are new to redux, here is the guide to dive before we start our topic. In a complex javascript application, asynchronous function is always one of the most annoying part where encounters tons of bugs. If not handle them properly, the app usually ends up with call back hell. Haven’t heard of CallBack Hell?Well, in javascript, the only way you can suspend a computation and have the rest operations doing later is to put the rest operations into a callback function. This callback function usually returns a Promise (And has a type of Promise&lt;any&gt;). In order to easily mark those async functions, after ES6 javascript provides extra modifiers async and await, which actually wraps up the original utilities of promise and makes it more readable to programmers. Hummm, sounds like things are going better… NO!! It doesn’t resolve anything! The core problem leads to a callback hell is the hierarchical async calls, for example you have some simple synchronous functions which are in a chain to accomplish some logics: a = getSomething(); b = getMore(a); c = getMoreAndMore(b); ... It looks fine for now, but what if they all turn out to be async? Then you have to follow the callback style to make them operate one right after another is done: getSomthing(function(a) { getMore(a, function(b) { getMoreAndMore(b, function(c) { //keep going... }); }); }); Or you prefer ES6: async function getSomething(a) { await b = ToDo(a); return await getMore((b) =&gt; { return await ToDo(b); }).then((c) =&gt; { return await ToDo(c); }).then(...); } Looks really confused? This will getting even uglier if we are using callbacks in loops. Redux ThunksBack to our redux app, we usually want to update some states after an async call to inform the UI that the data is ready to be fetched. That is always achieved by dispatching an action from the component to the reducer: async const callAPI = () =&gt; { ... return response; }; ... async const updateUI = (...params) =&gt; { const res = await callAPI(); if (res.status === 200) dispatch({type: &quot;UPDATE&quot;, isSuccess: true}); }; ... render({ ... this.props.isSuccess? showData() : showError() }); This isn’t bad, but we are always looking for something better. An advanced way to rewrite it is using redux middleware. Middleware is somewhere you can put the code after the framework receives a request and before it generates a response. For example, we want to add a logger in the redux store so that when the store catches an action, before it returns the new state, the logger can log the previous state and the new generated state. This is what can be added as a middleware: function logger(store) { return function wrapDispatch (next) { return function dispatchAndLog (action) { console.log(&quot;dispatching.. &quot;, action); let result = next(action); console.log(&quot;new state&quot;, store.getState()); return result; } } } There are more advanced ways to add a logger. If you are interested, please refer to the offical documentation. With our middleware, the previous example can be written in a cleaner way: const callAPI = () =&gt; { return((dispatch) =&gt; { dispatch(startCallingApiAction); actualCallApi().then(data =&gt; { dispatch(successAction(data)); }).fail(err =&gt; { dispatch(failedAction(err)); }); }); }; The successful response data is wrapped in the payload of the action, sent to the reducer. Once the store updates the data, it will be mapped as a prop back to the component and request for a rerender. This middleware is also called thunk. By applying thunk to decouple the presentation layer, we can get rid of most of the side effects in components, instead, managing and orchestrating side effects in thunks. This is great, so why are we even considering saga? Well, one of the advantages of middleware is that it can be chained. Every middleware mounted in redux store starts an individual thread (or something really looks like a thread in NodeJS). When a middleware captures an action and handles its side effect, it can dispatch a new action to another middleware to do nested logics. This behavior of middleware indicates that thunks can be chained as well, for example thunkA forwards its return payload to thunkB and thunkB forwards its return payload to… Wait! That sounds quite familiar!! Is that the case of callback hell?? Unfortunately, a good thing plus another good feature doesn’t always end up with something better. It could be some shit as well (笑) In this case, true, this is exactly the callback hell. Redux SagaTo handle the possible endless callback functions and also to make it more easily to test in a component which has complicated logics, we need to change our previous thoughts. Just like shifting from Process Oriented Programming to Object Oriented Programming, instead of telling the application how to handle the side effects, suppose it already knows how to call a function and how to dispatch an action, all we need to do is to give instructions about what to do next and we don’t care about how those instructions will be executed (Saga handles the executions). Then the thunks example can be changed as following: export function* apiSideEffect(action) { try{ const data = yield call(actualCallApi); yield put({ type:&quot;SUCCESS&quot;, payload: data }); } catch(err) { yield put({ type:&quot;FAILED&quot;, payload: err }); } } export function* apiSaga() { yield takeEvery(&quot;CLICK_TO_CALL_API&quot;, apiSideEffect); } There are serval fucntions already being integrated in Saga: Call: the method call will return only a plain object describing the operation so redux-saga can take care of the invocation and returns the result to the generator. The first parameter is the generator function ready to be called and the rest params are all the arguments in the generator. Put: Instead of dispatching an action inside the generator (Don’t ever ever do that), put Returns an object with instructions for the middleware to dispatch the action. Select: Returns value from the selector function, similar with getState(). Note: It is not recommended to use this function because it returns the value corresponding to the contents of the store state tree, which is most likely a plain Javascript object and is mutable (Redux wants you to handle state immutably, which means return a new state instead of changing the old one). Take: It creates a command object that tells the middleware to wait for a specific action. The resulting behavior of the call Effect is the same as when the middleware suspends the generator until a promise resolves. In the take case, it’ll suspend the generator until a matching action is dispatched By working with Saga, we make the side effects to be declarative rather than imperative. Declarative: describing what the program must accomplish, rather than describe how to accomplish it Imperative: consists of commands for the computer to perform, focuses on describing how a program operates In the case of take, the control is inverted. Instead of the actions being pushed to the handler tasks, the Saga is pulling the action by itself. An additional generator, known as watcher which contains take has to be created to watch a specific action and being triggered once the following action is dispatched in the application. There are two ways to create a watcher, one is using the buid-in functions (Saga Helper): function* watchFetchData() { yield takeEvery(&quot;FETCH_REQUEST&quot;, callFetchDataApi); } takeEvery allows multiple request to be proceeding at the same time. Or if you just want the latest request to be fired (the older one will be overrided during each time the watcher is triggered function* watchFetchData() { yield takeLatest(&quot;FETCH_REQUEST&quot;, callFetchDataApi); } However by using take, it is possible to fully control an action observation process to build complex control flow: function* watchFetchData() { while(true) { const action = yield take(&quot;FETCH_REQUEST&quot;); console.log(action); yield call(callFetchDataApi, action.payload); } } All right, now you have been exposed to everything you need to know before start trying redux saga on your own. Here is a short overall example that may also help:Store: const sagaMiddleware = createSagaMiddleware(); const store = createStore(rootReducer, appluMiddleware(sagaMiddleware)); sagaMiddleware.run(watchFetch); Sagas: function* watchFetch(): Generator&lt;*, *, *&gt; { yield takeEvery(&quot;FETCH_ACTION&quot;, callFetchAPI); } function* callFetchAPI(): Generator&lt;*, *, *&gt; { try { yield put({ type: &quot;FETCHING&quot;, payload: ... }); const data = yield call(actualCallApi); yield put({ type: &quot;FETCH_SUCCESS&quot;, payload: data }); } catch(err) { yield put({ type: &quot;FETCH_FAILED&quot;, payload: err }); } } Reducer: const reducer = (state = initState, action) =&gt; { switch(action) { case &quot;FETCHING&quot;: return { loading: true, ...state }; case &quot;FETCH_SUCCESS&quot;: return { loading: false, success: true, data: action.payload, ...state }; case &quot;FETCH_FAILED&quot;: return { loading: false, success: false, error: true, ...state }; default: return { ...state }; } }; Component: class myComponent extends React.Component { const mapStateToProps = ... const mapDispatchToProps = ... render() { return ( &lt;button onClick = { () =&gt; this.props.dispatch({type: &quot;FETCH_ACTION&quot;}) }/&gt; { this.props.loading? &lt;p&gt;Loading..&lt;/p&gt; : this.props.error? &lt;p&gt;Error!&lt;/p&gt; : &lt;p&gt;{this.props.data}&lt;/p&gt; } ); } } export default connect(mapStateToProps, mapDispatchToProps)(myComponent); For more advanced concepts, there is a well-organized Saga offical documentation you can refer to if you want to dive deeper. How to test Saga?A function that returns a simple object is easier to test than a function that directly makes an asynchronous call. For redux saga, each time you yield a function call will return a plain javascript object which makes the workflow much easier to test. You don’t need to use the real API, fake it, or mock it, instead just iterating over the generator function, asserting for equality on the values yielded. describe(&quot;fetch work flow&quot;, () =&gt; { const generator = cloneableGenerator(callFetchAPI)({ type: &quot;FETCH_ACTION&quot; }); expect(generator.next().value).toEqual(put({ type: &quot;FETCHING&quot;, payload: ... })); test(&quot;fetch success&quot;, () =&gt; { const clone = generator.clone(); expect(clone.next().value).toEqual(put({ type: &quot;FETCH_SUCCESS&quot; })); expect(generator.next().done).toEqual(true); }); }); In the above example, we use clone() to test different control flows and next() to iterate to the next function ready be yielded. The mock return value can also be injected as an argument of next(): expect(clone.next(false).value).toEqual( put(fetchFailedAction()) ); Saga vs ObservablesRedux saga is not the only solution to our apps which may have complex control flows, they are other helpful tools providing different trade-offs which can also resolve the async problems. Here are some good code snippets of saga vs observables that can open your mind :D References:https://redux-saga.js.org/https://stackoverflow.com/questions/25098066/what-is-callback-hell-and-how-and-why-rx-solves-ithttps://redux.js.org/advanced/middlewarehttps://pub.dartlang.org/packages/redux_thunkhttps://codeburst.io/how-i-test-redux-saga-fcc425cda018https://engineering.universe.com/what-is-redux-saga-c1252fc2f4d1https://www.sitepoint.com/redux-without-react-state-management-vanilla-javascript/https://redux.js.org/introduction/getting-startedhttps://blog.logrocket.com/understanding-redux-saga-from-action-creators-to-sagas-2587298b5e71]]></content>
      <tags>
        <tag>Redux</tag>
        <tag>Saga</tag>
        <tag>React</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F04%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
